[
    {
        "entity": "Luke Zettlemoyer",
        "step": 20312,
        "passage": " yet consider the likelihood of data and present OLM, a novel explanation method, which uses a language model to resample occluded words. It is especially suited for word-level relevance of sentence classification with state-of-the-art NLP models. We also introduce the Class Zero-Sum Axiom for explanation methods, compare it with an existing axiom. Furthermore, we show other axioms that OLM satisfies. We argue that with this more solid theoretical foundation OLM can be regarded as an improvement over existing NLP classification explanation methods. In our experiments, we compare our methods to other occlusion and gradient explanation methods. We do not consider these experiments to be exhaustive. Unfortunately, there is no general evaluation for explanation methods.\nWe show that our method adds value by showing distinctive results and better founded theory. A practical difficulty of OLM is the approximation with a language model. First, a language model can create syntactically correct data, that does not make sense for the task. Second, even state-of-the-art language models do not always produce syntactically correct data. However, we argue that using a language model is a suitable way for finding reference inputs.\nIn the future, we want to extend this method to language features other than words. NLP tasks with longer input are probably not very sensitive to single word occlusion, which could be measured with OLM-S.\nWe would like to thank Leonhard Hennig, Robert Schwarzenberg and the anonymous reviewers for their feedback on the paper. This work was partially supported by the German Federal Ministry of Education and Research as part of the projects BBDC2 (01IS18025E) and XAINES.\nAncona et al. (2018) Marco Ancona, Enea Ceolini, Cengiz \u00d6ztireli, and Markus Gross. 2018. Towards better understanding of gradient-based attribution methods for deep neural networks. International Conference on Learning Representations.\nBach et al. (2015) Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140.\nBahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. International Conference on Learning Representations.\nDevlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186. Association for Computational Linguistics.\nGosiewska and Biecek (2019) Alicja Gosiewska and Przemyslaw Biecek. 2019. Do not trust additive explanations. CoRR, arXiv:1903.11420.\nHewitt and Manning (2019) John Hewitt and Christopher D. Manning. 2019. A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129\u20134138.\nJain and Wallace (2019) Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3543\u20133556.\nLiu et al. (2019a) Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019a. Linguistic knowledge and transferability of contextual representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1073\u20131094.\nLiu et al. (2019b) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: A robustly optimized BERT pretraining approach. CoRR, arXiv:1907.11692.\nLundberg and Lee (2017) Scott M. Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 4765\u20134774. Curran Associates, Inc.\nRibeiro et al. (2016) Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144. ACM.\nRobnik-\u0160ikonja and Kononenko (2008) Marko Robnik-\u0160ikonja and Igor Kononenko. 2008. Explaining classifications for individual instances. IEEE Transactions on Knowledge and Data Engineering, 20(5):589\u2013600.\nShrikumar et al. (2016) Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. 2016. Not just a black box: Learning important features through propagating activation differences. CoRR, arXiv:1605.01713.\nSimonyan et al. (2013) Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, arXiv:1312.6034.\nSocher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP, pages 1631\u20131642.\nSundararajan et al. (2017) Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic Attribution for Deep Networks. In International Conference on Machine Learning, pages 3319\u20133328.\nVaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998\u20136008.\nWarstadt et al. (2018) Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2018. Neural network acceptability judgments. CoRR, arXiv:1805.12471.\nC, i.e. \u2211c\u2208Cfc(x)=1. Let x=(x1,...,xn) be a input split into n input features.\nfrom Completeness. Contradiction.\n2. OLM satisfies Class Zero-Sum. Let rf,c now be the OLM relevance method from equations (1) and (2) in the paper.\n3. OLM satisfies Implementation Invariance. OLM is a black box method and only evaluates the function of the neural network. Thus, it has to satisfy Implementation Invariance.\n4. OLM satisfies Sensitivity-1. OLM is defined as an Occlusion method, so it necessarily gives the difference of prediction when an input variable is occluded.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 20400,
        "passage": " SIGSEM, and on the editorial board for Computational Linguistics.\nDespite constant changes in the way computational linguistics research is carried out, evaluated, and disseminated, NAACL (and ACL) have maintained a stellar record of supporting the community and nurturing the field. The secretary is responsible for communications at the organization and so plays a particularly important part in these activities, notably in outreach efforts that can broaden participation, engage students and break down disciplinary barriers. I\u2019m excited to help push these activities forward.\nAt the same time, our marquee activity is a conference that most participants fly to. We have a big carbon footprint. I\u2019d like to discuss ways we might deepen the diverse program of collocated events we offer, so conference attendance can substitute for other travel. In tandem, as secretary, I\u2019d explore online infrastructure and resources that could make the organization more effective at supporting our members, and promoting computational linguistics, anywhere anytime.\nColin Cherry is a Research Scientist at Google. Previously, he has worked at National Research Council Canada and Microsoft Research. He received his Ph.D. in Computing Science from the University of Alberta. His primary research area is machine translation, but he has also been known to venture into parsing, morphology and information extraction. He is currently secretary for the NAACL and an action editor for the Transactions of the ACL. He has recently served as workshop co-chair for HLT-NAACL 2012, as publications co-chair for HLT-NAACL 2013, on the editorial board of Computational Linguistics from 2013 to 2015, and as machine translation area co-chair for ACL 2014 and IJCNLP 2017.\nThe NAACL Secretary is responsible for helping to organize meetings, elections and general communications, including the website and social media. As secretary, I\u2019ve done what I can to keep these things running smoothly, including managing a shift of naacl.org to a new markdown language. As a board member, I was part of the organizing team for awarding NAACL-sponsored scholarships to attend the 2017 Jelinek Summer School.\nI am generally happy with the NAACL web site and our election process. The secretary responsibility with the greatest room for improvement continues to be NAACL\u2019s use of social media. The organization\u2019s Twitter account has previously been used only to announce the executive\u2019s direct activities. This year, I will extend the account\u2019s role to aggressively promote the social media activities of the NAACL conference. Going beyond that with social media raises the question of what role, if any, NAACL should play in promoting and commenting on general news about natural language technologies in North America. This question is increasingly important as the impact of our technologies grows. It is not a straightforward question, but it is the sort of thing I\u2019d like to help us figure out if given a second term.\nAs a board member, I would also like to continue to make our conferences family friendly. At this year\u2019s ACL, we made big steps toward this by providing on-site child care. I\u2019d like to also look into providing support to families that don\u2019t necessarily want child care, by providing space or activities to allow children and spouses to get to know each other.\nI have greatly enjoyed my time as NAACL Secretary, and I sincerely hope you\u2019ll give me the chance to serve for another two years.\nMarie-Catherine de Marneffe is an Assistant Professor in Linguistics at The Ohio State University. She received her PhD from Stanford University in December 2012 under the supervision of Christopher D. Manning. Her research focuses on computational pragmatics. She is one of the principal developers of the Stanford Dependencies and the Universal Dependencies representations. She helped organize the shared task on Named-Entity recognition in Twitter at the ACL 2015 and Coling 2016 workshops on Noisy User-generated Text, and co-chaired with Joakim Nivre the first workshop on Universal Dependencies in 2017. She served as an area chair for ACL 2016. She is currently a member of the NAACL board as well as of the Computational Linguistics editorial board.\nIn recent years, the NAACL board aimed at the improvement of the reviewing process and paper selection, a better integration between different disciplines, and the inclusion of all of the Americas. In the last two years, I have worked, together with other board members, towards these three goals (for instance, Hal Daum\u00e9 III and I organized the author response survey, which led to recommendations to program chairs; I helped Joel Tetreault with the Emerging Region Fund). By continuing to serve on the board, I can materialize the efforts started. I also want to make sure that our field stays true to its interdisciplinary nature, and stays grounded in the linguistics part of \u201ccomputational linguistics\u201d. NAACL has to become a better illustration of how research in theoretical linguistics and NLP can complement each other. I will continue to work to make this happen.\nMarilyn Walker is a Professor of Computer Science at the University of California at Santa Cruz. Her current research includes work on computational models of dialogue interaction and conversational agents, statistical and expressive natural language generation, analysis of social phenomena in social media dialogue, and research on the analysis and generation of narratively structured texts. Walker was a professor of Computer Science at the University of Sheffield from 2003 to 2009 where she held a Royal Society Wolfson Award. From 1996 to 2003, she was a principal member of the research staff at AT&T Bell Labs and AT&T Research, where she focused on statistical methods for dialogue management and statistical natural language generation. She earned a B.A. in Computer and Information science at UC Santa Cruz, an M.S. in Computer Science at Stanford University, and M.A. in Linguistics and Ph.D. in Computer and Information Science at the University of Pennsylvania.\nThis is an exciting time for the NLP community with the rapid growth and expansion in the field. Our first NAACL conference, only 17 years ago, had eight area chairs and received 166 submissions. In 2018 we will have around 65 area chairs with a guestimate of getting possibly more than 1500 submissions. The downside of this rapid growth is maintaining the quality of the conference, along with the potential for change in the culture and feeling of our community.\nThis community has always been one of the most diverse sub-areas of computer science and that is something that is valuable: we have to work to maintain and nourish this diversity. I will continue to support and work on the initiatives that recently resulted in changes to the ACL constitution to enforce more gender and geographical diversity and the establishment of the Women in NLP workshop.\nWe need to make an explicit attempt to value our roots. We could consider establishing a Test of Time award for work published in NAACL more than ten years ago. As we grow we do not want to lose the valuable contributions from scientists in other disciplines that have always had a core interest in human language.\nFinally, I hope to work towards methods and approaches by which NAACL can explicitly encourage an even greater diversity of research topics and approaches. We need to encourage reviewers to place greater weight on highly creative and original research, and area chairs to prioritize the final selection of papers that are both well-grounded in the literature and very creative. We could expand the idea of best paper awards to have special sessions that would highlight the most creative and original work, perhaps by explicitly adding new types of paper awards.\nLuke Zettlemoyer is an Associate Professor in the Allen School of Computer Science & Engineering at the University of Washington, and also runs the AllenNLP group at the Allen Institute for Artificial Intelligence. His research focuses on empirical computational semantics, covering a wide range of core language understanding problems such as semantic parsing and coreference resolution. He has served the *ACL community for over 10 years, as reviewer, area chair, and action editor for various conference and journals. He has never served on a board, but would be happy to do so if elected.\nNAACL is a healthy, growing community. I think that, overall, we should be careful to maintain the high quality of the conference, while making incremental adjustments to improve reviewing quality, inclusiveness, institutional memory, support for interdisciplinary research, and recognition of high quality work.\nAs a member of the NAACL Executive Board, I would like to work on addressing two main issues.\nThe first issue is how to ensure that a larger number of high quality papers introducing creative and non-mainstream ideas are part of the NAACL program. Admittedly, addressing this issue will be closely linked to addressing the quality of reviewing, which has been one of the issues tackled by the executive board and certainly a recurring topic of discussion at our conferences. One step in this direction would be to revisit the idea of implementing different reviewing criteria for different types of papers. In addition, I would like to work with the executive board on implementing a feedback mechanism from area chairs to reviewers with the goal of promoting fair and constructive reviews.\nThe second issue I would like to tackle is strengthening the participation of undergraduate and high-school students at NAACL conferences and/or workshops. At NAACL 2015 SRW, where I served as one of the faculty advisors, we introduced for the first time the undergraduate research track, which proved to be successful. I would like to continue to improve the participation of undergraduate students and to establish a tradition of having the winners of NACLO (North American Computational Linguistics Olympiad for high-school students) attend NAACL.\nAline Villavicencio is a Lecturer/Reader in Computer Science affiliated to the Federal University of Rio Grande do Sul (Brazil) and to the University of Essex (UK). Her research interests include lexical semantics, multiling<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 20529,
        "passage": "imagination by you. Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a \u2026 We use the sample softma\u2026 With the addition feature concatenation of GloVe and character n-gram embeddings further boosts the performance of all downstream tasks. IP multicast can achieve these requirements but has control- and data-plane \u2026 In particular, McCann et al. ELMo embeddings, developed at Allen NLP, are one of many great pre-trained models available on Tensorflow Hub. View Demo Get Started. Elmo, stylized as elmo (the name is a blend of elastic and monkey), is a computer shogi evaluation function and book file created by Makoto Takizawa (\u7027\u6fa4\u8aa0).It is designed to be used with a third-party shogi alpha\u2013beta search engine. ELMo: Embeddings from Language Models. Modern cloud applications frequently exhibit one-to-many communication patterns and, at the same time, require sub-millisecond latencies and high throughput. Performance shows that the addition of ELMo representations improves all previous state-of-the-art including CoVe. Use Git or checkout with SVN using the web URL. Toggle Navigation.\nmaster. According to many, leather is the most versatile of all natural materials available. relmo (elmo WCSC27 + rezero8), rezero; Silent Majority; Spear; SSP; Tanuki (\u30ca\u30a4\u30c4\u30fb\u30aa\u30d6\u30fb\u30bf\u30cc\u30ad WCSC27, \u5e73\u6210\u5c06\u68cb\u5408\u6226\u307d\u3093\u307d\u3053 SDT5) TJshogi; YaneuraOu (\u3084\u306d\u3046\u3089\u738b) Yomita (\u8aad\u307f\u592a) See also.\n(2016) before you continue.\nGitHub is where people build software. GitHub is home to over 50 million developers working together to host and review code, manage projects, and build software together. For each token, a L-layer biLM computes \u2026 AllenNLP makes it easy to design and evaluate new deep learning models for nearly any NLP problem, along with the \u2026 Introduction Intended audience: people who want to train ELMo from scratch and understand the details of official implementation. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer. It\u2019s been shown to outperform GloVe and Word2Vec embeddings on a wide variety of NLP tasks. February 6, 2020. ELMo actually goes a step further and trains a bi-directional LSTM \u2013 so that its language model doesn\u2019t only have a sense of the next word, but also the previous word.\nBig Peer Review Challenge.\n[2] from AllenAI, on the other hand, take advantage of monolingual data, which is much easier to collect than a parallel corpus.\nCite as:\tarXiv:1802.05365 [cs.CL] Subjects:\tComputation and Language (cs.CL) I hope this post could help you ramp up quickly. The architecture of The Embeddings from Language Models (ELMo) is L-layers of bidirectional LSTMs (biLM) similar to their previous work TagLM [3] where token embeddings (CNN over characters) serve as input, except ELMo tie the parameters for both the token representation and Softmax layer in the forward and backward directions.\nCategories. More than 50 million people use GitHub to discover, fork, and contribute to over 100 million projects. In order to prove that, they conduct extensive experiments on several language understanding problems including textual entailment, question answering, sentiment analysis, semantic role labeling, coreference resolution and named entity extraction. ELMo: Embeddings from Language Models, which comes from the paper \"Deep contextualized word representations\".This resource includes various methods of using ELMo, visual analysis of ELMo, and paper interpretation.Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke ZettlemoyerOur word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. Application of state-of-the-art text classification techniques ELMo and ULMFiT to A Dataset of Peer Reviews (PeerRead) Continue reading. Using, visualizing and understanding EMLo by examples! Our full benchmarks are available under reports/evaluation.csv.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 20827,
        "passage": " into new or existing pipelines based on that framework.\nOver the past few years, we have seen LLMs used to great effect by researchers and practitioners alike in a variety of verticals and on a variety of tasks (text classification, sequence tagging, relation extraction, etc.). In this post, we use some of the relatively \u201csmall\u201d LLMs (BERT-base and BART-large) on a set of public datasets as a simple illustrative example of the types of trends we have observed in practice on a larger scale.\nWe start by using BERT as a zero-shot classifier. No additional training data\u2014just immediate predictions for new tasks. We then show how even just a handful of relevant training examples (a few-shot learning setting) can help BERT to become a significantly stronger contributor, though the benefit of additional data points quickly peters off. Finally, we look at a version of the BART model that has been pre-trained on entailment tasks in order to make it a better zero-shot classifier without the extra labeled data burden of a few-shot model. The model is then pre-trained to predict whether the hypothesis is \u201centailed\u201d by (i.e. logically follows from) the premise.\nFor each of these settings, we compare the performance of using the LLM classifier directly, using a set of human-generated LFs to create a training set for a RoBERTa model to learn from, and using the combination of an LLM LF plus human-generated LFs to create a training set for the same model.\nAll human-generated LFs come from the WRENCH repository, a baseline for weak supervision with nearly two dozen datasets and associated labeling functions. In this post we report experimental results on four datasets in particular: YouTube spam classification, IMDB sentiment classification, Yelp sentiment classification, and AGNews topic classification. For consistency, all weak supervision experiments use the MeTaL label model for combining multiple LF votes into resulting training labels. All results reported are averaged over 3 random seeds.\nFinal model accuracy averaged over four datasets with callouts of the differentials compared to \u201cHuman LFs + LLM LF\u201d. Note the y-axis starts at 50%.\nUsing BERT out of the box as a zero-shot classifier sadly does not yield great results. It significantly underperforms the human-generated LFs, and actually hurts overall performance if added to them. To be effective, it needs to either be tuned to the new tasks with a few labeled examples, or have its zero-shot abilities improved.\nAdding a few \u201cshots\u201d of labeled examples to fine-tune BERT helps to better connect what it knows to the problems at hand. We updated the model weights following the strategy outlined here, and used a range of 3 to 20 shots per-label (up to 80 labeled examples for the AGNews dataset, which has 4 labels) to see how well the model performed at different levels of training data availability.\nFinal model accuracy at varying amounts of training data. Again, averaged over four datasets with callouts of the differentials compared to \u201cHuman LFs + LLM LF\u201d. Note the y-axis starts at 50%.\nSuccess! We saw an overall improvement in both scenarios for every single number of \u201cshots\u201d \u2013 3, 5, 10 and 20. The fact that we see improvement at all on these datasets in conjunction with the human-created LFs is impressive, as these LFs were manually curated to provide high-level performance on-par with using a fully labeled training set 3.\nWe can dive a little deeper into where BERT does well, and where it struggles. Below we see a per-dataset view of BERT used as a zero- and few-shot learning classifier, with and without weak supervision. The gap between using the LLM with and without weak supervision is substantial, even as the amount of training data we feed into BERT increases.\nMore interestingly, we can also note that the incremental performance gain given by more training data drops off fairly quickly \u2013 performance both with and without weak supervision seems to plateau midway through the graph. At a certain point, it seems that it is more beneficial to focus your efforts on creating new labeling functions to complement what your LLM knows (jumping from the light blue to the dark blue line in the graphs), rather than hand-labeling more and more data to eke out slightly more performance with the few-shot model. Doing this will give you the best of both worlds \u2013 utilizing both the few-shot capabilities of these models, and the efficiency benefits of weak supervision as you increase performance without new labeled data.\nNow, while these models did show impressive results, they did still have some drawbacks. The few-shot language models took a non-trivial amount of GPU time (10-30 minutes per dataset) to train, as well as figuring out good default hyperparameters to conduct that training. They also required us to constrain our answer space mapping (a map from tokens output by the model to classes in the label space of the problem) to a single token \u2013 which can be hard when a class requires a more detailed description than \u201cspam\u201d or \u201cpositive\u201d (see more here).\nWhat we\u2019d really like is a way to help the model improve its ability to contribute to new tasks without requiring extra training or restricting how we frame our problem.\nThe \u201cBART-large-MNLI\u201d model by Facebook is a BART model 4 that has then been fine-tuned on the Multi-NLI dataset, an entailment task. At a high level, this just means that you can pass the model two input texts: a premise, and a hypothesis. To bring this back to our problem, all we have to do is treat each example we are trying to classify (e.g., a YouTube comment to identify as spam or not) as a premise, then pass the model our task-specific True/False statement (\u201cThis comment is spam.\u201d).\nGeneralize to unseen data\u2014few-shot learning models can have bad failure modes when new data samples are dissimilar from the (few) that they were trained on. Capable zero-shot models, however, have never seen your task-specific data and can generalize to domain shifts much better.\nLarge language models continue to delight with every surprising demo of their zero- and few-shot learning capabilities. Such demos hint at the rich knowledge stored inside these models, and researchers are still working on creative ways to extract and incorporate all of this knowledge into solutions to real-world problems.\nWeak Supervision stands as an excellent way to leverage these models in your production pipeline. It not only gives significant performance gains compared to using these models off the shelf, but it also offers flexibility in how we drill into that rich untapped signal. Treating BERT, T5, or GPT-3 as an additional axis of knowledge instead of an oracle allows us to incorporate other subject matter expertise into our solution, and provides a benefit in both directions\u2014subject matter experts get a free boost in signal from an external resource, and large language models don\u2019t need to reach production levels of accuracy before they can be utilized.\nIn the experiments outlined briefly in this post, we found that even in a simple case where labeling functions are created independently from the LLM (rather than in response to its gaps) and only one of the \u201csmallest\u201d LLMs is used (BERT), overall model quality can be instantly improved by plugging in a pre-trained LLM as an additional labeling function. As the study and development of LLMs continue, we expect to see increasingly large gains from incorporating these rich resources into weak supervision pipelines.\nLearn more about Snorkel AI\u2019s published research on our website. To learn more about data-centric AI with Snorkel Flow, request a demo, or stay informed about future news and research from the Snorkel team, you can follow us on Twitter, Linkedin, Facebook, or Instagram.\n2 \u201cT-Zero/README.Md At Master \u00b7 Bigscience-Workshop/T-Zero\u201d. 2022. Github. https://github.com/bigscience-workshop/t-zero/blob/master/inference/README.md.\n3 Zhang, Jieyu, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and Alexander Ratner. 2021. \u201cWRENCH: A Comprehensive Benchmark For Weak Supervision\u201d. Arxiv.Org. https://arxiv.org/abs/2109.11377.\n4 Lewis, Mike, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. \u201cBART: Denoising Sequence-To-Sequence Pre-Training For Natural Language Generation, Translation, And Comprehension\u201d. Arxiv.Org. https://arxiv.org/abs/1910.13461.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 20873,
        "passage": " several main components. (1) A self-learning Algorithm to encoding the sentence with the self-attention (2) A Combination-level RNN to handle the output of the encoding and classify the label of the sentence. We describe the details below.\nSelf-learning algorithm encodes a variable-length sentence into a fixed size. There are two types of the algorithm; one base on the Self\u2013Attention and another just base on the deep contextualization word representation.\nwill encode a variable-length sequence using attention mechanism that considers the different position, token, and segment within the sequence. Inspired by Devin et al. (2018) and Tran et al. (2017), we use the Combination-Level RNN (Section 3.2) into a self-attractive encoder (Lin et al. 2017). The result of the encoding will get out a 2-D vector for each sentence. We follow the instruction of Vipul Raheja and Joel Tetreault (2019) and Joel Tetreault and Liu et al. (2019) to explain the modification below.\nThe utterance ti is also mapped into the embedding layer and result in s-dimensional embedding for each word in sequence based on the Transformer (Vaswani et al. 2017). Then the embedding would be put into the bidirectional-GRU layer.\nHere WS1 is a weight matrix, WS2 and WS3 is a matrix of parameters. b is a bias of vector representing. Equation 2 can be treated as a 2-layer MLP with bias, and da with hidden unit.\nuse PCA and t-SNE to reduce the dimensions from a higher level to a lower level. Then we use the Combination-Level RNN (Section 3.2) which provide us the previous hidden state of utterance encode. It provides us the context relationship in the sentences and combines all hidden states of words in sentences. After that, the deep contextualization word representation encoder encodes the combination into the 2-D vectors of each sentence. We follow the instruction of the Peters at el. (2018) to explain our modification below.\nIn (1), the sjtask is softmax-normalized weights, and the scalar parameter \u03b3task allows the task model to scale the entire vector. In the simple case,the representation would choose the top layer and E(Rk) =.\nas defined in Lee and Dernoncourt (2016).\nTable 1 shows the statistics for both datasets. They both exist many kinds of the labels of the class to classify the kind of sentences they are. There are some special DA classes in both datasets, such as Tag-Question in SwDA and Statement-Question in NLTK. Both datasets make over 25% of the question type labels in each set.\ngive out the most related decoder to decode them to the related labels.\nThe model that we use combines the all final representative of the combination for hidden layers by the self-learning and self- attention. It can help us figure out what the labels of those utterances and give out the result. The score we compute for the algorithm is to calculate the accuracy of the correct labels in the classifications though Hossin M. and Sulaiman M.N. (2015) suggests. Also, we apply some advanced check for the question and answer problem. For those that are an unsure sentence, we would put them into the parser tree to have another classification. The parser tree we use is based on the Huang (2018). We use its Tensor Product Representation to rebuild our parser tree for our model. In our model, we use the bi-LSTM with the attention algorithm to rebuild the parser tree and get the tree graph with POS tags, which is useful to calcify the structure of the sentence. After that, we use the graph we get to analyze the structure of utterances and give out the classification of the unsure sentence in the document. Finally, we will give out the combination result to the users to check the question and answer problems.\nWe developed a new model which perform the QA classification with attention and make comparisons with the common-use algorithms by testing with the SwDA dataset. We experience different utterance representation method and show that the context details highly depend on the classification performance. Working with attention and combination level to the classification, which has not previously been applied in this kind of task, enable the model can learn more from the context and get more real meaning of words in utterance than before. It helps improve the performance of the classification for those kinds of tasks.\nAs future work, we would try more attention mechanisms, such as block self-attention (Shen et al., 2018b), or hierarchical attention (Yang et al., 2016), hypergraph attention (Song et al. 2019). Because they can incorporate the information from different representation for the various position and they can capture both local and long-range context dependency.\nJi Young Lee and Franck Dernoncourt. 2016. Sequential short-text classification with recurrent and convolutional neural networks. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 515\u2013520. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations.\nDan Jurafsky, Liz Shriberg, and Debra Biasca. 1997. Switchboard SWBD-DAMSL shallow-discoursefunction annotation coders manual, draft 13. Technical report, University of Colorado at Boulder Technical Report 97-02.\nRajpurkar P, Zhang J, Lopyrev K, Liang P. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv [cs.CL].\nQuan Hung Tran, Ingrid Zukerman, and Gholamreza Haffari. 2017. A hierarchical neural model for learning sequences of dialogue acts. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 428\u2013437. Association for Computational Linguistics.\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. In International Conference on Learning Representations 2017 (Conference Track).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6000\u20136010.\nWei Li and Yunfang Wu. 2016. Multi-level gated recurrent neural network for dialog act classification. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1970\u20131979. The COLING 2016 Organizing Committee.\nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543. Association for Computational Linguistics.\nQiuyuan Huang, Li Deng, Dapeng Wu, Chang Liu, and Xiaodong He. Feb 2018. Attentive Tensor Product Learning. arXiv [cs.CL].\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, and Chengqi Zhang. 2018b. Bi-directional block selfattention for fast and memory-efficient sequence modeling. In International Conference on Learning Representations.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1480\u20131489. Association for Computational Linguistics.\nSong Bai, Feihu Zhang, and Philip H.S. Torr. Jan 2019. Hypergraph Convolution and Hypergraph Attention. arXiv [cs.CL].\n\"Question Classification with Deep Contextualized Transformer.\" UKEssays.com. 11 2018. All Answers Ltd. 07 2020 <https://www.ukessays.com/essays/computer-science/question-classification-with-deep-contextualized-transformer.php?vref=1>.\n\"Question Classification with Deep Contextualized Transformer.\" All Answers Ltd. ukessays.com, November 2018. Web. 14 July 2020. <https://www.ukessays.com/essays/computer-science/question-classification-with-deep-contextualized-transformer.php?vref=1>.\nUKEssays. November 2018. Question Classification with Deep Contextualized Transformer. [online]. Available from: https://www.ukessays.com/essays/computer-science/question-classification-with-deep-contextualized-transformer.php?vref=1 [Accessed 14 July 2020].\nUKEssays. Question Classification with Deep Contextualized Transformer [Internet]. November 2018. [<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 20944,
        "passage": " and the Toyota Technological Institute at Chicago) released\n   with the paper `ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\n   <https://arxiv.org/abs/1909.11942>`__, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\n   Sharma, Radu Soricut.\n2. :doc:`BART <model_doc/bart>` (from Facebook) released with the paper `BART: Denoising Sequence-to-Sequence\n   Pre-training for Natural Language Generation, Translation, and Comprehension\n   <https://arxiv.org/pdf/1910.13461.pdf>`__ by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\n   Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\n3. :doc:`BARThez <model_doc/barthez>` (from \u00c9cole polytechnique) released with the paper `BARThez: a Skilled Pretrained\n   French Sequence-to-Sequence Model <https://arxiv.org/abs/2010.12321>`__ by Moussa Kamal Eddine, Antoine J.-P.\n   Tixier, Michalis Vazirgiannis.\n4. :doc:`BERT <model_doc/bert>` (from Google) released with the paper `BERT: Pre-training of Deep Bidirectional\n   Transformers for Language Understanding <https://arxiv.org/abs/1810.04805>`__ by Jacob Devlin, Ming-Wei Chang,\n   Kenton Lee and Kristina Toutanova.\n5. :doc:`BERT For Sequence Generation <model_doc/bertgeneration>` (from Google) released with the paper `Leveraging\n   Pre-trained Checkpoints for Sequence Generation Tasks <https://arxiv.org/abs/1907.12461>`__ by Sascha Rothe, Shashi\n   Narayan, Aliaksei Severyn.\n6. :doc:`Blenderbot <model_doc/blenderbot>` (from Facebook) released with the paper `Recipes for building an\n   open-domain chatbot <https://arxiv.org/abs/2004.13637>`__ by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary\n   Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n7. :doc:`BlenderbotSmall <model_doc/blenderbot_small>` (from Facebook) released with the paper `Recipes for building an\n   open-domain chatbot <https://arxiv.org/abs/2004.13637>`__ by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary\n   Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n8. :doc:`BORT <model_doc/bort>` (from Alexa) released with the paper `Optimal Subarchitecture Extraction For BERT\n   <https://arxiv.org/abs/2010.10499>`__ by Adrian de Wynter and Daniel J. Perry.\n9. :doc:`CamemBERT <model_doc/camembert>` (from Inria/Facebook/Sorbonne) released with the paper `CamemBERT: a Tasty\n   French Language Model <https://arxiv.org/abs/1911.03894>`__ by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz\n   Su\u00e1rez*, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah and Beno\u00eet Sagot.\n10. :doc:`ConvBERT <model_doc/convbert>` (from YituTech) released with the paper `ConvBERT: Improving BERT with\n    Span-based Dynamic Convolution <https://arxiv.org/abs/2008.02496>`__ by Zihang Jiang, Weihao Yu, Daquan Zhou,\n    Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n11. :doc:`CTRL <model_doc/ctrl>` (from Salesforce) released with the paper `CTRL: A Conditional Transformer Language\n    Model for Controllable Generation <https://arxiv.org/abs/1909.05858>`__ by Nitish Shirish Keskar*, Bryan McCann*,\n    Lav R. Varshney, Caiming Xiong and Richard Socher.\n12. :doc:`DeBERTa <model_doc/deberta>` (from Microsoft Research) released with the paper `DeBERTa: Decoding-enhanced\n    BERT with Disentangled Attention <https://arxiv.org/abs/2006.03654>`__ by Pengcheng He, Xiaodong Liu, Jianfeng Gao,\n    Weizhu Chen.\n13. :doc:`DialoGPT <model_doc/dialogpt>` (from Microsoft Research) released with the paper `DialoGPT: Large-Scale\n    Generative Pre-training for Conversational Response Generation <https://arxiv.org/abs/1911.00536>`__ by Yizhe\n    Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n14. :doc:`DistilBERT <model_doc/distilbert>` (from HuggingFace), released together with the paper `DistilBERT, a\n    distilled version of BERT: smaller, faster, cheaper and lighter <https://arxiv.org/abs/1910.01108>`__ by Victor\n    Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into `DistilGPT2\n    <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__, RoBERTa into `DistilRoBERTa\n    <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__, Multilingual BERT into\n    `DistilmBERT <https://github.com/huggingface/transformers/tree/master/examples/distillation>`__ and a German\n    version of DistilBERT.\n15. :doc:`DPR <model_doc/dpr>` (from Facebook) released with the paper `Dense Passage Retrieval for Open-Domain\n    Question Answering <https://arxiv.org/abs/2004.04906>`__ by Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick\n    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\n16. :doc:`ELECTRA <model_doc/electra>` (from Google Research/Stanford University) released with the paper `ELECTRA:\n    Pre-training text encoders as discriminators rather than generators <https://arxiv.org/abs/2003.10555>`__ by Kevin\n    Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n17. :doc:`FlauBERT <model_doc/flaubert>` (from CNRS) released with the paper `FlauBERT: Unsupervised Language Model\n    Pre-training for French <https://arxiv.org/abs/1912.05372>`__ by Hang Le, Lo\u00efc Vial, Jibril Frej, Vincent Segonne,\n    Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Beno\u00eet Crabb\u00e9, Laurent Besacier, Didier Schwab.\n18. :doc:`Funnel Transformer <model_doc/funnel>` (from CMU/Google Brain) released with the paper `Funnel-Transformer:\n    Filtering out Sequential Redundancy for Efficient Language Processing <https://arxiv.org/abs/2006.03236>`__ by\n    Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\n19. :doc:`GPT <model_doc/gpt>` (from OpenAI) released with the paper `Improving Language Understanding by Generative\n    Pre-Training <https://blog.openai.com/language-unsupervised/>`__ by Alec Radford, Karthik Narasimhan, Tim Salimans\n    and Ilya Sutskever.\n20. :doc:`GPT-2 <model_doc/gpt2>` (from OpenAI) released with the paper `Language Models are Unsupervised Multitask\n    Learners <https://blog.openai.com/better-language-models/>`__ by Alec Radford*, Jeffrey Wu*, Rewon Child, David\n    Luan, Dario Amodei** and Ilya S<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 20950,
        "passage": ".\nMartin Sundermeyer, Hermann Ney, and Ralf Schl\u00fcter. 2015. From feedforward to recurrent LSTM neural networks for language modeling. IEEE ACM Trans. Audio Speech Lang. Process., 23(3):517\u2013529.\nWietse de Vries, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, and Malvina Nissim. 2019. Bertje: A dutch BERT model. CoRR, abs/1912.09582.\nAlex Wang and Kyunghyun Cho. 2019. BERT has a mouth, and it must speak: BERT as a markov random field language model. CoRR, abs/1902.04094.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface\u2019s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771.\nShijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Emerging crosslingual structure in pretrained language models. In Proceedings of ACL 2020.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019a. PAWS-x: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3687\u2013 3692, Hong Kong, China. Association for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019b. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 5754\u20135764.\nPierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2018. Overview of the third bucc shared task: Spotting parallel sentences in comparable corpora. In Proceedings of 11th Workshop on Building and Using Comparable Corpora, pages 39\u201342.\nDespite these two outliers, we consider the rest of our prompts to be of high quality. Even if small inflection or grammatical gender assignment mistakes occur (e.g. in Greek) this should not render the prompt unintelligible to native speakers \u2013 the burden is on the model to be robust to such slight variations, just as humans are. We point out that the prompts can be awkward or incorrect for some senses captured by the relation, an issue unrelated to our gender heuristics or automatic inflection. This issue, though, is also present in the LAMA English prompts (Petroni et al., 2019; Jiang et al., 2020) and is the result of the original Wikidata annotation.\nWe outline here the exact concrete formulation of our multi-token decoding algorithms. Given a sentence with multiple mask tokens, e.g., Eq. 2, we can either generate outputs in parallel independently or one at a time conditioned on the previously generated tokens. These methods are similar to the prediction problems that BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019b) perform in their pre-training stages respectively. We define c \u2208 Rn as the confidence of each prediction, with details varying by prediction method.\nIn the refinement stage, we choose from all predicted tokens the one with the lowest confidence (i.e., the lowest probability) and re-predict it (Ghazvininejad et al., 2019): yk = argmax p(yk|si:j \\ k), ck = p(yk|si:j \\ k), yk k = argmin ck.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 21295,
        "passage": " negative spans can be better divided with the correct answer \u201cJerusalem\u201d. This shows that SCL in our KECP framework is reliable and can improve the performance for EQA.\nThe Accuracy of Answer Generation. A major difference between previous works and ours is that we model the EQA task as text generation. Intuitively, if the model correctly generates the first answer token, it is easy to generate the remaining answer tokens because of the very small search space. Therefore, we analyze how difficult it is for the model to generate the first token correctly. Specifically, we check whether the generated first token and the first token of the ground truth are within a fixed window size nw. As shown in Table 5, we find the accuracy of our method is lower than RoBERTa-base Liu et al. (2019) when nw=1. Yet, we achieve the best performance when increasing the window size nw to 5. We think that our KECP can generate some rehabilitation text for the answer. For example in Figure 4, the PLM may generate \u201cthe conquest of Jerusalem\u201d rather than the correct answer with single token \u201cJerusalem\u201d. This phenomenon reflects the reason why we achieve lower accuracy when nw=1. But, we think that the generated results are still in the vicinity of the correct answer.\nTable 5: The accuracy of predicting the first [MASK] in the query prompt with full training samples for each task. #nw denotes the window size.\nTo bridge the gap between the pre-training and fine-tuning objectives, KECP views EQA as an answer generation task. In KECP, the knowledge-aware prompt encoder injects external domain-related knowledge into the passage, and then enhances the representations of selected prompt tokens in the query. The span-level contrastive learning objective is proposed to improve the performance of EQA. Experiments on multiple benchmarks show that our framework outperforms the state-of-the-art methods. In the future, we will i) further improve the performance of KECP by applying controllable text generation techniques, and ii) explore the prompt-tuning for other types of MRC tasks, such as cloze-style MRC and multiple-choice MRC.\nBrown et al. (2020) Tom B. Brown, Benjamin Mann, and etc. Nick Ryder. 2020. Language models are few-shot learners. In NeurIPS.\nChen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A simple framework for contrastive learning of visual representations. In ICML, volume 119, pages 1597\u20131607.\nDai et al. (2021) Damai Dai, Hua Zheng, Zhifang Sui, and Baobao Chang. 2021. Incorporating connections beyond knowledge embeddings: A plug-and-play module to enhance commonsense reasoning in machine reading comprehension. CoRR, abs/2103.14443.\nDettmers et al. (2018) Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018.\nConvolutional 2d knowledge graph embeddings.\nDunn et al. (2017) Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur G\u00fcney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q&a dataset augmented with context from a search engine. CoRR, abs/1704.05179.\nFisch et al. (2019) Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In EMNLP, pages 1\u201313.\nGao et al. (2021) Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In ACL, pages 3816\u20133830.\nHan et al. (2021) Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2021. PTR: prompt tuning with rules for text classification. CoRR, abs/2105.11259.\nJoshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predicting spans. TACL, 64\u201377.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL, pages 1601\u20131611.\nKwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, and et al. 2019. Natural questions: a benchmark for question answering research. TACL.\nLai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard H. Hovy. 2017. RACE: large-scale reading comprehension dataset from examinations. In EMNLP, pages 785\u2013794.\nLevy et al. (2017) Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In CoNLL, pages 333\u2013342.\nLi and Liang (2021a) Xiang Lisa Li and Percy Liang. 2021a. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP, pages 4582\u20134597. Association for Computational Linguistics.\nLi and Liang (2021b) Xiang Lisa Li and Percy Liang. 2021b. Prefix-tuning: Optimizing continuous prompts for generation. In ACL, pages 4582\u20134597.\nLiu et al. (2021a) Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021a. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. CoRR.\nLiu et al. (2021b) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT understands, too. CoRR, abs/2103.10385.\nLiu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, and et al. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR.\nQin and Eisner (2021) Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. In NAACL-HLT, pages 5203\u20135212.\nRajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable questions for squad. CoRR, abs/1806.03822.\nRajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. CoRR.\nRam et al. (2021) Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, and Omer Levy. 2021. Few-shot question answering by pretraining span selection. In ACL.\nSchick and Sch\u00fctze (2021) Timo Schick and Hinrich Sch\u00fctze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In EACL, pages 255\u2013269.\nShin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP.\nTrischler et al. (2017) Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017. NewsQA: A machine comprehension dataset. In WRLNLP, pages 191\u2013200.\nVan der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne.\nVinyals et al. (2015) Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In NIPS, pages 2692\u20132700.\nWang and Jiang (2019) Chao Wang and Hui Jiang. 2019. Explicit utilization of general knowledge in machine reading comprehension. In ACL, pages 2263\u20132272. Association for Computational Linguistics.\nWang et al. (2022) Chengyu Wang, Minghui Qiu, Taolin Zhang, Tingting Liu, Lei Li, Jianing Wang, Ming Wang, Jun<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 21346,
        "passage": "attikara, Sergey Levine, and Sergio Guadarrama. From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following. In ICLR, 2019.\n[Fulda et al.2017] Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. What can you do with a rock? affordance extraction via word embeddings. arXiv preprint arXiv:1703.03429, 2017.\n[Goldberg2019] Yoav Goldberg. Assessing BERT\u2019s Syntactic Abilities. CoRR, abs/1901.05287, 2019.\n[Gopnik and Meltzoff1987] Alison Gopnik and Andrew Meltzoff. The development of categorization in the second year and its relation to other cognitive and linguistic developments. Child development, 1987.\n[Gordon et al.2018] Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. Iqa: Visual question answering in interactive environments. In CVPR, 2018.\n[Goyal et al.2019] Prasoon Goyal, Scott Niekum, and Raymond J. Mooney. Using Natural Language for Reward Shaping in Reinforcement Learning. IJCAI, 2019.\n[Hermann et al.2017] Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, Marcus Wainwright, Chris Apps, Demis Hassabis, and Phil Blunsom. Grounded Language Learning in a Simulated 3d World. arXiv:1706.06551 [cs, stat], 2017.\n[Ho and Ermon2016] Jonathan Ho and Stefano Ermon. Generative Adversarial Imitation Learning. In NIPS, 2016.\n[Howard and Ruder2018] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In ACL, 2018.\n[Hu et al.2019] Hengyuan Hu, Denis Yarats, Qucheng Gong, Yuandong Tian, and Mike Lewis. Hierarchical decision making by generating and following natural language instructions. arXiv preprint arXiv:1906.00744, 2019.\n[Infocom1980] Infocom. Zork I, 1980.\n[Janner et al.2018] Michael Janner, Karthik Narasimhan, and Regina Barzilay. Representation learning for grounded spatial reasoning. TACL, 2018.\n[Johnson et al.2016] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell.\n[Johnson et al.2017] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017.\n[Kollar et al.2010] Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas Roy. Toward understanding natural language directions. In HRI, 2010.\n[Kostka et al.2017] B. Kostka, J. Kwiecieli, J. Kowalski, and P. Rychlikowski. Text-based adventures of the golovin AI agent. In Conference on Computational Intelligence and Games (CIG), 2017.\n[Kuhlmann et al.2004] Gregory Kuhlmann, Peter Stone, Raymond Mooney, and Jude Shavlik. Guiding a Reinforcement Learner with Natural Language Advice: Initial Results in RoboCup Soccer. 2004.\n[MacGlashan et al.2015] James MacGlashan, Monica Babes-Vroman, Marie desJardins, Michael L. Littman, Smaranda Muresan, Shawn Squire, Stefanie Tellex, Dilip Arumugam, and Lei Yang. Grounding english commands to reward functions. In Robotics: Science and Systems XI, 2015.\n[MacMahon et al.2006] Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. Walk the talk: Connecting language, knowledge, and action in route instructions. In AAAI, 2006.\n[Massiceti et al.2018] Daniela Massiceti, N Siddharth, Puneet K Dokania, and Philip HS Torr. Flipdial: A generative model for two-way visual dialogue. In CVPR, 2018.\n[Mei et al.2016] Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to Action Sequences. AAAI, 2016.\n[Mey1993] J. Mey. Pragmatics: An Introduction. Blackwell, 1993.\n[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. NIPS, 2013.\n[Misra et al.2017] Dipendra Misra, John Langford, and Yoav Artzi. Mapping Instructions and Visual Observations to Actions with Reinforcement Learning. EMNLP, 2017.\n[Narasimhan et al.2015] Karthik Narasimhan, Tejas D. Kulkarni, and Regina Barzilay. Language understanding for text-based games using deep reinforcement learning. In EMNLP, 2015.\n[Narasimhan et al.2018] Karthik Narasimhan, Regina Barzilay, and Tommi Jaakkola. Grounding Language for Transfer in Deep Reinforcement Learning. JAIR, 2018.\n[Oh et al.2017] Junhyuk Oh, Satinder P. Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. In ICML, 2017.\n[Osa et al.2018] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. An algorithmic perspective on imitation learning. Foundations and Trends\u00ae in Robotics, 7(1-2):1\u2013179, 2018.\n[Peters et al.2018a] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In NAACL, 2018.\n[Peters et al.2018b] Matthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual word embeddings: Architecture and representation. In EMNLP, 2018.\n[Radford et al.2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\n[Shu et al.2018] Tianmin Shu, Caiming Xiong, and Richard Socher. Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning. ICLR, 2018.\n[Shusterman et al.2011] Anna Shusterman, Sang Ah Lee, and Elizabeth Spelke. Cognitive effects of language on human navigation. Cognition, 2011.\n[Silver et al.2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go without human knowledge. Nature, 2017.\n[Singh et al.2002] Satinder Singh, Diane Litman, Michael Kearns, and Marilyn Walker. Optimizing dialogue management with reinforcement learning: Experiments with the njfun system. JAIR, 2002.\n[Socher et al.2013] Richard Socher, Milind Ganjoo, Christopher D. Manning, and Andrew Y. Ng. Zero-shot Learning Through Cross-modal Transfer. In NIPS, 2013.\n[Spelke and Kinzler2007] Elizabeth Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 2007.\n[Sutton and Barto2018] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n[Tellex et al.2011] Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R Walter, Ashis Gopal Banerjee, Seth Teller, and Nicholas Roy. Understanding natural language commands for robotic navigation and mobile manipulation. In AAAI, 2011.\n[Tenney et al.2019] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Sam Bowman, Dipanjan Das, and Ellie Pavlick. What do you learn from context<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 21411,
        "passage": " abs/1810.0.\nAllyson Ettinger. 2019. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models.\nJerry A. Fodor and Zenon W. Pylyshyn. 1988. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3\u201371.\nZachary S L Foster, Thomas J Sharpton, and Niklaus J Grunwald. 2017. Metacoder: An {R} package for visualization and manipulation of community taxonomic diversity data. PLoS Computational Biology, 13(2).\nYoav Goldberg. 2019. Assessing BERT\u2019s Syntactic Abilities.\nChristoph Goller and Andreas Kuechler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In IEEE International Conference on Neural Networks - Conference Proceedings, volume 1, pages 347\u2013352. IEEE.\nJohn Hewitt and Christopher D Manning. 2019. {A} Structural Probe for Finding Syntax in Word Representations. In Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129\u20134138, Minneapolis, Minnesota. Association for Computational Linguistics.\nAlon Jacovi, Oren Sar Shalom, and Yoav Goldberg. 2018. Understanding Convolutional Neural Networks for Text Classification. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 56\u201365, Stroudsburg, PA, USA. Association for Computational Linguistics.\nGanesh Jawahar,, Beno\u0131t Sagot,, and Djame Seddah. 2019. What Does BERT Learn about the Structure of Language? In Proceedings of the Conference of the Association for Computational Linguistics, pages 3651\u20133657. Association for Computational Linguistics (ACL).\nW Johnson and J Lindenstrauss. 1984. Extensions of Lipschitz mappings into a Hilbert space. Contemp. Math., 26:189\u2013206.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2015. Skip-Thought Vectors. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201915, page 3294\u20133302, Cambridge, MA, USA. MIT Press.\nAran Komatsuzaki. 2019. One Epoch Is All You Need. pages 1\u201313.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the Dark Secrets of BERT.\nAdhiguna Kuncoro, Lingpeng Kong, Daniel Fried, Dani Yogatama, Laura Rimell, Chris Dyer, and Phil Blunsom. 2020. Syntactic Structure Distillation Pretraining For Bidirectional Encoders.\nZachary C. Lipton. 2016. The Mythos of Model Interpretability. ICML Workshop on Human Interpretability in Machine Learning, 61(Whi):36\u201343.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach. CoRR, abs/1907.1.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. 7th International Conference on Learning Representations, ICLR 2019.\nChristopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. The {S}tanford {C}ore{NLP} Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55\u201360, Baltimore, Maryland. Association for Computational Linguistics.\nDavid Marecek and Rudolf Rosa. 2019. Extracting Syntactic Trees from Transformer Encoder SelfAttentions. pages 347\u2013349.\nJeff Mitchell and Mirella Lapata. 2008. Vector-based Models of Semantic Composition. In Proceedings of ACL-08: HLT, pages 236\u2013244, Columbus, Ohio. Association for Computational Linguistics.\nAlessandro Moschitti. 2006. Making Tree Kernels practical for Natural Language Learning. In Proceedings of EACL\u201906. Trento, Italy.\nTsendsuren Munkhdalai and Hong Yu. 2017. Neural Tree Indexers for Text Understanding. In Proceedings of the conference of the Association for Computational Linguistics, volume 1, pages 11\u201321. NIH Public Access.\nDaniele Pighin and Alessandro Moschitti. 2010. On Reverse Feature Engineering of Syntactic Tree Kernels. In Conference on Natural Language Learning (CoNLL-2010), Uppsala, Sweden.\nT A Plate. 1995. Holographic reduced representations. IEEE Transactions on Neural Networks, 6(3):623\u2013 641.\nJordan B. Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46(1-2):77\u2013105.\nAndrea Santilli and Fabio Massimo Zanzotto. 2018. SyntNN at SemEval-2018 Task 2: is Syntax Useful for Emoji Prediction? Embedding Syntactic Trees in Multi Layer Perceptrons. In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 477\u2013481, New Orleans, Louisiana. Association for Computational Linguistics (ACL).\nRichard Socher, Cliff Chiung Yu Lin, Andrew Y Ng, and Christopher D Manning. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning, ICML 2011, pages 129\u2013136.\nRichard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, pages 1631\u20131642. Association for Computational Linguistics.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15(56):1929\u20131958.\nEmma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. 2018. Linguistically-informed self-attention for semantic role labeling. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5027\u20135038.\nSandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J Pal. 2018. Learning general purpose distributed sentence representations via large scale multitask learning. In 6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings.\nKai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pages 1556\u20131566, Stroudsburg, PA, USA. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NIPS.\nJesse Vig. 2019. A multiscale visualization of attention in the transformer model. ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of System Demonstrations, pages 37\u201342.\nRichard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference, pages 1201\u20131211.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not not Explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 11\u201320, Stroudsburg, PA, USA. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u2019emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace\u2019s Transformers: State-of-the-art Natural Language Processing. ArXiv, abs/1910<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 21426,
        "passage": "The trend of building ever larger language models has dominated much research in NLP over the last few years. In this talk, I will discuss our recent efforts to (at least partially) answer two key questions in this area: Will we be able to keep scaling? And, how will we actually use the models, if we do? I will cover our recent efforts on learning new types of sparse mixtures of experts (MoEs) models. Unlike model-parallel algorithms for learning dense models, which are very difficult to further scale with existing hardware, our sparse approaches have significantly reduced cross-node communication costs and could possibly provide the next big leap in performance, although finding a version that scales well in practice remains an open challenge. I will also present our recent work on prompting language models that better controls for surface form variation, to improve performance of models that are so big we can only afford to do inference, with little to no task-specific fine tuning. Finally, time permitting, I will discuss work on new forms of supervision for language model training, including learning from the hypertext and multi-modal structure of web pages to provide new signals for both learning and prompting the model. Together, these methods present our best guesses for how to keep the scaling trend alive as we move forward to the next generation of NLP models.\nThis talk describes work done at the University of Washington and Meta, primarily led by Armen Aghajanyan, Suchin Gururangan, Ari Holtzmann, Mike Lewis, Margaret Li, Sewon Min, and Peter West.\nLuke Zettlemoyer is a Professor in the Paul G. Allen School of Computer Science & Engineering at the University of Washington, and a Research Director at Meta. His research focuses on empirical methods for natural language semantics, and involves designing machine learning algorithms, introducing new tasks and datasets, and, most recently, studying how to best develop self-supervision signals for pre-training. His honors include being named an ACL Fellow as well as winning a PECASE award, an Allen Distinguished Investigator award, and multiple best paper awards. Luke received his PhD from MIT and was a postdoc at the University of Edinburgh.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 21432,
        "passage": " texts when necessary.\nFurther extensions planned for improving the neural difficulty detection models involve several lines of research. First, this study focused almost exclusively on reading exercises for language learners. We need more experiments on studying variations in the link between difficulty and linguistic features with respect to different difficulty assessment needs or the composition of the training datasets. Even within the area of studying language teaching and expressing difficulty via the CEFR levels, different datasets might have different approaches to what constitutes a B1 text, for example. Some texts are also included into a textbook for a specific level not because they fully correspond to a specific level, but because they can be used in other exercises for this level. For example, an authentic interview included into a B1 textbook might contain rare words or more complex grammatical constructions beyond expectations of typical B1 students, while it can be a good basis for a number of exercises for understanding how native speakers express their opinions. From the viewpoint of Machine Learning, an interview of this kind, even if legitimately included in the textbook, acts as noise for training neural prediction models. We need to experiment with various statistical tests to establish how annotation noise can lead to less reliable predictions and how to improve our prediction models (for example, see Paun et al. 2018).\nSecond, there is a rise in research on causal models (for example, Fytas et al. 2021), because when we have a classifier, it is important to know whether this decision has been made for the right reasons, rather than because of mere correlations in our training data. Recent causal interaction methods can explain some of the issues with interpretation of predictions reported above (Janizek et al. 2021).\nThird, a related line of research involves assessment of the process of mapping CEFR levels of documents to the level of segments. The process of segmentation used in this study can lead to noise, because some 3-sentence segments coming from a textbook of a higher level can still be suitable for students on lower levels. This has already been noticed in the context of using simplified Wikipedia (Vajjala & Meurers 2014). A similar task exists in other areas, for example, turning models which predict the quality of sentence-level translations to models predicting word quality (Zhai et al. 2020).\nFinally, we need to pay more attention to cognitive aspects of difficulty processing beyond simple scores, such as exemplified by the CEFR levels. For example, this involves adding an explicit model for processing named entities (NEs), such as people\u2019s names or locations. Anecdotal experience shows that language learners can often handle NEs, even if they are very rare, either because they are similar to how they are expressed in their native languages (see the example with Lockheed Martin above) or because they can understand the function of a personal name or a location even without knowing this particular entity. This needs to be quantified. NEs are also important in a different way, as neural models can be brittle to NE replacements. For example, replacing NEs in the co-reference task changes 85% of predictions (Balasubramanian et al. 2020).\nThe order of the linguistic features and their codes are taken from (Biber 1988). The conditions for detecting the features for English replicate the published procedures from (Biber 1988), many of them are expressed via lists of lexical items or via POS annotations, which in this study are provided by UDPIPE (Straka & Strakov\u00e1 2017). The Russian features are either based on translating the English word lists or on using identical or functionally similar constructions.\nBaayen, Harald. 2008. Analyzing Linguistic Data. Cambridge University Press, Cambridge.\nBalasubramanian, Sriram, Naman Jain, Gaurav Jindal, Abhijeet Awasthi & Sunita Sarawagi. 2020. What\u2019s in a name? Are BERT named entity representations just as good for any other name? Proceedings of the 5th Workshop on Representation Learning for NLP. Association for Computational Linguistics, Online. 205-214.\nBenko, Vladim\u00edr. 2016. Two years of Aranea: Increasing counts and tuning the pipeline. Proc LREC. Portoro\u017e, Slovenia.\nBiber, Douglas. 1988. Variation Across Speech and Writing. Cambridge University Press.\nBiber, Douglas. 1995. Dimensions of Register Variation: A Cross-Linguistic Comparison. Cambridge University Press.\nCollins-Thompson, Kevyn. 2014. Computational assessment of text readability: A survey of current and future research. International Journal of Applied Linguistics 165(2). 97-135.\nCollins-Thompson, Kevyn & Jamie Callan. 2004. A language modeling approach to predicting reading difficulty. Proc. of HLT/NAACL. Boston. 193-200.\nConneau, Alexis, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer & Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.\nDebnath, Alok & Michael Roth. 2021. A computational analysis of vagueness in revisions of instructional texts. Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop. Association for Computational Linguistics, Online. 30-35.\nDoughty, Catherine, J. Michael & H. Long. 2008. The Handbook of Second Language Acquisition 27. John Wiley & Sons.\nDuBay, William H. 2004. The Principles of Readability. Technical report, Impact Information.\nFytas, Panagiotis, Georgios Rizos & Lucia Specia. 2021. What makes a scientific paper be accepted for publication? Proceedings of the First Workshop on Causal Inference and NLP. Association for Computational Linguistics, Punta Cana, Dominican Republic. 44-60.\nHalliday, M.A.K. 1992. Language as system and language as instance: The corpus as a theoretical construct. In J. Svartvik (ed.), Directions in corpus linguistics: Proceedings of Nobel Symposium 82 Stockholm 65, 61-77. Walter de Gruyter.\nHosmer Jr, David W., Stanley Lemeshow & Rodney X. Sturdivant. 2013. Applied Logistic Regression. John Wiley & Sons.\nJanizek, Joseph D., Pascal Sturmfels & Su-In Lee. 2021. Explaining explanations: Axiomatic feature interactions for deep networks. Journal of Machine Learning Research 22(104). 1-54.\nJuilland, Alphonse. 1964. Frequency Dictionary of Spanish Words. Mouton.\nK\u00e4ding, Friedrich Wilhelm (ed.). 1897. H\u00e4ufigkeitsw\u00f6rterbuch der Deutschen Sprache. Selbstverlag.\nKhallaf, Nouran & Serge Sharoff. 2021. Automatic difficulty classification of Arabic sentences. Proceedings of the Sixth Arabic Natural Language Processing Workshop. Association for Computational Linguistics, Kyiv, Ukraine (Virtual). 105-114.\nKunilovskaya, Maria & Ekaterina Lapshinova-Koltunski. 2019. Translationese features as indicators of quality in English-Russian human translation. Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019). Incoma Ltd., Shoumen, Bulgaria, Varna, Bulgaria. 47-56.\nLaposhina, Antonina N., Tatyana Veselovskaya, Maria Lebedeva & Olga Kupreshchenko. 2018. Automated text readability assessment for Russian second language learners. Computational Linguistics and Intellectual Technologies: Proceedings of the International Conference \u201cDialogue\u2019\u2019.\nLorge, Irving. 1944. Predicting readability. Teachers College Record.\nNadeem, Farah & Mari Ostendorf. 2018. Estimating linguistic complexity for science texts. Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics, New Orleans, Louisiana. 45-55.\nCouncil of Europe. 2001. Common European Framework of Reference for Languages: Learning, Teaching, Assessment (CEFR). Technical report, Council of Europe, Strasbourg.\nOrlov, Jurij. 1983. Ein modell der h\u00e4ufigkeitsstruktur des vokabulars. In H. Guiter & M. Arapov (eds.), Studies on Zipf\u2019s law, 154-233.\nPaun, Silviu, Bob Carpenter, Jon Chamberlain, Dirk Hovy, Udo Kruschwitz & Massimo Poesio. 2018. Comparing Bayesian models of annotation. Transactions of the Association for Computational Linguistics 6. 571-585.\nPitler, Emily & Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. Proc EMNLP. 186-195.\nRogers, Anna, Olga Kovaleva & Anna Rumshisky. 2020. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics 8. 842-866.\nSharoff, Serge. 2021. Genre annotation for the web: Text-external and text-internal perspectives. Register Studies 3. 1-32.\nSharoff, Serge, Svitlana Kurella, & Anthony Hartley. 2008. Seeking needles in the Web haystack: Finding texts<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 21627,
        "passage": ". Learning attitudes and attributes from multi-aspect reviews. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 1020\u20131025. IEEE.\nNalisnick and Smyth (2016) Eric Nalisnick and Padhraic Smyth. 2016. Stick-breaking variational autoencoders. arXiv preprint arXiv:1605.06197.\nNiculae and Blondel (2017) Vlad Niculae and Mathieu Blondel. 2017. A regularized framework for sparse and structured neural attention. In Advances in Neural Information Processing Systems, pages 3338\u20133348.\nNiculae et al. (2018) Vlad Niculae, Andr\u00e9 F. T. Martins, and Claire Cardie. 2018. Towards dynamic computation graphs via sparse latent structure. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 905\u2013911. Association for Computational Linguistics.\nParikh et al. (2016) Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249\u20132255. Association for Computational Linguistics.\nPeng et al. (2018) Hao Peng, Sam Thomson, and Noah A. Smith. 2018. Backpropagating through structured argmax using a spigot. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1863\u20131873. Association for Computational Linguistics.\nPeters et al. (2018) Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227\u20132237. Association for Computational Linguistics.\nRezende et al. (2014) Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1278\u20131286, Bejing, China. PMLR.\nRibeiro et al. (2016) Marco Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \u201cwhy should i trust you?\u201d: Explaining the predictions of any classifier. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 97\u2013101. Association for Computational Linguistics.\nRolfe (2017) Jason Tyler Rolfe. 2017. Discrete variational autoencoders. In ICLR.\nSocci et al. (1998) Nicholas D. Socci, Daniel D. Lee, and H. Sebastian Seung. 1998. The rectified gaussian distribution. In M. I. Jordan, M. J. Kearns, and S. A. Solla, editors, Advances in Neural Information Processing Systems 10, pages 350\u2013356. MIT Press.\nSocher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642. Association for Computational Linguistics.\nThrun (1995) Sebastian Thrun. 1995. Extracting rules from artificial neural networks with distributed representations. In Advances in neural information processing systems, pages 505\u2013512.\nTitov and McDonald (2008) Ivan Titov and Ryan McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In Proceedings of ACL.\nWinn and Bishop (2005) John Winn and Christopher M Bishop. 2005. Variational message passing. Journal of Machine Learning Research, 6(Apr):661\u2013694.\nZaidan and Eisner (2008) Omar Zaidan and Jason Eisner. 2008. Modeling annotators: A generative approach to learning from annotator rationales. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 31\u201340, Honolulu, Hawaii. Association for Computational Linguistics.\nZaidan et al. (2007) Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using \u201cannotator rationales\u201d to improve machine learning for text categorization. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 260\u2013267. Association for Computational Linguistics.\nZhang et al. (2016) Ye Zhang, Iain Marshall, and Byron C. Wallace. 2016. Rationale-augmented convolutional neural networks for text classification. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 795\u2013804, Austin, Texas. Association for Computational Linguistics.\nFigure 7: Kuma plots for various (a, b) parameters.\nWe can generalise the support of a Kumaraswamy variable by specifying two constants l<r and transforming a random variable K\u223cKuma(a,b) to obtain T\u223cKuma(a,b,l,r) as shown in (20, left).\nwhere r\u2212l>0 by definition. This affine transformation leaves the cdf unchanged, i.e.\nThus we can obtain samples from this generalised-support Kumaraswamy by sampling from a uniform distribution U(0,1), applying the inverse transform (19), then shifting and scaling the sample according to (20, left).\nis the probability of drawing a continuous value in (0,1). Note that we used the result in (22) to express these probabilities in terms of the tractable cdf of the original Kumaraswamy variable.\nby chain rule. The term \\pdvLh depends on a differentiable observation model and poses no challenge; the term \\pdvht is the derivative of the hard-sigmoid function, which is 0 for t<0 or t>1, 1 for 0<t<1, and undefined for t\u2208{0,1}; the term \\pdvtk=r\u2212l follows directly from (20, left); the term \\pdvku=\\pdvuF\u22121K(u;a,b) depends on the Kumaraswamy inverse cdf (19) and also poses no challenge. Thus the only two discontinuities happen for t\u2208{0,1}, which is a 0 measure set under the stretched Kumaraswamy: we say this reparameterisation is differentiable almost everywhere, a useful property which essentially circumvents the discontinuity points of the rectifier.\nFigure 8 plots the pdf of the HardKumaraswamy for various a and b parameters. Figure 9 does the same but with the cdf.\nFigure 8: HardKuma pdf for various (a, b).\nFigure 9: HardKuma cdf for various (a, b).\nOur hyperparameters are taken from Lei et al. (2016) and listed in Table 4. The pre-trained word embeddings and data sets are available online at http://people.csail.mit.edu/taolei/beer/. We train for 100 epochs and select the best models based on validation loss. For the MSE trade-off experiments on all aspects combined, we train for a maximum of 50 epochs.\nTable 4: Beer hyperparameters.\nFor the Bernoulli baselines we vary L0 weight \u03bb1 among {0.0002,0.0003,0.0004}, just as in the original paper. We set the fused lasso (coherence) weight \u03bb2 to 2\u2217\u03bb1.\nFor the HardKuma models we set a target selection rate to the values targeted in Table 2, and optimize to this end using the Lagrange multiplier. We chose the fused lasso weight from {0.0001,0.0002,0.0003,0.0004}.\nas a function of a sampled prefix, and the shape parameters ai,bi=gi(x,z<i;\u03d5) are predicted in sequence.\nFor sentiment classification we make use of the PyTorch bidirectional LSTM module for encoding sentences, for both the rationale extractor and the classifier. The BiLSTM final states are concatenated, after which a linear layer followed by a softmax produces the prediction. Hyperparameters are listed in Table 5. We apply dropout to the embeddings and to the input of the output layer.\nTable 5: SST hyperparameters.\nOur hyperparameters are taken from Parikh et al. (2016) and listed in Table 6. Different from Parikh et al. is that we use Adam as the optimizer and a batch size of 64. Word embeddings are projected to 200 dimensions with a trained linear layer. Unknown words are mapped to 100 unknown word classes based on the MD5 hash function, just as in Parikh et al. (2016), and unknown word vectors are randomly initialized. We train for 100 epochs, evaluate every 1000 updates, and select the best model based on validation loss. Figure 10 shows a correct and incorrect example with HardKuma attention for each relation type (entailment, contradiction, neutral).\n<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 21651,
        "passage": " incorrect predictions on the original.\nAs we can see in these examples, by testing systems on minimal pair test items such as these we have the potential to zero in on the linguistic phenomena that any given system can and cannot handle. It is also clear that it is specifically when a system \u201cbreaks\u201d (makes a correct prediction on one but not the other item), and when the change in the pair is targeted enough, that we are able to draw straightforward conclusions. For instance, OSU pair 1a/b allows us to conclude that inferring the positive effect of the phrase just [\u2026] enough on a previously negative context is beyond the systems\u2019 capacities. On the other hand, the more diffuse changes in Melbourne pair 2a/b make it more difficult to determine the precise cause of a system breaking in one direction or the other.\nOf course, to be more confident about our conclusions, we would want to analyze system predictions on multiple different pairs that target the same linguistic phenomenon. This can be a goal for future iterations and analyses.\nTable 3: Sample minimal pairs: Examples of minimal pairs created by different breaker teams with the minimal changes highlighted. \u2018Label\u2019 is the label provided to the pairs by the breaker teams.\nTable 4: Sample minimal pair predictions: Builder system predictions on the example minimal pairs from Table 3. \u2018True Label\u2019 is the label provided to the pairs by the breaker teams.\nA variety of lessons came out of the shared task, which can be helpful for future iterations or future shared tasks of this type. We describe some of these lessons here.\nThe choice of NLP task is an important one. While QA-SRL is a promising task in terms of requiring linguistic robustness, it yielded lower participation than sentiment analysis. Strategies for encouraging buy-in from both builders and breakers will be important. One strategy would be to team up with existing shared tasks, to which we could add a breaking phase.\nWhile going through the labels assigned to the minimal pairs by breaker teams, we find some label choices to be questionable. Since unreliable labels will skew the assessment of builder performance, in future iterations there should be an additional phase in which we validate breaker labels with an external source (e.g., crowd-sourcing). To minimize cost and time, this could be done only for examples that are \u201ccontested\u201d by either builders or other breakers.\nThe notion of a \u201cminimal pair\u201d is critical to this task, so it is important that we define the notion clearly, and that we ensure that submitted pairs conform to this definition. Reviewing breaker submissions, we find that in some cases breakers have significantly changed the sentence, in ways that may not conform to our original expectations. In future iterations, it will be important to have clear and concrete definitions of minimal pair, and it would also be useful to have some external review of the pairs to confirm that they are permissible.\nFor this year\u2019s shared task we chose to limit breakers by requiring them to draw from existing data for creating their pairs. A potential variation to consider would be allowing breaker teams to create their own sentence pairs from scratch, in addition to drawing from existing sentences (with the restriction that sentences should fall in the specified domain). This greater freedom for breakers may increase the range of linguistic phenomena able to be targeted, and the precision with which breakers can target them.\nFinally, it is important to consider general strategies for encouraging participation. We identify two potential areas for improvement. First, the timeline of this year\u2019s shared task was shorter than would be optimal, which placed an undue burden in particular on builders, who needed to run systems and submit predictions in two different phases. A longer timeline could make participation more feasible. Second, participants may be reluctant to submit work to be broken\u2014to address this, we might consider anonymous system submissions in the future.\nThe First Workshop on Building Linguistically Generalizable NLP systems, and the associated first iteration of the Build It Break It, The Language Edition shared task, allowed us to begin exploring the limits of current NLP systems with respect to specific linguistic phenomena, and to extract lessons to build on in future iterations or future shared tasks of this type. We have described the details and results of the shared task, and discussed lessons to be applied in the future. We are confident that tasks such as this, that emphasize testing the effectiveness of NLP systems in handling of linguistic phenomena beyond the training data distributions, can make significant contributions to improving the robustness and quality of NLP systems as a whole.\nThe authors would like to acknowledge Chris Dyer who, as a panelist at the Workshop on Representation Learning for NLP, made the original observation about the brittleness of NLP systems which led to the conception of the current workshop and shared task. We would also like to thank the UMD Computational Linguistics and Information Processing lab, and the UMD Language Science Center. This work was partially supported by NSF grants NRT-1449815 and IIS-1618193, and an NSF Graduate Research Fellowship to Allyson Ettinger under Grant No. DGE 1322106. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor(s).\nAdi et al. (2016) Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2016. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. arXiv preprint arXiv:1608.04207.\nBender et al. (2011) Emily M. Bender, Dan Flickinger, Stephan Oepen, and Yi Zhang. 2011. Parser evaluation over local and non-local deep dependencies in a large corpus. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 397\u2013408, Edinburgh, Scotland, UK. Association for Computational Linguistics.\nCho (2017) Kyunghyun Cho. 2017. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Workshop on Building Linguistically Generalizable NLP Systems.\nCrain and Nakayama (1987) Stephen Crain and Mineharu Nakayama. 1987. Structure dependence in grammar formation. Language, pages 522\u2013543.\nDe Marneffe et al. (2006) Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449\u2013454. Genoa Italy.\nEttinger et al. (2016) Allyson Ettinger, Ahmed Elgohary, and Philip Resnik. 2016. Probing for semantic evidence of composition by means of simple classification tasks. ACL 2016, page 134.\nHe et al. (2015) Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015. Question-answer driven semantic role labeling: Using natural language to annotate natural language. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 643\u2013653.\nK\u00e1d\u00e1r et al. (2016) \u00c1kos K\u00e1d\u00e1r, Grzegorz Chrupa\u0142a, and Afra Alishahi. 2016. Representation of linguistic form and function in recurrent neural networks. arXiv preprint arXiv:1602.08952.\nKalchbrenner et al. (2014) Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. arXiv preprint arXiv:1404.2188.\nLangford et al. (2007) J Langford, L Li, and A Strehl. 2007. Vowpal wabbit online learning project.\nLegate and Yang (2002) Julie Anne Legate and Charles D Yang. 2002. Empirical re-assessment of stimulus poverty arguments. The Linguistic Review, 18(1-2):151\u2013162.\nLi et al. (2015) Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. 2015. Visualizing and understanding neural models in nlp. arXiv preprint arXiv:1506.01066.\nLi et al. (2017) Yitong Li, Trevor Cohn, and Timothy Baldwin. 2017. Bibi system description: Building with cnns and breaking with deep reinforcement learning. In Proceedings of the Workshop on Building Linguistically Generalizable NLP Systems.\nMahler et al. (2017) Taylor Mahler, Willy Cheung, Micha Elsner, David King, Marie-Catherine de Marneffe, Cory Shain, Symon Stevens-Guille, and Michael White. 2017. Breaking nlp: Using morphosyntax, semantics, pragmatics and world knowledge to fool sentiment analysis systems. In Proceedings of the Workshop on Building Linguistically Generalizable NLP Systems.\nMarelli et al. (2014) Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In Language Resources and Evaluation, pages 216\u2013223.\nPang and Lee (2005) Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 115\u2013124. Association for Computational Linguistics.\nRimell et al. (2009) Laura Rimell, Stephen Clark, and Mark Steedman. 2009. Unbounded dependency<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 21749,
        "passage": "Ann Irvine is a Senior Data Scientist at RedOwl Analytics. She completed her PhD at the Center for Language and Speech Processing and the Computer Science Department at Johns Hopkins University. She was advised by Chris Callison-Burch, and has worked closely with Alex Klementiev and David Yarowsky. She was a Graduate Fellow at the Human Language Technology Center of Excellence. Her thesis was on machine translation for low resource languages and domains.\nCynthia Matuszek is an assistant professor in the Computer Science and Electrical Engineering department at UMBC, after graduating from the University of Washington in Seattle, where she worked with Dieter Fox and Luke Zettlemoyer.\nHer primary research interests are in robotics and natural language processing; in her work, she combine these interests to support research in human-robot interaction, or HRI.\nSmaranda Muresan is a Research Scientist at the Center for Computational Learning Systems (CCLS) at Columbia University and an Adjunct Associate Professor in the Department of Computer Science at Columbia University.\nHer work focuses on data-driven methods for context-dependent language understanding, grammar induction, relational learning and applications in the area of health informatics, computational social science and human-computer instruction.\nJason Eisner is Professor of Computer Science at Johns Hopkins University, where he is also affiliated with the Center for Language and Speech Processing, the Machine Learning Group, the Cognitive Science Department, and the national Center of Excellence in Human Language Technology. His goal is to develop the probabilistic modeling, inference, and learning techniques needed for a unified model of all kinds of linguistic structure.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 21841,
        "passage": "huber. 2006.\nConnectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks.\nHannun et al. (2014) Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and Andrew Y. Ng. 2014. Deep speech: Scaling up end-to-end speech recognition. ArXiv.\nHiggins et al. (2017) Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In Proc. ICLR.\nHinton et al. (2012) G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. 2012. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6).\nHinton et al. (1986) Geoffrey E. Hinton et al. 1986.\nLearning distributed representations of concepts.\nIn Proceedings of the eighth annual conference of the cognitive science society.\nIrie et al. (2019) Kazuki Irie, Rohit Prabhavalkar, Anjuli Kannan, Antoine Bruguier, David Rybach, and Patrick Nguyen. 2019. On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition. In Proc. InterSpeech.\nKarita et al. (2019) Shigeki Karita, Nanxin Chen, Tomoki Hayashi, and other. 2019. A Comparative Study on Transformer vs RNN in Speech Applications. Proc. ASRU.\nKim et al. (2017) Suyoun Kim, Takaaki Hori, and Shinji Watanabe. 2017. Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning. In Proc. ICASSP.\nKingma and Ba (2014) Diederik P Kingma and Jimmy Lei Ba. 2014. Adam: A Method for Stochastic Optimization. In Proc. ICLR.\nKudo and Richardson (2018) Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proc. EMNLP: System Demonstrations.\nMathieu et al. (2016) Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann LeCun. 2016. Disentangling factors of variation in deep representation using adversarial training. In Advances in Neural Information Processing Systems 29.\nMohamed et al. (2019) Abdelrahman Mohamed, Dmytro Okhonko, and Luke Zettlemoyer. 2019. Transformers with convolutional context for ASR. In arXiv preprint arXiv:1904.11660.\nOtt et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proc. NAACL-HLT: Demonstrations.\nPark et al. (2019) Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. 2019.\nSpecAugment: A simple data augmentation method for automatic speech recognition.\nIn Proc. Interspeech.\nPovey et al. (2016) Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah Ghahremani, Vimal Manohar, Xingyu Na, Yiming Wang, and Sanjeev Khudanpur. 2016. Purely sequence-trained neural networks for asr based on lattice-free mmi.\nPurushwalkam et al. (2019) Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, and Marc\u2019Aurelio Ranzato. 2019. Task-driven modular networks for zero-shot compositional learning. CoRR.\nRumelhart et al. (1986) David E. Rumelhart, James L. McClelland, and CORPORATE PDP Research Group, editors. 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations.\nSocher et al. (2013) Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. 2013. Zero-Shot Learning Through Cross-Modal Transfer. In Proc. NeurIPS.\nSwietojanski et al. (2012) Pawel Swietojanski, Arnab Ghoshal, and Steve Renals. 2012. Unsupervised cross-lingual knowledge transfer in DNN-based LVCSR. In Proc. SLT.\nTishby et al. (1999) Naftali Tishby, Fernando C. Pereira, and William Bialek. 1999. The information bottleneck method. In Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing.\nVaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. NeurIPS.\nWatanabe et al. (2018) Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, et al. 2018. ESPnet: End-to-End Speech Processing Toolkit. In Proc. InterSpeech.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 22111,
        "passage": "vertical community connectivity seems to occur predominantly through the largest particles because communities in the largest size fractions showed the highest taxonomic similarity throughout the water column... waters remains poorly understood because spatial surveys focusing in the vertical dimension often describe the communities from each depth without assessing their potential linkages throughout the water column... Statistical analyses and plots were done in R (www.r-project. org) using the vegan (72), simba (73), spaa (74), betapart (75), and Bio-diversityR (76) packages....\n.), managed honeybees and flower diversity affected plant-flower visitor interactions over the whole elevational range distribution of the exotic plant. Location: Italian Alps.... Moreover, we computed the three species-level specialization indices excluding the honeybee from the network.... The three algorithms respectively constraint marginal totals of rows and columns, connectance and both marginal totals and connectance....\nMany graph codes have been designed for distributed memory or external memory.... Nevertheless, most experimental work in the literature report results on much smaller graphs, and the ones for the Hyperlink graph use distributed or external memory.... Acknowledgements We would like to thank the reviewers for their helpful comments. Thanks also to Lin Ma for helpful discussions....\nBiol. 50 N163-73 Online: http://www.ncbi.nlm.nih.gov/pubmed/16030374 Christian, Karger P and Peschke 2018 RBE and related modeling in carbon-ion therapy Phys. Med.... al 2018)....\nSpaas, 1995. Seasonal dynamics and spatial distribution of chydorid cladocerans in relation to chironomid larvae in the sandy littoral zone of an oligo-mesotrophic lake. 299(2): 125-138. 2835.... Computer simulation modelling of buoyancy change in Microcystis. 349(0): 111-117. 1177. Howard-Williams, C., A.-M. Schwarz & V. Reid, 1996....\nThe distribution of the ICER was calculated by dual bootstrap of cost and QALY differences. The % of PCI dominant and dominated was calculated.... Costs were totaled and compared as mean and bootstrap analysis to assess distribution. Survival beyond the trial was assessed from external databases.... The mean age was 73 years and 45% were male....\nSimilar trends exist in special-purpose computing systems, with only up to tens of megabytes of on-chip memory available in most recent AI accelerators.... We propose several solutions including two methods to compress general workloads in CPU caches, and two methods to compress AI workloads in special-purpose computing systems.... In NeurIPS, 2014. [73] Tim Dettmers and Luke Zettlemoyer....\n[73] When it comes to MFs, Schul<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 22130,
        "passage": " working within a proof assistant that enforces consistency between grammatical and semantic types. Seki et al. [84, 83] is the earliest approach we are aware of, using an alternative grammar formalism (HPSG [77] ) to translate natural langauge to first-order logic. Each of these approaches targets only a single logic, and assumes the translation is divorced from any particular use of formal specifications.\n[81] (another established use for categorial grammars beyond the semantic parsing we focus on).\nThere are also other approaches to bringing rigorous formalization closer to natural language, without attempting to capture natural language grammar in a systematic way. Isabelle/HOL\u2019s [69] Isar proof language [94] attempts to make proofs themselves more readable using proof manipulation commands resembling English. This is an example of what is known as controlled natural language [28], a pattern of system development where the input language is a heavily restricted fragment of natural language, usually (though not always) simple enough to enable fully automatic processing, usually by heavily restricting both grammatical constructions and vocabulary. This includes examples like Cramer et al. [22], who have worked on heavily restricted subsets of natural language that address both the specification of lemmas and their proofs, but like Isar do not attempt to capture any general natural language structure, and techniques which focus only on stating formulas in natural language [29]. By contrast, our proposed approach (1) reuses existing proof assistant machinery (typeclasses [86, 85] ) rather than requiring specialized support, and (2) aims to (eventually) permit almost arbitrary natural language grammar once an adequate base lexicon is developed (which can then be directly extended by individual proof developments).\nWe have presented evidence that it is plausible to support natural language specifications in current proof assistants by exploiting existing typeclass machinery, with no additional tooling required. Carried further, this could be useful in many ways. It can reduce the gap between informal and formal specifications, reducing (though not eliminating) trust in the manual formalization of requirements. Potentially non-experts in verification could understand some theorem statements, gaining confidence that a verification result matched their understanding of desired properties. And this could be used in educational contexts to help students learn or check informal-to-formal translations.\nOf course, the details matter as well, and it will take time to realize a prototype that is broadly useful. First and foremost, a rich lexicon is required. As explained earlier, at least the initial lexicon will need to be manually constructed (borrowing grammatical categories from existing lexicons, and filling in the semantics) before it would be fruitful to adapt techniques for learning lexicons. Guiding this effort would require a substantial collection of examples of natural-language descriptions of formal claims, both for prioritizing lexicon growth and for validation that the approach is growing to encompass real direct descriptions of claims. Despite the now-enormous body of formalized proofs of program properties and mathematical results, early efforts in this direction have revealed this is less trivial than it seems. Even popular and classic texts introducing formal specification like Software Foundations [74] and classic texts like Type Theory and Functional Programming [90] have remarkably few crisp natural language statements matching a specific formal statement, instead discussing various needs at length in order to motivate eventual details of the final formalization (which is sensible for expository texts). Reynolds\u2019 classic paper introducing separation logic [82] contains no English-language description of any full invariant involving separating conjunction. We do not necessarily require exemplars of full descriptions in a single sentence; it is common for one specification to imply multiple high-level properties, and we envision one style of use for natural language specifications to be checking that a given verified result implies multiple natural language claims which each cover part of the desired results.\nIt is possible that small differences will be required between standard natural language grammars and those used by this approach, arising from distinctions important to proof assistants but irrelevant to colloquial language. This is already the case, as mentioned, with the indexing of some grammatical categories with the semantic types of referents, following Ranta\u2019s early work on formalizing mathematical prose [80]. This direction offers opportunities to collaborate with linguists working in syntax and compositional semantics [8, 37, 88]. Such collaborations could both help with possible novel linguistic features of \u201csemi-formal\u201d natural language, and offers a setting for applying classical linguistic techniques in a domain where they provide unique value.\nA great deal of work lies ahead, but the potential benefits seem to more than justify further exploration in this direction.\n[1] Lasha Abzianidze, Johannes Bjerva, Kilian Evang, Hessel Haagsma, Rik van Noord, Pierre Ludmann, Duc-Duy Nguyen, and Johan Bos. The parallel meaning bank: Towards a multilingual corpus of translations annotated with compositional meaning representations. In EACL, 2017.\n[2] Kazimierz Ajdukiewicz. Die syntaktische konnexit\u00e4t. studia philosophica, 1: 1\u201327. reprinted in storrs mccall, ed., polish logic 1920\u20131939, 207\u2013231, 1935.\n[3] Bharat Ram Ambati, Tejaswini Deoskar, and Mark Steedman. Hindi ccgbank: A ccg treebank from the hindi dependency treebank. Language Resources and Evaluation, 52(1):67\u2013100, 2018.\n[4] Andrew W Appel. Foundational proof-carrying code. In Proceedings of the 16th Annual IEEE Symposium on Logic in Computer Science, LICS 2001, pages 247\u2013256. IEEE, 2001.\n[5] Yoav Artzi and Luke Zettlemoyer.\nWeakly supervised learning of semantic parsers for mapping instructions to actions.\nTransactions of the Association for Computational Linguistics, 1:49\u201362, 2013.\n[6] Jason Baldridge and Geert-Jan M. Kruijff. Multi-modal combinatory categorial grammar. In Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics - Volume 1, EACL \u201903, pages 211\u2013218, Stroudsburg, PA, USA, 2003. Association for Computational Linguistics. doi:10.3115/1067807.1067836.\n[7] Yehoshua Bar-Hillel. A quasi-arithmetical notation for syntactic description. Language, 29(1):47\u201358, 1953.\n[8] Chris Barker and Pauline Jacobson, editors. Direct compositionality. Oxford University Press, 2007.\n[9] Andrej Bauer, Jason Gross, Peter LeFanu Lumsdaine, Michael Shulman, Matthieu Sozeau, and Bas Spitters. The hott library: a formalization of homotopy type theory in coq. In Proceedings of the 6th ACM SIGPLAN Conference on Certified Programs and Proofs, pages 164\u2013172, 2017.\n[10] Daisuke Bekki. Dependent type semantics: An introduction. In Logic and Interactive Rationality (LIRA) Yearbook 2012, Volume 1, pages 277\u2013300. 2012.\n[11] Daisuke Bekki. Representing anaphora with dependent types. In Logical Aspects of Computational Linguistics - 8th International Conference, LACL 2014, Toulouse, France, June 18-20, 2014. Proceedings, pages 14\u201329, 2014. doi:10.1007/978-3-662-43742-1_2.\n[12] Emily M Bender and Alexander Koller. Climbing towards nlu: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185\u20135198, 2020.\n[13] David A Burke and Kristofer Johannisson. Translating formal software specifications to natural language. In International Conference on Logical Aspects of Computational Linguistics, pages 51\u201366. Springer, 2005.\n[14] Bob Carpenter. Type-logical semantics. MIT press, 1997.\n[15] Bob Carpenter. The turing-completeness of multimodal categorial grammars. JFAK: Essays dedicated to Johan van Benthem on the occasion of his 50th birthday. Institute for Logic, Language, and Computation, University of Amsterdam. Available on CD-ROM at http://turing. wins. uva. nl, 1999.\n[16] Stergios Chatzikyriakidis and Zhaohui Luo. Natural language inference in coq. Journal of Logic, Language, and Information, 23, 2014.\n[17] Stergios Chatzikyriakidis, Zhaohui Luo, et al. Modern perspectives in type-theoretical semantics, volume 98. Springer, 2017.\n[18] Stephen Clark and James R. Curran. Log-linear models for wide-coverage ccg parsing. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, Conference on Empirical Methods on Natural Language Processing \u201903, pages 97\u2013104, Stroudsburg, PA, USA, 2003. Association for Computational Linguistics.\n[19] Stephen Clark and James R. Curran. Wide-coverage efficient statistical parsing with ccg and log-linear models. Computational Linguistics, 33(4):493\u2013552, December 2007.\n[20] Stephen Clark, Julia Hockenmaier, and Mark Steedman. Building deep dependency structures with a wide-coverage ccg parser. In<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 22321,
        "passage": " infused phrase embeddings for named entity resolution. InProceedings of the Eigh-teenth Conference on Computational Natural Lan-guage Learning, pages 78\u201386. Association for Com-putational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep-resentations. InProc. of NAACL.\nAlexander Ratner, Stephen H. Bach, Henry R. Ehren-berg, Jason Alan Fries, Sen Wu, and Christopher R\u00b4e. 2017. Snorkel: Rapid training data creation with weak supervision. CoRR, abs/1711.10160.\nGeorges Rey. 2018. The analytic/synthetic distinction. In Edward N. Zalta, editor, The Stanford Encyclo-pedia of Philosophy, fall 2018 edition. Metaphysics Research Lab, Stanford University.\nBeth M. Sundheim. 1995. Overview of results of the muc-6 evaluation. In Proceedings of the 6th Conference on Message Understanding, MUC6 \u201995, pages 13\u201331, Stroudsburg, PA, USA. Association for Computational Linguistics.\nProceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003.\nGuillaume Wisniewski. 2018. Errator: a tool to help detect annotation errors in the universal dependen-cies project. In Proceedings of the Eleventh In-ternational Conference on Language Resources and Evaluation (LREC-2018), Miyazaki, Japan. Euro-pean Languages Resources Association (ELRA).\nVikas Yadav and Steven Bethard. 2018. A survey on recent advances in named entity recognition from deep learning models. InProceedings of the 27th In-ternational Conference on Computational Linguis-tics, pages 2145\u20132158. Association for Computa-tional Linguistics.\nFlair repository (issue 206 and 390).\nPart-of-speech taggingfrom 97% to 100%: Is it time for some linguistics?\nMuc-6 challenges and data sets.\nUsing Context Inference to Improve Sentence Ordering for Multi document Summariz...\nIncorporating Lexico semantic Heuristics into Coreference Resolution Sieves for...\nCMS Implementation Guide for Quality Reporting Document Architecture Category I...\nNamed entity recognition: challenges in document annotation, gazetteer construct...\nNEREA: Named entity recognition and disambiguation exploiting local document rep...\nAssessment of disease named entity recognition on a corpus of annotated sentence...\nIn domain Context aware Token Embeddings Improve Biomedical Named Entity Recogni...\nTransfer Learning and Sentence Level Features for Named Entity Recognition on Tw...\nSupporting Document-Category Management: An Ontology-based Document Clustering A...<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 22664,
        "passage": ", Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations.\nMarco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014), pages 1\u20138.\nTomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Eleventh annual conference of the international speech communication association.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206\u20135210. IEEE.\nAlvaro Peris and Francisco Casacuberta. 2015. A bidirectional recurrent neural language model for machine translation. Procesamiento del Lenguaje Natural, 55:109\u2013116.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 2227\u20132237.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning representations by backpropagating errors. nature, 323(6088):533\u2013536.\nJulian Salazar, Davis Liang, Toan Q Nguyen, and Katrin Kirchhoff. 2019. Pseudolikelihood reranking with masked language models. arXiv preprint arXiv:1910.14659.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86\u201396.\nYusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. 1999. Byte pair encoding: A text compression scheme that accelerates pattern matching. Technical report, Technical Report DOITR-161, Department of Informatics, Kyushu University.\nJoonbo Shin, Yoonhyung Lee, and Kyomin Jung. 2019. Effective sentence scoring method using bert for speech recognition. In Asian Conference on Machine Learning, pages 1081\u20131093.\nMartin Sundermeyer, Ralf Schluter, and Hermann Ney. 2012. Lstm neural networks for language modeling. In Thirteenth annual conference of the international speech communication association.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104\u20133112.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan Gomez, Stephan Gouws, Llion Jones, \u0141ukasz Kaiser, Nal Kalchbrenner, Niki Parmar, et al. 2018. Tensor2tensor for neural machine translation. In Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers), pages 193\u2013 199.\nAlex Wang and Kyunghyun Cho. 2019. Bert has a mouth, and it must speak: Bert as a markov random field language model. In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, pages 30\u201336.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Glue: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019.\nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, NelsonEnrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al. 2018. Espnet: Endto-end speech processing toolkit. Proc. Interspeech 2018, pages 2207\u20132211.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.\nMatthew D Zeiler. 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.\nFor the input features, we use 80-band Melscale spectrogram derived from the speech signal. The target sequence is processed in 5K caseinsensitive sub-word units created via unigram byte-pair encoding (Shibata et al., 1999). We use an attention-based encoder-decoder model as our acoustic model. The encoder is a 5-layer bidirectional LSTM, and there are bottleneck layers that conduct linear transformation between every LSTM layers. Also, there is a VGG module before the encoder, and it reduces encoding time steps by a quarter through two max-pooling layers. The decoder is 2-layer bidirectional LSTM with location-aware attention mechanism (Chorowski et al., 2015). All the layers have 1024 hidden units. The model is trained with additional CTC objective function because the left-to-right constraint of CTC helps learn alignments between speech-text pairs (Hori et al., 2017).\nTable 5 shows the oracle word error rates (WERs) of the 50-best lists, which are measured assuming that the best sentence is always picked from the candidates. We also include the oracle WERs from the 50-best lists of (Shin et al., 2019).<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 22828,
        "passage": " (2015) only assign numbers to the predicted template, reducing the search space significantly. More recently, Wang et al. (2017) provide a large dataset of Chinese algebra word problems and learn a hybrid model consisting of both retrieval and seq2seq components. The current work extends these approaches by exploring advanced techniques in data-driven solving.\nWe have thoroughly examined data-driven models for automatically solving algebra word problems, including retrieval, classification, and generation techniques. We find that a well-tuned classifier outperforms generation and retrieval on several datasets. One avenue for improving performance is to ensemble different models. However, in light of the error analysis provided, the incorporation of semantic and world knowledge will be necessary to achieve maximal success.\nFeigenbaum et al. (1963) Edward A Feigenbaum, Julian Feldman, et al. 1963. Computers and thought. New York.\n(3) Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Peters, Michael Schmitz, and Luke Zettlemoyer. Allennlp: A deep semantic natural language processing platform.\nHosseini et al. (2014) Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to Solve Arithmetic Word Problems with Verb Categorization. In EMNLP, pages 523\u2013533.\nHuang et al. (2016) Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, and Wei-Ying Ma. 2016. How well do computers solve math word problems? large-scale dataset construction and evaluation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 887\u2013896.\nKlein et al. (2017) Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. 2017. Opennmt: Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810.\nKoncel-Kedziorski et al. (2015) Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Ang. 2015. Parsing Algebraic Word Problems into Equations. TACL, 3:585\u2013597.\nKoncel-Kedziorski et al. (2016) Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. Mawps: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152\u20131157.\nKushman et al. (2014) Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 271\u2013281.\nLin et al. (2017) Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130.\nLiu et al. (2017) Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. 2017. Stochastic answer networks for machine reading comprehension. arXiv preprint arXiv:1712.03556.\nMitra and Baral (2015) Arindam Mitra and Chitta Baral. 2015. Learning to automatically solve logic grid puzzles. In EMNLP.\nMu et al. (2017) Jiaqi Mu, Suma Bhat, and Pramod Viswanath. 2017. Representing sentences as low-rank subspaces. ACL.\nRoy and Roth (2015) Subhro Roy and Dan Roth. 2015. Solving General Arithmetic Word Problems. In EMNLP.\nRoy and Roth (2017) Subhro Roy and Dan Roth. 2017. Unit dependency graph and its application to arithmetic word problem solving. AAAI.\nShi et al. (2015) Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xiaojiang Liu, and Yong Rui. 2015. Automatically Solving Number Word Problems by Semantic Parsing and Reasoning. In EMNLP.\nUpadhyay and Chang (2015) Shyam Upadhyay and Ming-Wei Chang. 2015. Draw: A challenging and diverse algebra word problem set. Technical report, Number MSR-TR-2015-78, Oct. 2015.[Online]. Available: https://www. microsoft. com/en-us/research/wp-content/uploads/2016/02/tech_rep. pdf.\nWang et al. (2018) Lei Wang, Dongxiang Zhang, Lianli Gao, Jingkuan Song, Long Guo, and Heng Tao Shen. 2018.\nMathdqn: Solving arithmetic word problems via deep reinforcement learning.\nWang et al. (2017) Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845\u2013854.\nZhou et al. (2015) Lipu Zhou, Shuaixiang Dai, and Liwei Chen. 2015. Learn to Solve Algebra Word Problems Using Quadratic Programming. In EMNLP.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 22835,
        "passage": " Aware Neural Machine Translation\nKehai Chen, Rui Wang, Masao Utiyama and Eiichiro Sumita\n\nContextual Embeddings: When Are They Worth It?\nSimran Arora, Avner May, Jian Zhang and Christopher R\u00e9\n\nContextual Neural Machine Translation Improves Translation of Cataphoric Pronouns\nKayYen Wong, Sameen Maruf and Gholamreza Haffari\n\nContextualized Sparse Representations for Real-Time Open-Domain Question Answering\nJinhyuk Lee, Minjoon Seo, Hannaneh Hajishirzi and Jaewoo Kang\n\nContextualizing Hate Speech Classifiers with Post-hoc Explanation\nBrendan Kennedy, Xisen Jin, Aida Mostafazadeh Davani, Morteza Dehghani and Xiang Ren\n\nContrastive Self-Supervised Learning for Commonsense Reasoning\nTassilo Klein and Moin Nabi\n\nControlled Crowdsourcing for High-Quality QA-SRL Annotation\nPaul Roit, Ayal Klein, Daniela Stepanov, Jonathan Mamou, Julian Michael, Gabriel Stanovsky, Luke Zettlemoyer and Ido Dagan\n\nConversational Word Embedding for Retrieval-Based Dialog System\nWentao Ma, Yiming Cui, Ting Liu, Dong Wang, Shijin Wang and Guoping Hu\n\nCrawling and Preprocessing Mailing Lists At Scale for Dialog Analysis\nJanek Bevendorff, Khalid Al Khatib, Martin Potthast and Benno Stein\n\nCrossing Variational Autoencoders for Answer Retrieval\nWenhao Yu, Lingfei Wu, Qingkai Zeng, Shu Tao, Yu Deng and Meng Jiang\n\nDeeBERT: Dynamic Early Exiting for Accelerating BERT Inference\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu and Jimmy Lin\n\nDesigning Precise and Robust Dialogue Response Evaluators\nTianyu Zhao, Divesh Lala and Tatsuya Kawahara\n\nDialogue State Tracking with Explicit Slot Connection Modeling\nYawen Ouyang, Moxin Chen, Xinyu Dai, Yinggong Zhao, Shujian Huang and Jiajun Chen\n\nDo Transformers Need Deep Long-Range Memory?\nJack Rae and Ali Razavi\n\nDo you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods\nNing Miao, Yuxuan Song, Hao Zhou and Lei Li\n\nDoes Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation\nBei Li, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu and Changliang Li\n\nDon\u2019t Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction\nZhenkai Wei, Yu Hong, Bowei Zou, Meng Cheng and Jianmin Yao\n\nDscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing\nJiangming Liu, Shay B. Cohen and Mirella Lapata\n\nDynamic Memory Induction Networks for Few-Shot Text Classification\nRuiying Geng, Binhua Li, Yongbin Li, Jian Sun and Xiaodan Zhu\n\nDynamic Sampling Strategies for Multi-Task Reading Comprehension\nAnanth Gottumukkala, Dheeru Dua, Sameer Singh and Matt Gardner\n\nDynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change\nHongfei Xu, Josef van Genabith, Deyi Xiong and Qiuhui Liu\n\nEfficient strategies for hierarchical text classification: external knowledge and auxiliary tasks\nKervy Rivas Rojas, Gina Bustamante, Arturo Oncevay and Marco Antonio Sobrevilla Cabezudo\n\nEmbarrassingly Simple Unsupervised Aspect Extraction\nSt\u00e9phan Tulkens and Andreas van Cranenburgh\n\nEnabling Language Models to Fill in the Blanks\nChris Donahue, Mina Lee and Percy Liang\n\nEncoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction\nMasahiro Kaneko, Masato Mita, Shun Kiyono, Jun Suzuki and Kentaro Inui\n\nENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation\nLifu Tu, Richard Yuanzhe Pang, Sam Wiseman and Kevin Gimpel\n\nEnhancing Machine Translation with Dependency-Aware Self-Attention\nEmanuele Bugliarello and Naoaki Okazaki\n\nEnhancing Pre-trained Chinese Character Representation with Word-aligned Attention\nYanzeng Li, Bowen Yu, Xue Mengge and Tingwen Liu\n\nEnriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing\nDaniel Fern\u00e1ndez-Gonz\u00e1lez and Carlos G\u00f3mez-Rodr\u00edguez\n\nEntity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification\nNianzu Ma, Sahisnu Mazumder, Hao Wang and Bing Liu\n\nEstimating Mutual Information Between Dense Word Embeddings\nVitalii Zhelezniak, Aleksandar Savkov and Nils Hammerla\n\nEvaluating Dialogue Generation Systems via Response Selection\nShiki Sato, Reina Akama, Hiroki Ouchi, Jun Suzuki and Kentaro Inui\n\nEvaluating Robustness to Input Perturbations for Neural Machine Translation\nXing Niu, Prashant Mathur, Georgiana Dinu and Yaser Al-Onaizan\n\nEvery Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks\nYufeng Zhang, Xueli Yu, Zeyu Cui, Shu Wu, Zhongzhen Wen and Liang Wang\n\nExpBERT: Representation Engineering with Natural Language Explanations\nShikhar Murty, Pang Wei Koh and Percy Liang\n\nExploiting Personal Characteristics of Debaters for Predicting Persuasiveness\nKhalid Al Khatib, Michael V\u00f6lske, Shahbaz Syed, Nikolay Kolyada and Benno Stein\n\nExploring Content Selection in Summarization of Novel Chapters\nFaisal Ladhak, Bryan Li, Yaser Al-Onaizan and Kathy McKeown\n\nFact-based Content Weighting for Evaluating Abstractive Summarisation\nXinnuo Xu, Ond\u0159ej Du\u0161ek, Jingyi Li, Verena Rieser and Ioannis Konstas\n\nFatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts\nAgostina Calabrese, Michele Bevilacqua and Roberto Navigli\n\nFew-Shot NLG with Pre-Trained Language Model\nZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu and William Yang Wang\n\nFLAT: Chinese NER Using Flat-Lattice Transformer\nXiaonan Li, Hang Yan, Xipeng Qiu and Xuanjing Huang\n\nGAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples\nDanilo Croce, Giuseppe Castellucci and Roberto Basili\n\nGeometry-aware domain adaptation for unsupervised alignment of word embeddings\nPratik Jawanpuria, Mayank Meghwanshi and Bamdev Mishra\n\nGive Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?\nKobi Leins, Jey Han Lau and Timothy Baldwin\n\nGlyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs\nHong-You Chen, SZ-HAN YU and Shou-de Lin\n\nGPT-too: A language-model-first approach for AMR-to-text generation\nManuel Mager, Ram\u00f3n Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian and Salim Roukos\n\nHow Can We Accelerate Progress Towards Human-like Linguistic Generalization?\nTal Linzen\n\nHypernymy Detection for Low-Resource Languages via Meta Learning\nChanglong Yu, Jialong Han, Haisong Zhang and Wilfred Ng\n\nIdentifying Principals and Accessories in a Complex Case based on the Comprehension of Fact Description\nYakun Hu, Zhunchen Luo and Wenhan Chao\n\nImplicit Discourse Relation Classification: We Need to Talk about Evaluation\nNajoung Kim, Song Feng, Chulaka Gunasekara and Luis Lastras\n\nImproved Speech Representations with Multi-Target Autoregressive Predictive Coding\nYu-An Chung and James Glass\n\nImproving Entity Linking through Semantic Reinforced Entity Embeddings\nFeng Hou, Ruili Wang, Jun He and Yi Zhou\n\nImproving<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 22846,
        "passage": " adhering to Template 1 and Template 2, more than 5% among the lower-ranked hypotheses constitute a template combination such as the example shown in the bottom part of Figure 2. For 60% of the examined inputs, there was at least one such hypothesis resulting from template combination, of which two thirds were actually correct verbalizations of the input.\nSince we found that the models frequently ranked correct hypotheses below hypotheses with content errors, we implemented a simple rule-based reranker based on verbatim matches of attribute values. The reranker assigns an error point to each omission and addition of an attribute value. As can be seen in the final row of Table 7, this simple reranker successfully places correct hypotheses higher up in the ranking, improving the practical usability of the generation model by now offering almost three correct variants for each input among the top five hypotheses on average.\nWe compared word-based and character-based Seq2Seq models for data-to-text NLG on two datasets and analyzed their output diversity. Our main findings are as follows: Overall, Seq2Seq models can learn to verbalize structured inputs in a decent way; their success depends on the extent of the domain and available (clean) training data.\nSecond, in a comparison with texts produced by humans, we saw that neural NLG models can even surpass human performance in terms of automatic evaluation measures. On the one hand, this unveils the ability of the models to extract general patterns from the training data that approximate many reference texts, but on the other hand also once more stresses the limited utility of such measures to evaluate NLG systems.\nThird, in light of the multi-faceted analysis we performed, it is difficult to draw a general conclusion on whether word- or character-based processing is more useful for data-to-text generation. Both models yielded comparable results with respect to automatic evaluation measures. In the manual error analysis, the character-based model performed better on the E2E dataset, whereas the word-based model generated more correct outputs on the WebNLG dataset. Character-based models were found to have a significantly higher output diversity.\nFinally, in a controlled experiment with word-based Seq2Seq models trained on data synthesized from templates, we showed the capability of such models to perfectly reproduce the templates they were trained on. More importantly, models trained on two templates could generalize beyond their training data and come up with novel texts. In future work, we would like to extend this line of research and train more model variants on a higher number of templates.\nAgarwal and Dymetman (2017) Shubham Agarwal and Marc Dymetman. 2017. A surprisingly effective out-of-the-box char2char model on the e2e nlg challenge dataset. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 158\u2013163, Saarbr\u00fccken, Germany.\nAngeli et al. (2010) Gabor Angeli, Percy Liang, and Dan Klein. 2010. A simple domain-independent probabilistic approach to generation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201910, pages 502\u2013512, Stroudsburg, PA, USA.\nBahdanau et al. (2014) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. arXiv e-prints, abs/1409.0473.\nBawden (2017) Rachel Bawden. 2017. Machine translation, it\u2019s a question of style, innit? the case of english tag questions. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, pages 2507\u20132512, Copenhagen, Denmark.\nBird et al. (2009) Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python, 1st edition. O\u2019Reilly Media, Inc.\nCheyer and Guzzoni (2006) Adam Cheyer and Didier Guzzoni. 2006. Method and apparatus for building an intelligent automated assistant. Patent US 11/518,292 (Patent pending).\nCho et al. (2014) Kyunghyun Cho, Bart Van Merri\u00ebnboer, \u00c7a\u011flar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, Doha, Qatar.\nChung et al. (2016) Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. 2016. A character-level decoder without explicit segmentation for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, Volume 1: Long Papers, pages 1693\u20131703, Berlin, Germany.\nDevlin et al. (2015) Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, and Margaret Mitchell. 2015. Language models for image captioning: The quirks and what works. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, Volume 2: Short Papers, pages 100\u2013105, Beijing, China.\nDu\u0161ek and Jurc\u00edcek (2016) Ond\u0159ej Du\u0161ek and Filip Jurc\u00edcek. 2016. Sequence-to-sequence generation for spoken dialogue via deep syntax trees and strings. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, Volume 2: Short Papers, pages 41\u201351, Berlin, Germany.\nFicler and Goldberg (2017) Jessica Ficler and Yoav Goldberg. 2017. Controlling linguistic style aspects in neural language generation. CoRR, abs/1707.02633.\nGardent et al. (2017a) Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017a. Creating training corpora for NLG micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Volume 1: Long Papers, pages 179\u2013188, Vancouver, Canada.\nGardent et al. (2017b) Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017b. The WebNLG Challenge: Generating Text from RDF Data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124\u2013133, Santiago de Compostela, Spain.\nGatt and Krahmer (2018) Albert Gatt and Emiel Krahmer. 2018. Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Journal of Artificial Intelligence Research (JAIR), 61:65\u2013170.\nGoyal et al. (2016) Raghav Goyal, Marc Dymetman, and \u00c9ric Gaussier. 2016. Natural language generation through character-based rnns with finite-state prior knowledge. In Proceedings of the 26th International Conference on Computational Linguistics, COLING 2016, Technical Papers, pages 1083\u20131092, Osaka, Japan.\nGraves (2013) Alex Graves. 2013. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850.\nGu et al. (2016) Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, Volume 1: Long Papers, pages 1631\u20131640, Berlin, Germany.\nHerzig et al. (2017) Jonathan Herzig, Michal Shmueli-Scheuer, Tommy Sandbank, and David Konopnicki. 2017. Neural response generation for customer service based on personality traits. In Proceedings of the 10th International Conference on Natural Language Generation, INLG 2017, pages 252\u2013256, Santiago de Compostela, Spain.\nHochreiter and Schmidhuber (1997) Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8).\nKiddon et al. (2016) Chlo\u00e9 Kiddon, Luke Zettlemoyer, and Yejin Choi. 2016. Globally coherent text generation with neural checklist models. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, pages 329\u2013339, Austin, TX, USA.\nKingma and Ba (2015) Diederik Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations, San Diego, CA, USA.\nKlein et al. (2017) Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. 2017. Opennmt: Open-source toolkit for neural machine translation. CoRR, abs/1701.02810.\nKoehn (2017) Philipp Koehn. 2017. Neural machine translation. CoRR, abs/<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 23150,
        "passage": "aleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4364\u20134373, Hong Kong, China. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations.\nLinqing Liu, Huan Wang, Jimmy Lin, Richard Socher, and Caiming Xiong. 2019a. Attentive student meets multi-task teacher: Improved knowledge distillation for pretrained models.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019b. Linguistic knowledge and transferability of contextual representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1073\u20131094, Minneapolis, Minnesota. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019c. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.\nJ. S. McCarley. 2019. Pruning a bert-based question answering model.\nPaul Michel, Omer Levy, and Graham Neubig. 2019a. Are sixteen heads really better than one? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 14014\u201314024. Curran Associates, Inc.\nPaul Michel, Omer Levy, and Graham Neubig. 2019b. Are sixteen heads really better than one? CoRR, abs/1905.10650.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), New Orleans, Louisiana. Association for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin, Texas. Association for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. 2019. Q-bert: Hessian based ultra low precision quantization of bert.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4322\u20134331, Hong Kong, China. Association for Computational Linguistics.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. \u201dmobilebert: Task-agnostic compression of bert by progressive knowledge transfer\u201d. In International Conference on Learning Representations.\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin. 2019. Distilling task-specific knowledge from BERT into simple neural networks. CoRR, abs/1903.12136.\nElena Voita, Rico Sennrich, and Ivan Titov. 2019a. The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019b. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium. Association for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u2019emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface\u2019s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert: Quantized 8bit bert.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 23253,
        "passage": " four diverse datasets, including both generated and natural questions, and with provided contexts of passages, documents and even passage sets.\nThe span selection task is suitable for pre-training on any domain, since it makes no assumptions about document structure or availability of summary/article pairs.\nThis allows pre-training of language understanding models in a very generalizable way.\nWe review related work in three categories: other efforts to use automatically constructed tasks similar to extractive QA, research towards adding new pre-training tasks, and work that extends the pre-training with more data.\nPrevious work has explored tasks similar to span selection pre-training. These are typically cast as approaches to augment the training data for question answering systems, rather than alleviating the need for encoding world knowledge in a language model or general pre-training for language understanding.\nHermann et al [2015] introduces a reading comprehension task constructed automatically from news articles with summaries. In this view the constructed dataset is used both for training and test. Also, entities were replaced with anonymized markers to limit the influence of world knowledge. Unlike our span selection pre-training task, this requires summaries paired with articles and focuses only on entities. A similar approach was taken by Dhingra et al [2018] to augment training data for question answering. Wikipedia articles were divided into introduction and body with sentences from the introduction used to construct queries for the body passage. Phrases and entities are used as possible answer terms.\nChris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. Synthetic QA corpora generation with roundtrip consistency. CoRR, abs/1906.05416, 2019a. URL http://arxiv.org/abs/1906.05416.\nChris Alberti, Kenton Lee, and Michael Collins. A bert baseline for the natural questions, 2019b. URL https://arxiv.org/abs/1901.08634v2.\nRie Kubota Ando and Tong Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817\u20131853, 2005.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. URL https://arxiv.org/pdf/1810.04805.pdf.\nBhuwan Dhingra, Danish Danish, and Dheeraj Rajagopal. Simple and effective semi-supervised question answering. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), volume 2, pages 582\u2013587, 2018. URL https://aclweb.org/anthology/N18-2092.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1693\u20131701. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf.\nJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328\u2013339. Association for Computational Linguistics, 2018. URL http://aclweb.org/anthology/P18-1031.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. arXiv preprint arXiv:1907.10529, 2019.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. TACL, 201URL https://tomkwiat.users.x20web.corp.google.com/papers/natural-questions/main-1455-kwiatkowski.pdf.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.\nTakeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, and David McAllester. Who did what: A large-scale person-centered cloze dataset. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2230\u20132235, 2016. URL https://aclweb.org/anthology/D16-1241.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227\u20132237. Association for Computational Linguistics, 2018. doi: 10.18653/v1/N18-1202. URL http://aclweb.org/anthology/N18-1202.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Preprint, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, 2016. URL https://aclweb.org/anthology/D16-1264.\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.\nStephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333\u2013389, April 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL http://dx.doi.org/10.1561/1500000019.\nMrinmaya Sachan and Eric Xing. Self-training for jointly learning to ask and answer questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 629\u2013640. Association for Computational Linguistics, 2018. doi: 10.18653/v1/N18-1058. URL http://aclweb.org/anthology/N18-1058.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. CoRR, abs/1701.06538, 2017. URL http://arxiv.org/abs/1701.06538.\nLeslie N. Smith. A disciplined approach to neural network hyper-parameters: Part 1 \u2013 learning rate, batch size, momentum, and weight decay, 2018.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. Ernie 2.0: A continual pre-training framework for language understanding. arXiv preprint arXiv:1907.12412, 2019.\nWilson L Taylor. \u201cCloze procedure\u201d: A new tool for measuring readability. Journalism Bulletin, 30(4):415\u2013433, 1953.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998\u20136008. Curran Associates, Inc., 2017. URL http://papers.nips.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 23261,
        "passage": " I am investigating generating coherent document-level text from meaning representations, programming language code and multiple documents, as well as fact-based text summaries. I am also interested in modeling open-domain dialogue, with an emphasis on NLG, as well as grounded language acquisition through visual perception and language interaction.\nI was also faculty advisor for Team Alana, HWU entry to Alexa Prize Challenge 2018 that finished 3rd (out of ~200 participants).\nIf you are interested in doing a PhD have a look here.\nI continuously enjoy investigating new research fi\felds, hence I have also acquired skills in Psycholinguistically-Motivated Parsing and Semantics, as well as Information Retrieval from my past employments.\nI was a postdoctoral researcher in the department of Computer Science & Engineering, at the University of Washington, working with Luke Zettlemoyer, and Yejin Choi, between 2015 ans 2017.\nI have also worked as a research associate (post-doc) at the University of Edinburgh, with Frank Keller and Mirella Lapata, on the SynSem EPSRC project, aiming to integrate semantics into a full syntactic parser.\nI completed my PhD in 2013, 'Joint Models for Concept-to-text Generation', at the University of Edinburgh, supervised by Mirella Lapata.\nFact-acurate Text Summarisation (2020-20). The Royal Society.\nF28ED - User-Centred Experimental Design, Semester 1, 2019-20, Heriot-Watt University.\nF29AI - Artificial Intelligence and Intellgience Agents (2nd half), Semester 1, 2017-18, Heriot-Watt University.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 23778,
        "passage": " is hiring five well-regarded computer scientists in the U.S. and Europe, adding new facilities to bolster its artificial intelligence research division, and focus on robotics and related-technologies.\nThe company, which is in a race with other large technology firms to push the boundaries of artificial intelligence and apply these breakthroughs to its products, said it was hiring researchers in Menlo Park, California, where it is headquartered, as well as in Pittsburgh, Seattle and London.\nFacebook\u2019s AI Research division (FAIR) is hiring Jessica Hodgins and Abhinav Gupta, both currently professors at Carnegie Mellon University in Pittsburgh, to head a new lab in the city focusing on robotics.\nFacebook has not been at the forefront of robotic research. However Yann LeCun, Facebook\u2019s chief AI scientist, said having a robotics program was essential for recruiting the most promising young scientists and engineers to Facebook. \"We can\u2019t attract other researchers without having research in this area,\" he said.\nResearchers at Facebook have recently created a program aimed at AI being able to a find objects within your house. This year the tech company has also patented a self-balancing robot. LeCun said Facebook currently uses a few robots to help maintain some of its data centers.\nFacebook\u2019s more pressing challenge is using AI to help manage the vast amount of content on its platforms. While many technology companies, including Alphabet Inc., Twitter Inc., Amazon.com Inc. and Salesforce.com Inc., have been working on getting machines to better understand language, improvements in this area are considered particularly important for Facebook, where the proliferation of false news and extremist propaganda on its social network have provoked popular and government backlash against it.\nChief Executive Officer Mark Zuckerberg has told the U.S. Congress that eventually artificial intelligence should be able to help police such content, but today machine\u2019s ability to understand natural language -- and its context -- are too limited.\nThe company said it was hiring Luke Zettlemoyer, an expert on natural language processing at the University of Washington, to join its AI research team in Seattle. It also said has recruited two computer vision researchers, Jitendra Malik, a well-known scientist from the University of California, Berkeley, who has joined Facebook to head up its AI research efforts in Menlo Park, and Andrea Vedaldi, an associate professor at the University of Oxford, who will join Facebook\u2019s London AI research team.\nLeCun said Vedaldi would work alongside a group of researchers working on natural language processing that Facebook acquired last month through the purchase of their London-based startup Bloomsbury AI. The London and Pittsburgh offices add to FAIR\u2019s five existing labs in Menlo Park, New York City, Seattle, Montreal and Paris.\nThe intensity of the war for AI talent has helped drive salaries for top researchers to record levels, with some of the best known academics able to secure multi-million-dollar, multi-year contracts. In some cases, technology companies have raided entire academic departments, such as when Uber in 2015 hired 50 researchers, or one third of the workforce, from Carnegie Mellon University\u2019s robotics lab in Pittsburgh.\nLeCun said working in robotics was important for exploring ways to make machine learning more efficient and work better for real world applications. \"We would like to figure out how to train machines without having to have them interact with humans for thousands of hours,\" he said. \"We think research in complex robotics will lead to advances there.\"<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 24298,
        "passage": "2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2227\u20132237.\nQian et al. (2017) Qiao Qian, Minlie Huang, Jinhao Lei, and Xiaoyan Zhu. 2017. Linguistically regularized LSTM for sentiment classification. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1679\u20131689.\nQian et al. (2015) Qiao Qian, Bo Tian, Minlie Huang, Yang Liu, Xuan Zhu, and Xiaoyan Zhu. 2015. Learning tag embeddings and tag-specific composition functions in recursive neural network. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, pages 1365\u20131374.\nRadford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. In Technical Report.\nRadford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. In Technical Report.\nRajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392.\nSoares et al. (2019) Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for relation learning. In Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 2895\u20132905.\nSocher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642.\nSong et al. (2019) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. MASS: masked sequence to sequence pre-training for language generation. In Proceedings of the 36th International Conference on Machine Learning, pages 5926\u20135936.\nSun et al. (2019a) Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019a. ERNIE: enhanced representation through knowledge integration. arXiv preprint arXiv: 1904.09223.\nSun et al. (2019b) Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2019b. ERNIE 2.0: A continual pre-training framework for language understanding. arXiv preprint arXiv: 1907.12412.\nTeng et al. (2016) Zhiyang Teng, Duy-Tin Vo, and Yue Zhang. 2016. Context-sensitive lexicon features for neural sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1629\u20131638.\nVaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998\u20136008.\nWang et al. (2019) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations.\nWarstadt and Bowman (2019) Alex Warstadt and Samuel R Bowman. 2019. Grammatical analysis of pretrained sentence encoders with acceptability judgments. arXiv preprint arXiv:1901.03438.\nWilliams et al. (2018) Adina Williams, Nikita Nangia, and Samuel R. Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1112\u20131122.\nWilson et al. (2005) Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 347\u2013354.\nWu et al. (2019) Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, and Songlin Hu. 2019. Conditional BERT contextual augmentation. In 19th International Conference on Computational Science, pages 84\u201395.\nWu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gregory S. Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv: 1609.08144.\nXu et al. (2019) Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2019. BERT post-training for review reading comprehension and aspect-based sentiment analysis. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2324\u20132335.\nYang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv: 1906.08237.\nZhang et al. (2015) Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems, pages 649\u2013657.\nZhang et al. (2019) Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: enhanced language representation with informative entities. In Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 1441\u20131451.\nZhu et al. (2014) Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and Svetlana Kiritchenko. 2014. An empirical study on the effect of negation words on sentiment. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 304\u2013313.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 24371,
        "passage": " additional baseline for future NAT research. AT deep-shallow deteriorates much less on the raw data compared to the iterative NAT methods, suggesting that our strategy of speeding up autoregressive models is better suited to modeling raw, complex data than the NAT methods.\nTable 5: WMT14 EN\u2192DE test results in BLEU that analyze the effects of distillation in fast translation methods. All distillation data are obtained from a transformer large. T denotes the number of iterations.\nRaw Dist. \u2206 CMLM, T = 4 22.3 25.9 3.6 CMLM, T = 10 24.6 27.0 2.4 Imputer, T = 4 24.7 27.9 3.2 Imputer, T = 8 25.0 27.9 2.9 DisCo Enc-6 Dec-6 24.8 27.4 2.6 AT Deep-Shallow (12-1) 26.9 28.3 1.4 AT Enc-6 Dec-6 27.4 28.3 0.9 Table 5 : WMT14 EN\u2192DE test results in BLEU that analyze the effects of distillation in fast translation methods. All distillation data are obtained from a transformer large. T denotes the number of iterations.\nSpeedup and Batch Size When decoding with large mini-batches, NAT models can be slower than their AT counterpart ( \u00a75.1). Here we further study this effect. Fig. 5 plots the relative speedups of different models' decoding with varying numbers of sentences per batch up to the hardware limit (\"max,\" \u00a73.1). The speedup by NAT models diminishes as the batch size grows: they have similar decoding latency to AT 6-6 with batch size 50, and become slower with larger batch sizes. In contrast, AT deep-shallow achieves consistent speedups over the AT 6-6 baseline. Can we reduce the decoder further? We saw that an autoregressive model with a single-layer decoder and a sufficiently deep encoder can retain the accuracy of the baseline with 6 layers each. One may ask whether we can make the decoder even more compact. Our preliminary experiments showed that we can remove the feed-forward module from the decoder (Fig. 1) without hurting performance. This reduces the S 1 latency by 10%. We leave further exploration to future work.\nFigure 5: Relative speedup compared to the standard AT Enc-6 Dec-6 with varying batch size. Evaluated on the WMT14 EN\u2192DE test data.\nNon-autoregressive Translation In addition to the work already discussed, several other works proposed to iteratively refine (or insert) output predictions (Mansimov et al., 2019; Stern et al., 2019; Gu et al., 2019a; Chan et al., 2019a,b;. Other approaches include adding a light autoregressive module to parallel decoding (Kaiser et al., 2018; Sun et al., 2019; Ran et al., 2019), partially decoding autoregressively (Stern et al., 2018 (Stern et al.,, 2019, rescoring output candidates autoregressively (e.g., Gu et al., 2018), mimicking hidden states of an autoregressive teacher, training with different objectives than vanilla cross-entropy (Libovick\u00fd and Helcl, 2018; Shao et al., 2020; Tu et al., 2020; Saharia et al., 2020; Ghazvininejad et al., 2020a), reordering input sentences (Ran et al., 2019), training on additional data from an autoregressive model (Zhou and Keung, 2020), and modeling with latent variables (Ma et al., 2019; Shu et al., 2020). The approach of adding a light autoregressive module is closest to our method, but note that we pack all non-autoregressive computation into the encoder.\nOptimizing Autoregressive Transformer Prior work has suggested ways to optimize autoregressive transformers for fast inference. For example, Kim et al. (2019) employed layer tying (Dabre and Fujita, 2019; Dehghani et al., 2019) on the transformer decoder and found that it sped up inference on CPUs, but not on a GPU. Shi and Knight (2017) proposed a vocabulary reduction method to speed up the last softmax computation. used dynamic programming in an average attention network to accelerate inference. Press and Smith (2018) proposed an eager translation method to avoid attention computation. Reformer (Kitaev et al., 2020) reduced the quadratic complexity of attention computation by locality-sensitive hashing. Some of these methods can be used orthogonally to further facilitate fast inference in a transformer with a deep encoder and shallow decoder.\nRich Encoding, Light Decoding Our experiments suggest that rich features from a deep encoder avoid the need for multiple layers of decoding in machine translation. Wang et al. (2019a) showed that using more encoder transformer layers while keeping 6 decoder layers improves translation quality. Barone et al. (2017) found that RNNbased models with a deep encoder and a shallow decoder can reduce training time with a small performance drop. We took an extreme configuration of a single-layer transformer decoder and focused on inference latency, but all of these results corroborate the benefit of deep encoders. Beyond machine translation, a surprisingly light decoder (e.g., multilayer perceptrons) with a powerful encoder (e.g., bidirectional LSTMs) has proven successful in structured prediction, such as syntactic and semantic parsing (Kiperwasser and Goldberg, 2016; Manning, 2017, 2018; Kasai et al., 2018). Generating a target translation is perhaps a more complex task than producing a parse tree, but our results provide further support for the claim that useful distributed representations of natural language can be obtained in a conditionally independent manner.\nTable 6: Autoregressive translation fairseq hyperparameters and setting.\nTable 7: Non-autoregressive translation fairseq hyperparameters and setting.\nWe presented extensive empirical studies to demonstrate that autoregressive translation can be dramtically sped up by a simple layer allocation strat-egy: deep encoder, shallow decoder. Compared to strong non-autoregressive models, deep-shallow autoregressive models achieve substantial improvement in translation quality with comparable latency. Our results suggest that layer allocation is an important factor that future work on fast machine translation, particularly non-autoregressive machine translation, should take into consideration. More generally, our work suggests that a better layer allocation between the encoder and decoder might be able to accelerate inference in any sequence-to-sequence task. In particular, a model with a deep encoder and a shallow decoder can be used for large-scale pretraining for sequence generation such as BART where latency reduction will be key in a wide range of real-world applications. Table 8 : BLEU and speed comparisons with varying number of encoder (E) and decoder (D) layers.\nTable 8: BLEU and speed comparisons with varying number of encoder (E) and decoder (D) layers.\nLayer normalization(Ba et al., 2016) is applied after attention and feed forward. We suppress this for brevity.\nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer normalization.\nTimothy Dozat and Christopher D. Manning. 2018. Simpler but more accurate semantic dependency parsing. In Proc. of ACL.\nChris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameteriza- tion of ibm model 2. In Proc. of NAACL.\nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translation at scale. In Proc. of EMNLP.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. 2017. Convolutional se- quence to sequence learning. In Proc. of ICML.\nMarjan Ghazvininejad, Vladimir Karpukhin, Luke Zettlemoyer, and Omer Levy. 2020a. Aligned cross entropy for non-autoregressive machine translation.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke S. Zettlemoyer. 2019. Mask-predict: Paral- lel decoding of conditional masked language models. In Proc. of EMNLP.\nMarjan Ghazvininejad, Omer Levy, and Luke Zettle- moyer. 2020b. Semi-autoregressive training im- proves mask-predict decoding.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic- tor O. K. Li, and Richard Socher. 2018. Non- autoregressive neural machine translation. In Proc. of ICLR.\nJiatao Gu, Qi Liu, and Kyunghyun Cho. 2019a. Insertion-based decoding with automatically in- ferred generation order. TACL.\nJiatao Gu, Changhan Wang, and Jake Zhao. 2019b. Levenshtein transformer. In Proc. of NeurIPS.\nAntonio Valerio Miceli Barone, Jind\u0159ich Helcl, Rico Sennrich, Barry Haddow, and Alexandra Birch. 2017. Deep architectures for neural machine trans- lation. In Proc. of WMT.\nMark Harris. 2007. Optimizing parallel reduction in CUDA.\nSepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation.\nHakan Inan, Khashayar Khosravi, and Richard Socher. 2017. Tying<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 24775,
        "passage": "If you have a question about this list, please contact: NLIP Seminars; Andrew Caines; Paula Buttery; James Thorne; Guy Aglionby. If you have a question about a specific talk, click on that talk to find its organiser.\nFrancis Bond, Associate Professor at the Division of Linguistics and Multilingual Studies, Nanyang Technological University, Singapore.\nJan Buys, University of Oxford.\nA special one day workshop.\nAnn Copestake (Cambridge), Aurelie Herbelot (Trento), Diana Maynard (Sheffield), Behrang QasemiZadeh (D\u00fcsseldorf), Nandaja Varma, Esther Seyffarth (D\u00fcsseldorf), Hrishikesh K.B. (Swathanthra Malayalam Computing).\nSeminar Room FW11, Computer Laboratory.\nFelix Sanchez-Garcia, The Guardian.\nOmer Levy, Bar-Ilan University.\nWe only have LT2 for the hour, so please don't enter before 1pm and don't be late for the talk, as it will start promptly at 1:05 and we'll have to leave before 2pm.\nBrian Murphy, Queen's University Belfast.\nFelix Bildhauer, Freie Universit\u00e4t Berlin.\nMaarten de Rijke, University of Amsterdam.\nSW 01, Computer Laboratory, William Gates Builiding.\nEva Maria Vecchi, University of Trento.\nDan Flickinger, CSLI, Stanford University.\nOmri Abend, The Hebrew University of Jerusalem.\nVerena Rieser, Heriot Watt University.\nTamara Polajnar, University of Cambridge.\nDr. C. Lee Giles, Pennsylvania State University.\nGC22, Computer Laboratory.\nJoakim Nivre, Uppsala University.\nAbby Levenberg, University of Oxford.\nHong Yu, University of Wisconsin-Milwaukee.\nShane Bergsma - Johns Hopkins University.\nAnders S\u00f8gaard, University of Copenhagen.\nMohan Ganesalingam (University of Cambridge).\nVarious NLP researchers, University of Cambridge.\nNigel Collier - National Institute of Informatics, Tokyo.\nYue Zhang and Stephen Clark, University of Cambridge.\nChing-Yun Chang and Stephen Clark, University of Cambridge.\nKarin Verspoor - University of Colorado Denver.\nLaura Rimell - University of Cambridge.\nEkaterina Shutova, University of Cambridge.\nArnold Neumaier, University of Vienna.\nCharles Elkan, University of California, San Diego.\nChing-Yun (Frannie) Chang, University of Cambridge.\nMarcus Eichenberger-Liwicki, German Research Center for Artificial Intelligence.\nAnoop Sarkar, Simon Fraser University.\nGerard Steen - Vrije Universiteit Amsterdam.\nKatja Markert - University of Leeds.\nTed Briscoe - University of Cambridge.\nAlexander Clark - Royal Holloway University of London.\nPreslav Nakov - National University of Singapore.\nDanushka Bollegala - University of Tokyo.\nLuke Zettlemoyer, University of Edinburgh.\nPhil Blunsom, University of Oxford.\nDan Jurafsky, Stanford University.\nAdam Lopez, University of Edinburgh.\nTheresa Wilson, University of Edinburgh.\nTrevor Cohn, University of Sheffield.\nHugo Gon\u00e7alo Oliveira, University of Coimbra, Portugal.\nMehrnoosh Sadrzadeh, Oxford University Computing Laboratory.\nLaura Rimell, Computer Laboratory, University of Cambridge.\nRichard Bergmair, Computer Laboratory, University of Cambridge.\nEva Banik, Open University.\nCarlos G\u00f3mez - University of Corunna.\nRoberto Navigli, University of Rome \"La Sapienza\".\nDr. Inderjeet Mani.\nAndreas Vlachos, Cambridge University.\nMark Stevenson - Sheffield University.\nYue Zhang, University of Oxford.\nJeroen Geertzen, University of Cambridge.\nBrian Harrington, University of Oxford.\nJohn Carroll - University of Sussex.\nFrancois Mairesse, Department of Engineering, University of Cambridge.\nLaura Rimell, Oxford University.\nJurgen van Gael, Department of Engineering, University of Cambridge.\nCaroline Gasperin, Computer Laboratory, University of Cambridge.\nDouglas W. Oard, University of Maryland, USA.\nSmall Lecture Theatre, Computer Laboratory.\nFuzzy Logic: Fading Hype or Technology of the Future?\nDr. Ulrich Bodenhofer, Johannes Kepler University, Austria.\nValia Kordoni (LT-Lab DFKI GmbH and Dept. of Computational Linguistics, Saarland University, Germany).\nRachele De Felice, University of Oxford.\nJennifer Foster, Dublin City University.\nDavid Hardcastle, The Open University.\nDiane Litmann, University of Edinburgh.\nDiarmuid O'Seaghdha, Computer Laboratory, University of Cambridge.\nAurelie Herbelot, Computer Laboratory, University of Cambridge.\nDan Flickinger, CSLI Stanford University and Cambridge Computer Laboratory.\nKarsten Borgwardt, University of Cambridge.\nAndrew Clegg, Birkbeck, University of London.\nMartin Szummer, Microsoft Research Cambridge.\nTimothy Baldwin - University of Melbourne..\nDiarmuid O'Seaghdha - University of Cambridge.\nPeter Corbett - University of Cambridge.\nColin Batchelor - Royal Society of Chemistry.\nJohn Carroll - Department of Informatics, University of Sussex.\nSophia Ananiadou - University of Manchester.\nAdvaith Siddharthan, University of Cambridge.\nIelka van der Sluis, University of Aberdeen.\nNikiforos Karamanis, University of Cambridge.\nRichard Berg<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 24805,
        "passage": "). \n#### Machine Translation (`./eval_data/mt/`): \nWe train two MT systems (standard Transformer and finetuned [MBART](https://github.com/pytorch/fairseq/tree/master/examples/mbart)) on the simulated low-resource (patent domain) training data, and evaluate on the patent domain. \nWe ask bilingual speakers to evaluate if machine translations contain hallucinations at token-level on 150 sentences from the patent test set.\nUnder `./eval_data/mt/`, `*source` are raw source sentences, `*target` are model outputs, `*ref` are references, `*label` are annotated labels of `*target`.\n`1` indicates hallucinated words and `0` indicates faithful translation words.\n- `./eval_data/mt/trans2s.*` are annotations for standard Transformer outputs.\n- `./eval_data/mt/mbart.*` are annotations for finetuned MBART outputs.\n#### XSum\nWe processed the annotations released from [google](https://github.com/google-research-datasets/xsum_hallucination_annotations) by aggregating the labels for each word from 3 annotators with majority voting.\nThe aggregated results for four models (BERTSeq2Seq, Pointer-generator, Topic-aware Convolutional Network and standard Transformer Seq2Seq) are under `./eval_data/xsum/`.\n\n## Create Synthetic Data\nTo train a hallucination prediction model on your own bi-text dataset, the first step is creating the synthetic labeled data.\nThis is decomposed into the following two sub-steps.\n- **Generate synthetic target data with BART**\n\n  You can tune the hyperparameters for generating noised data at the top of `./util_scripts/run_gen_synthetic_data_with_bart.sh`, then run the following command.\n  The set of noise hyperparameters will be used to name the output, namely `config`.\n\n  Please first download the BART (for English, [here](https://github.com/pytorch/fairseq/tree/master/examples/bart)) or MBART (for other languages, [here](https://www.dropbox.com/sh/rf1yx5ic1rmprbq/AACPGueHCjJpGj_1VS-I03PYa?dl=0), we noticed that the MBART model released in fairseq is broken) model\nand then specify the path to model and bpe dictionary in `Line 33-45` of `./util_scripts/gen_bart_batch.py`.\n  Then run the following command:\n  ```commandline\n  bash./util_scripts/run_gen_synthetic_data_with_bart.sh path/to/the/target/file path/to/the/valid/file\n  ```\n  e.g.,\n  ```commandline\n  bash util_scripts/run_gen_synthetic_data_with_bart.sh toy_data/train.en toy_data/valid.en\n  ```\n  With the default setting, the noise `config=mask_0.0_0.6_random_0.0_0.3_insert_0.2_wholeword_0`.\n  After this, a new directory `bart_gen` is created under the directory of your input and you will see the output under `bart_gen`.\n \n- **Create pseudo labels and binarize datasets**\n\n  The examples scripts `./util_scripts/make_synthetic_data_mt.sh` and `./util_scripts/make_synthetic_data_xsum.sh` \nare used for pseudo label creation and dataset binarization for machine translation and summarization respectively.\n\n  You need to download the model you will finetune on later and the corresponding dictionaries prior to the following steps.\nTo predict hallucination for a cross-lingual conditional sequence generation task, e.g. MT, you could use [XLM-Roberta](https://github.com/pytorch/fairseq/tree/master/examples/xlmr);\nto predict hallucination for a monolingual conditional sequence generation task, e.g. summarization, you could use [Roberta](https://github.com/pytorch/fairseq/tree/master/examples/roberta).\n\n  These models also come along with the dictionaries and the subword models (sentencepiece for XLM-R, and gpt-2 bpe for Roberata).\nFollowing is an example processing script when finetuning XLM-R model:\n  ```commandline\n  bash./util_scripts/make_synthetic_data_mt.sh config directory/of/target/data path/to/sentencepiece/model path/to/dictionary \n  ```\n  e.g.,\n  ```commandline\n  bash util_scripts/make_synthetic_data_mt.sh mask_0.0_0.6_random_0.0_0.3_insert_0.2_wholeword_0 toy_data path/to/xlmr.large/sentencepiece.bpe.model path/to/xlmr.large/dict.txt\n  ```\n  Similarly, you can run for Roberta model with example script `./util_scripts/make_synthetic_data_xsum.sh`. Please see the scripts for more details.\n\n  After this step, you will see the binarized datasets with source, target, reference and labels under a new directory `data` under `directory/of/target/data`.\n\n## Train a Hallucination Detection Model\nYou can finetune XLM-R or Roberta with the above created binarized data.\nWe provide the batch scripts to run this for MT and abstractive summarization respectively.\n```commandline\nsbatch./train_exps/example_finetune_mt.sh path/to/the/binarized/data\n```\nor \n```commandline\nsbatch./train_exps/example_finetune_xsum.sh path/to/the/binarized/data\n```\nYou may want to tune the hyperparameters inside the scripts for better performance, such as --dropout-ref (dropout reference words to prevent the model from learning edit distance), --max-update, etc.\n\n## Evaluation\nWe provide the evaluation scripts for the benchmark datasets under `./eval_data`.\nTo evaluate on these datasets, we provide python scripts `./util_scripts/eval_predict_hallucination_mt.py` and\n`./util_scripts/eval_predict_hallucination_xsum.py` for MT and summarization respectively (they only differ slightly).\nFirst, you need to specify the path to the saved detection model directory and training data path in `Line 12-13`, then run them.\n\n## Pretrained Models\nYou can download our trained models for these benchmark datasets for [zhen-MT](https://dl.fbaipublicfiles.com/detect-hallucination/zhen.mt.xlmr.tar.gz) and [XSum](https://dl.fbaipublicfiles.com/detect-hallucination/xsum.roberta.tar.gz), and evalutate them with the above scripts by first setting the `models` to be `['path/to/the/unzipped/folder']` and `datapath` to be the folder of data inside the unzipped file.\n\n## Prediction\nTo simply use the trained model for hallucination prediction for your own input, we provide an example script `./util_scripts/predict_hallucination_mt.py`\nthat predicts labels for a hypothesis file conditioned on its source file.\nAgain, please specify the path to your input files, the trained model, the training data and the output directory in `Line 12-23`, and then run it.\n\n## Scripts for Word-level Quality Estimation\nThe directory `word_level_qe/` contains scripts for both supervised and unsupervised experiments for word-level quality estimation from the [WMT18 shared task](http://www.statmt.org/wmt18/quality-estimation-task.html) (task 2 of QE). \n\n## Reference\n```\n@inproceedings{zhou21aclfindings,\n    title = {Detecting Hallucinated Content in Conditional Neural Sequence Generation},\n    author = {Chunting Zhou and Graham Neubig and Jiatao Gu and Mona Diab and Francisco Guzm\u00e1n and Luke Zettlemoyer and Marjan Ghazvininejad},\n    booktitle = {Findings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP Findings)},\n    address = {Virtual},\n    month = {August},\n    url = {https://arxiv.org/abs/2011.02593},\n    year = {2021}\n}\n```<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 25130,
        "passage": " Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\nDou et al. [2020] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig. Gsum: A general framework for guided neural abstractive summarization. arXiv preprint arXiv:2010.08014, 2020.\nBelkebir and Guessoum [2015] Riadh Belkebir and Ahmed Guessoum. A supervised approach to arabic text summarization using adaboost. In New contributions in information systems and technologies, pages 227\u2013236. Springer, 2015.\nAL-Khawaldeh and Samawi [2015] Fatima T AL-Khawaldeh and Venus W Samawi. Lexical cohesion and entailment based segmentation for arabic text summarization (lceas). World of Computer Science & Information Technology Journal, 5(3), 2015.\nFejer and Omar [2014] Hamzah Noori Fejer and Nazlia Omar. Automatic arabic text summarization using clustering and keyphrase extraction. In Proceedings of the 6th International Conference on Information Technology and Multimedia, pages 293\u2013298. IEEE, 2014.\nAbu Nada et al. [2020] Abdullah M Abu Nada, Eman Alajrami, Ahmed A Al-Saqqa, and Samy S Abu-Naser. Arabic text summarization using arabert model using extractive text summarization approach. International Journal of Academic Information Systems Research (IJAISR), 2020.\nEl-Kassas et al. [2021] Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, and Hoda K Mohamed. Automatic text summarization: A comprehensive survey. Expert Systems with Applications, 165:113679, 2021.\nAl-Saleh and Menai [2016] Asma Bader Al-Saleh and Mohamed El Bachir Menai. Automatic arabic text summarization: a survey. Artificial Intelligence Review, 45(2):203\u2013234, 2016.\nElsaid et al. [2022] Asmaa Elsaid, Ammar Mohammed, Lamiaa Fattouh, and Mohamed Sakre. A comprehensive review of arabic text summarization. IEEE Access, 2022.\nEl-Haj et al. [2010] Mahmoud El-Haj, Udo Kruschwitz, and Chris Fox. Using mechanical turk to create a corpus of arabic summaries. European Language Resources Association, 2010.\nEl-Haj and Koulali [2013] Mahmoud El-Haj and Rim Koulali. Kalimat a multipurpose arabic corpus. In Second Workshop on Arabic Corpus Linguistics (WACL-2), pages 22\u201325, 2013.\nEl-Ghannam and El-Shishtawy [2014] Fatma El-Ghannam and Tarek El-Shishtawy. Multi-topic multi-document summarizer. arXiv preprint arXiv:1401.0640, 2014.\nChouigui et al. [2021] Amina Chouigui, Oussama Ben Khiroun, and Bilel Elayeb. An arabic multi-source news corpus: Experimenting on single-document extractive summarization. Arabian Journal for Science and Engineering, 46(4):3925\u20133938, 2021.\nHasan et al. [2021] Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Islam, Kazi Samin, Yuan-Fang Li, Yong-Bin Kang, M Sohel Rahman, and Rifat Shahriyar. Xl-sum: Large-scale multilingual abstractive summarization for 44 languages. arXiv preprint arXiv:2106.13822, 2021.\nV\u00f6lske et al. [2017] Michael V\u00f6lske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining Reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages 59\u201363, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-4508. URL https://aclanthology.org/W17-4508.\nGrusky et al. [2018] Max Grusky, Mor Naaman, and Yoav Artzi. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. arXiv preprint arXiv:1804.11283, 2018.\n, volume 4, pages 128\u2013135. Citeseer, 2004.\nAl-Maleh and Desouki [2020] Molham Al-Maleh and Said Desouki.\nArabic text summarization using deep learning approach.\nJournal of Big Data, 7(1):1\u201317, 2020.\nAyedh et al. [2016] Abdullah Ayedh, Guanzheng Tan, Khaled Alwesabi, and Hamdi Rajeh. The effect of preprocessing on arabic document categorization. Algorithms, 9(2):27, 2016.\nMubarak [2017] Hamdy Mubarak. Build fast and accurate lemmatization for arabic. arXiv preprint arXiv:1710.06700, 2017.\nAbdelali et al. [2016] Ahmed Abdelali, Kareem Darwish, Nadir Durrani, and Hamdy Mubarak. Farasa: A fast and furious segmenter for arabic. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 11\u201316, 2016.\nEl-Defrawy et al. [2015] Mahmoud El-Defrawy, Yasser El-Sonbaty, and Nahla Belal. Enhancing root extractors using light stemmers. In Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters, pages 157\u2013166, 2015.\nPasha et al. [2014] Arfath Pasha, Mohamed Al-Badrashiny, Mona T Diab, Ahmed El Kholy, Ramy Eskander, Nizar Habash, Manoj Pooleery, Owen Rambow, and Ryan Roth. Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic. In LREC, volume 14, pages 1094\u20131101, 2014.\nEl-Khair [2017] Ibrahim Abu El-Khair. Effects of stop words elimination for arabic information retrieval: a comparative study. arXiv preprint arXiv:1702.01925, 2017.\nAl-Taani and Al-Omour [2014] Ahmad T Al-Taani and Maha M Al-Omour. An extractive graph-based arabic text summarization approach. In The International Arab Conference on Information Technology, 2014.\nBoudchiche and Mazroui [2019] Mohamed Boudchiche and Azzeddine Mazroui. A hybrid approach for arabic lemmatization. International Journal of Speech Technology, 22(3):563\u2013573, 2019.\nObeid et al. [2020] Ossama Obeid, Nasser Zalmout, Salam Khalifa, Dima Taji, Mai Oudah, Bashar Alhafni, Go Inoue, Fadhl Eryani, Alexander Erdmann, and Nizar Habash.\nCamel tools: An open source python toolkit for arabic natural language processing.\nIn Proceedings of the 12th language resources and evaluation conference, pages 7022\u20137032, 2020.\nGat [2022] Gate ahram newspaper (egypt). http://gate.ahram.org.eg/, 2022. Accessed: 2020-02-02.\nXue et al. [2020] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020.\nTang et al. [2020] Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. Multilingual translation with extensible multilingual pretraining and finetuning. arXiv preprint arXiv:2008.00401, 2020.\nTran et al. [2020] Chau Tran, Yuqing Tang, Xian Li, and Jiatao Gu. Cross-lingual retrieval for iterative self-supervised training. Advances in Neural Information Processing Systems, 33:2207\u20132219, 2020.\nTiedemann and Thottingal [2020] J\u00f6rg Tiedemann and Santhosh Thottingal. OPUS-MT \u2013 building open translation services for the world<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 25213,
        "passage": " to favor the true solution. The model sometimes gives the wrong prediction\u2014for example, at t=16k, and changes its prediction from the true solution to the wrong solution, \u201837-36\u2019\u2014but again changes its prediction to be a true solution afterward. In addition, its intermediate wrong solution, \u201837-36\u2019 indicates the model was confused with distinguishing the longest field goal of Rob Bironas (40 vs. 37), which is an understandable mistake.\nWe also compare the predictions from the model with our method to those from the model with MML, which is shown in Appendix C.\nQuality of the predicted solution.\nWe analyze if the model outputs the correct solution, since the solution executing the correct answer could be spurious. First, on NarrativeQA and DROPnum, we manually analyze 50 samples from the development set and find that 98% and 92% of correct cases produce the correct solution respectively. Next, on WikiSQL, we compare the predictions from the model to the annotated SQL queries on the development set. This is possible because gold SQL queries are available in the dataset for the full supervision. Out of 8,421 examples, 7,110 predictions execute the correct answers. Among those, 88.5% of the predictions are exactly same as the annotated queries. Others are the cases where (i) both queries are correct, (ii) the model prediction is correct but the annotated query is incorrect, and (iii) the annotated query is correct and the model prediction is spurious. We show a full analysis in Appendix C.\nRobustness to the noise in |Z|.\nSometimes noise arises during the construction of |Z|, such as |Z| constructed based on ROUGE-L for NarrativeQA. To explore the effect of noise in Z, we experiment with more noisy solution set by picking all the spans with scores that is equal to or larger than the 5th highest. The new construction method increases |Z| from 4.3 to 7.1 on NarrativeQA. The result by MML objective drops significantly (56.07\u219251.14) while the result by ours drops marginally (58.77\u219257.97), suggesting that MML suffers more with a noisier Z while ours is more robust.\nIn this paper, we demonstrated that, for many QA tasks which only provide the answer text as supervision, it is possible to precompute a discrete set of possible solutions that contains one correct option. Then, we introduced a discrete latent variable learning algorithm which iterates a procedure of predicting the most likely solution in the precomputed set and further increasing the likelihood of that solution. We showed that this approach significantly outperforms previous approaches on six QA tasks including reading comprehension, open-domain QA, discrete reasoning task and semantic parsing, achieving absolute gains of 2\u201310% and setting the new state-of-the-art on five well-studied datasets.\nThis research was supported by ONR (N00014-18-1-2826, N00014-17-S-B001), DARPA N66001-19-2-403, NSF (IIS-1616112, IIS-1252835, IIS-1562364), ARO (W911NF-16-1-0121), an Allen Distinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google and Amazon.\nThe authors would like to thank the anonymous reviewers, Eunsol Choi, Christopher Clark, Victor Zhong and UW NLP members for their valuable feedback.\nAbolafia et al. (2018) Daniel A Abolafia, Mohammad Norouzi, Jonathan Shen, Rui Zhao, and Quoc V Le. 2018. Neural program synthesis with priority queue training. arXiv preprint arXiv:1801.03526.\nAgarwal et al. (2019) Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. 2019. Learning to generalize from sparse and underspecified rewards. In ICML.\nAlberti et al. (2019) Chris Alberti, Kenton Lee, and Michael Collins. 2019. A BERT baseline for the Natural Questions. arXiv preprint arXiv:1901.08634.\nArtzi and Zettlemoyer (2013) Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. In ACL.\nBerant et al. (2013) Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In EMNLP.\nChen et al. (2017) Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open-domain questions. In ACL.\nClarke et al. (2010) James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world\u2019s response. In CoNLL.\nDong and Lapata (2018) Li Dong and Mirella Lapata. 2018. Coarse-to-fine decoding for neural semantic parsing. In ACL.\nDua et al. (2019) Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL.\nHe et al. (2019) Pengcheng He, Yi Mao, Kaushik Chakrabarti, and Weizhu Chen. 2019. X-SQL: reinforce schema representation with context. arXiv preprint arXiv:1908.08113.\nHurley and Rickard (2009) Niall Hurley and Scott Rickard. 2009. Comparing measures of sparsity. IEEE Transactions on Information Theory.\nHwang et al. (2019) Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and Minjoon Seo. 2019. A comprehensive exploration on WikiSQL with table-aware word contextualization. arXiv preprint arXiv:1902.01069.\nIyyer et al. (2017) Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In ACL.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In ACL.\nKadlec et al. (2016) Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. 2016. Text understanding with the attention sum reader network. In ACL.\nKo\u010disk\u1ef3 et al. (2018) Tom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. TACL.\nKrishnamurthy et al. (2017) Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. 2017. Neural semantic parsing with type constraints for semi-structured tables. In EMNLP.\nKwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Change, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. TACL.\nLee et al. (2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In ACL.\nLiang et al. (2017) Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. 2017.\nLiang et al. (2018) Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V Le, and Ni Lao. 2018. Memory augmented policy optimization for program synthesis and semantic parsing. In NIPS.\nLin (2004) Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. Text Summarization Branches Out.\nMin et al. (2019) Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. Compositional questions do not necessitate multi-hop reasoning. In ACL.\nNishida et al. (2019) Kyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda, Atsushi Otsuka, Hisako Asano, and Junji Tomita. 2019. Multi-style generative reading comprehension. In ACL.\nPaszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017.\nAutomatic differentiation in PyTorch.\nRajpur<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 25672,
        "passage": ". In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884, Minneapolis, Minnesota. Association for Computational Linguistics.\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, Wolfgang Macherey, Zhifeng Chen, and Yonghui Wu. 2019. Massively multilingual neural machine translation in the wild: Findings and challenges. CoRR, abs/1907.05019.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. 2017. Unsupervised neural machine translation. arXiv preprint arXiv:1710.11041.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2019. On the cross-lingual transferability of monolingual representations.\nMauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit3: Web inventory of transcribed and translated talks. In Conference of European Association for Machine Translation, pages 261\u2013268.\nMauro Cettolo, Niehues Jan, St\u00fcker Sebastian, Luisa Bentivogli, Roldano Cattoni, and Marcello Federico. 2015. The iwslt 2015 evaluation campaign. In International Workshop on Spoken Language Translation.\nPeng-Jen Chen, Jiajun Shen, Matt Le, Vishrav Chaudhary, Ahmed El-Kishky, Guillaume Wenzek, Myle Ott, and Marc\u2019Aurelio Ranzato. 2019. Facebook ai\u2019s wat19 myanmar-english translation task submission. arXiv preprint arXiv:1910.06848.\nXilun Chen and Claire Cardie. 201Unsupervised multilingual word embeddings. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 261\u2013270, Brussels, Belgium. Association for Computational Linguistics.\nYun Chen, Yang Liu, Yong Cheng, and Victor OK Li. 2017. A teacher-student framework for zero-resource neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1925\u20131935.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Association for Computational Linguistics (NAACL).\nChenchen Ding, Hnin Thu Zar Aye, Win Pa Pa, Khin Thandar Nwet, Khin Mar Soe, Masao Utiyama, and Eiichiro Sumita. 2019. Towards Burmese (Myanmar) morphological analysis: Syllable-based tokenization and part-of-speech tagging. ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP), 19(1):5.\nChenchen Ding, Masao Utiyama, and Eiichiro Sumita. 2018. NOVA: A feasible and flexible annotation system for joint tokenization and part-of-speech tagging. ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP), 18(2):17.\nSergey Edunov, Alexei Baevski, and Michael Auli. 2019. Pre-trained language model representations for language generation. arXiv preprint arXiv:1903.09722.\nOrhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-way, multilingual neural machine translation with a shared attention mechanism. In NAACL.\nJiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K. Li. 20Universal neural machine translation for extremely low resource languages. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 344\u2013354, New Orleans, Louisiana. Association for Computational Linguistics.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. 20Improved zero-shot neural machine translation via ignoring spurious correlations. arXiv preprint arXiv:1906.01181.\nFrancisco Guzm\u00e1n, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample, Philipp Koehn, Vishrav Chaudhary, and Marc\u2019Aurelio Ranzato. 2019. The FLORES evaluation datasets for low-resource machine translation: Nepali\u2013 English and Sinhala\u2013English. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6097\u20136110, Hong Kong, China. Association for Computational Linguistics.\nS\u00e9bastien Jean, Stanislas Lauly, Orhan Firat, and Kyunghyun Cho. 2017. Does neural machine translation benefit from larger context? CoRR, abs/1704.05135.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, et al. 2017. Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339\u2013351.\nTaku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371, Brussels, Belgium. Association for Computational Linguistics.\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhattacharyya. 2017. The IIT bombay englishhindi parallel corpus. CoRR, abs/1710.02855.\nGuillaume Lample and Alexis Conneau. 2019. Cross-lingual language model pretraining. arXiv preprint arXiv:1901.07291.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. 2018a. Unsupervised machine translation using monolingual corpora only. In International Conference on Learning Representations.\nGuillaume Lample, Alexis Conneau, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. 2018b. Word translation without parallel data. In International Conference on Learning Representations.\nGuillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. 2018c. Phrase-based & neural unsupervised machine translation. arXiv preprint arXiv:1804.07755.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.\nLiangyou Li, Xin Jiang, and Qun Liu. 2019. Pretrained language models for document-level neural machine translation. arXiv preprint arXiv:1911.03110.\nYang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. arXiv preprint arXiv:1908.08345.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson. 2018. Documentlevel neural machine translation with hierarchical attention networks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2947\u20132954, Brussels, Belgium. Association for Computational Linguistics.\nTomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013. Exploiting similarities among languages for machine translation. Co<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 25720,
        "passage": "Luke Zettlemoyer \u201800 joined the faculty in the Department of Computer Science and Engineering at the University of Washington this fall. In addition to his teaching responsibilities, Zettlemoyer will continue his research on artificial intelligence.\nDuring his time at<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 25741,
        "passage": " complex systems are often elusive, while their measurement data are available. This talk will address theoretical and computational challenges for targeted coordination of both isolated and networked ensemble systems arising in diverse areas at different scales. Both data-driven and model-based approaches for learning, decoding, control, and computation of dynamic structures and patterns in ensemble systems will be presented. Practical control designs, including synchronization waveforms for pattern formation in nonlinear oscillatory networks and optimal pulses in quantum control will be illustrated along with their experimental realizations. Lastly, future directions and opportunities in Systems and Controls will be discussed.\nBiography: Dr. Jr-Shin Li is currently Das Family Distinguished Career Development Associate Professor of Systems Science and Mathematics in the Department of Electrical and Systems Engineering at Washington University in St. Louis, where he also holds a joint appointment in the Division of Biology & Biomedical Sciences since he joined Washington University in 2006. Dr. Li received a B.S. and an M.S. from National Taiwan University, and a Ph.D. in Applied Mathematics from Harvard University in 2006. His research interests lie in the areas of systems, computational, and data sciences, and their applications to biology, neuroscience, quantum physics, brain medicine, and public health. He is a recipient of the NSF Career Award in 2008 and the AFOSR Young Investigator Award in 2010. He is currently Associate Editor of the SIAM Journal on Control and Optimization (SICON) and the IEEE Transactions on Control Systems Technology (TCST).\nAbstract: Maxwell's equations are responsible for explaining the fundamental operating principles behind most of today's technology. In this talk, we will explore how understanding and controlling electromagnetic fields can lead to significant impact across a multitude of applications over a wide frequency range on the electromagnetic spectrum. Starting from the low-frequency end of the spectrum, I will present the design and implementation of a new integrated magnetic biosensor. The magnetic biosensor is fabricated in a standard CMOS foundry process without any post-fabrication processing and can perform in-vitro detection of DNA, proteins, and cells by utilizing magnetic nanoparticles as labels. We will discuss three different, improved sensor designs, which address sensor gain uniformity, enable multiplex target detection, and compensate sensor electrical and thermal drift based on spatial and temporal manipulations of the magnetic fields. I will present initial in-vitro biodetection experiments, and discuss future research directions moving towards in-vivo sensing with wearable and implantable devices, as well as actuation via targeted therapeutics. Next, we will look into the RF domain and develop maximal performance bounds for antennas. I will present a rapid simulation technique which, when coupled with heuristic optimization algorithms, can quickly and effectively produce new antenna structures de-novo with little or no manual intervention. The efficacy of these techniques will be shown in the context of a 3D printed coupling antenna for a dielectric waveguide communication link. Moving higher in frequency, we will explore the near-infrared (NIR) part of the spectrum in the context of silicon photonic device optimization. I will present on-going work in designing grating coupler and power splitting devices with arbitrary splitting ratios by using adjoint optimization and highly efficient integral equation techniques. We will also explore exciting future directions in these research areas, leveraging modern computation and efficient numerical algorithms as well as holistic co-design of circuits and electromagnetics.\nBiography: Constantine Sideris received the B.S., M.S., and PhD degrees with honors from the California Institute of Technology in 2010, 2011, and 2016 respectively. He was a visiting scholar at UC Berkeley's Wireless Research Center from 2013 to 2014. He was a lecturer in the Electrical Engineering department for Caltech's popular machine learning project course in 2017. He is currently a postdoctoral scholar in the Electrical Engineering and Computational and Mathematical Sciences departments at Caltech. His research interests include RF and millimeter-wave integrated circuits and computational electromagnetics for biomedical applications, wireless communications, and silicon photonics. He was a recipient of an NSF graduate research fellowship in 2010, the Analog Devices Outstanding Student Designer Award in 2012, and the Caltech Leadership Award in 2017.\nAbstract: Until recently the conventional wisdom was that proposing a new chip startup in the US was a bad bet. Recently that perception has changed. There are dozens of startups that have found funding for new chip architectures that perform neural network computations much faster while consuming less power than general purpose CPUs. In fact, over 1.5 billion dollars in venture funding has already been dispersed for such startups. There are several factors behind this change of heart. First has been a slowing of Moore's Law that has made application specific computers more attractive. Second is the existence of application specific computers that could easily be repurposed, as exemplified by Digital Signal Processors and Graphics Processors. Finally, the presence of independent foundries such as the Taiwan Semiconductor Manufacturing Company and the United Microelectronics Corporation removed the need for every chip startup to build its own multi-billion dollar fabrication facility. In this talk I will discuss the reasons for this explosion starting with an overview of the problems these machines are targeting. I will then examine the aforementioned factors in more detail. Lastly, I will outline the co-design process that has led to many of the existing solutions. My concluding remarks will discuss the barriers to the success of these new architectures.\nBiography: Trevor Mudge received the Ph.D. in Computer Science from the University of Illinois, Urbana. He is now the Bredt Family Professor of Computer Science and Engineering at the University of Michigan, Ann Arbor. He is author of numerous papers on computer architecture, programming languages, VLSI design, and computer vision. He has also chaired 54 theses in these areas. In 2014 he received the ACM/IEEE CS Eckert-Mauchly Award and the University of Illinois Distinguished Alumni Award. He is a Life Fellow of the IEEE, a Fellow of the ACM, and a member of the IET and the British Computer Society.\nAbstract: In this talk we propose to use natural language as a guide for what people can perceive about the world from images and what ultimately machines should aim to see as well. We discuss two recent structured prediction efforts in this vein: scene graph parsing in Visual Genome, a framework derived from captions, and visual semantic role labeling in imSitu, a formalism built on FrameNet and WordNet. In scene graph parsing, we examine the problem of modeling higher order repeating structure motifs and present new state of the art baselines and methods. We then look at the problem semantic sparsity in visual semantic role labeling: infrequent combinations of output semantics are frequent. We present new compositional and data-augmentation methods for dealing with this challenge, significantly improving on prior work.\nBiography: Mark Yatskar is a post-doc at the Allen Institute for Artificial Intelligence and recipient of their Young Investigator Award. His primary research is in the intersection of language and vision, natural language generation, and ethical computing. He received his Ph.D. from the University of Washington with Luke Zettlemoyer and Ali Farhadi and in 2016 received the EMNLP best paper award and his work has been featured in Wired and the New York Times.\nAbstract: The Cornell School of Operations Research and Information Engineering has been working with the bike-sharing company Citi Bike since Citi Bike began operations in New York City in 2013. We provide data analysis and advice about strategy and operations, not just to Citi Bike, but also to its parent company Motivate that operates several bike-sharing programs around the USA. I will describe some of our modeling work with Citi Bike, focusing on a suite of models (not just simulation models) that informs the decision about where to position both racks and bikes around the approximately 600 stations in NYC. Joint work with Daniel Freund, Nanjing Jian, Eoin OMahony and David Shmoys.\nThe Systems Leadership Series is a series of interactive conversations with leading systems thinkers who explore and examine the nature and complexity of systems that modern society depends upon. The series is an unparalleled learning opportunity as prominent speakers come to share cutting edge ideas, leadership styles and personal philosophies with students and faculty members.\nBiography: Shane G. Henderson is professor and director of the School of Operations Research and Information Engineering at Cornell University. He has previously held positions in the Department of Industrial and Operations Engineering at the University of Michigan and the Department of Engineering Science at the University of Auckland. He is the editor in chief of Stochastic Systems. He has served as chair of the INFORMS Applied Probability Society, and as simulation area editor for Operations Research. He is an INFORMS Fellow. His research interests include discrete-event simulation, simulation optimization, and emergency services planning.\nWebcast: Register at the event link.\nWebCast Link: Register at the event link.\nAbstract: The ability to discover physical laws and governing equations from data is one of humankind's greatest intellectual achievements. A quantitative understanding of dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled advanced technology, including aircraft, combustion engines, satellites, and electrical power. There are many more critical data-driven problems, such as understanding cognition from neural recordings, inferring patterns in climate, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an increasingly important role in these efforts.\nThis work develops a general framework to discover the governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity-promoting techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 25747,
        "passage": "RoBERTa\n----------------------------------------------------\n\nThe RoBERTa model was proposed in `RoBERTa: A Robustly Optimized BERT Pretraining Approach <https://arxiv.org/abs/1907.11692>`_\nby Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\nVeselin Stoyanov. It is based on Google's BERT model released in 2018.\n\nIt builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining\nobjective and training with much larger mini-batches and learning rates.\n\nThe abstract from the paper is the following:\n\n*Language model pretraining has led to significant performance gains but careful comparison between different\napproaches is challenging. Training is computationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the final results. We present a replication\nstudy of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can match or exceed the performance<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 25954,
        "passage": " task of multi-lingual language modeling task, so that a more general version of the multi-lingual contextual embedding can be investigated.\nZero-shot Cross-lingual Transfer. The main strands of work focused on learning cross-lingual word embeddings. Ruder et al [2017] surveyed methods [Klementiev et al, 2012; Kociskyet al., 2014; Guo et al, 2016] for learning cross-lingual word embeddings by either joint training or post-training mappings of monolingual embeddings. Xing et al [2015], Lample et al [2018] and Chen and Cardie [2018] proposed to take pre-trained monolingual word embeddings of different languages as input, aligning them into a shared semantic space. Our work follows in the recent line of cross-lingual contextualized embedding methods [Huang et al, 2019; Devlin et al, 2019; Wu and Dredze, 2019; Conneau and Lample, 2019; Artetxe et al, 2019], which are trained using masked language modeling or other auxiliary pre-training tasks to encourage representation in source and target language space closer, achieving state-of-the-art performance on a variety of zero-shot cross-lingual NLP tasks. We propose a data augmentation framework to dynamically construct multi-lingual code-switching data for training, which encourages model implicitly to align similar words in different languages into the same space.\n[Artetxe and Schwenk, 2018] Mikel Artetxe and Holger Schwenk. Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. arXiv preprint arXiv:1812.10464, 2018.\n[Artetxe et al., 2019] Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolingual representations. arXiv preprint arXiv:1910.11856, 2019.\n[Barnes et al., 2018] Jeremy Barnes, Roman Klinger, and Sabine Schulte im Walde. Bilingual sentiment embeddings: Joint projection of sentiment across languages. In Proc. of ACL, pages 2483\u20132493, Melbourne, Australia, July 2018. Association for Computational Linguistics.\n[Chen and Cardie, 2018] Xilun Chen and Claire Cardie. Unsupervised multilingual word embeddings. arXiv preprint arXiv:1808.08933, 2018.\n[Chen et al., 2018] Wenhu Chen, Jianshu Chen, Yu Su, Xin Wang, Dong Yu, Xifeng Yan, and William Yang Wang. XL-NBT: A cross-lingual neural belief tracking framework. In Proc. of EMNLP, October-November 2018.\n[Conneau and Lample, 2019] Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Advances in Neural Information Processing Systems, pages 7057\u20137067, 2019.\n[Conneau et al., 2018] Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. XNLI: Evaluating crosslingual sentence representations. In Proc. of EMNLP, 2018.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. of NAACL, 2019.\n[Guo et al., 2016] Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng Wang, and Ting Liu. A representation learning framework for multi-source transfer parsing. In Proc. of AAAI, 2016.\n[Huang et al., 2019] Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks. In Proc. of EMNLP, November 2019.\n[Klementiev et al., 2012] Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. Inducing crosslingual distributed representations of words. In Proc. of COLING, 2012.\n[Kociskyet al., 2014] Tomas Kocisky, Karl Moritz Hermann, and Phil Blunsom. Learning bilingual word representations by marginalizing alignments. In Proc. of ACL, June 2014.\n[Lample et al., 2018] Guillaume Lample, Alexis Conneau, Marc\u2019Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Word translation without parallel data. In International Conference on Learning Representations, 2018.\n[Liu et al., 2019a] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n[Liu et al., 2019b] Zihan Liu, Genta Indra Winata, Zhaojiang Lin, Peng Xu, and Pascale Fung. Attention-informed mixed-language training for zero-shot cross-lingual taskoriented dialogue systems, 2019.\n[Mrksicet al., 2017] Nikola Mrksic, Ivan Vulic, Diarmuid O Seaghdha, Ira Leviant, Roi Reichart, Milica Gasic, Anna Korhonen, and Steve Young. Semantic specialization of distributional word vector spaces using monolingual and cross-lingual constraints. Transactions of the Association for Computational Linguistics, 5:309\u2013324, 2017.\n[Ruder et al., 2017] Sebastian Ruder, Ivan Vulic, and Anders S\u00f8gaard. A survey of cross-lingual word embedding models. arXiv preprint arXiv:1706.04902, 2017.\n[Schuster et al., 2019a] Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike Lewis. Cross-lingual transfer learning for multilingual task oriented dialog. In Proc. of NAACL, June 2019.\n[Schuster et al., 2019b] Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson. Cross-lingual alignment of contextual word embeddings, with applications to zeroshot dependency parsing. In Proc. of NAACL, June 2019.\n[Schwenk and Li, 2018] Holger Schwenk and Xian Li. A corpus for multilingual document classification in eight languages. In Proceedings of the 11th Language Resources and Evaluation Conference, May 2018.\n[Wang et al., 2019] Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, and Ting Liu. Cross-lingual BERT transformation for zero-shot dependency parsing. In Proc. of EMNLP, November 2019.\n[Wu and Dredze, 2019] Shijie Wu and Mark Dredze. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. In Proc. of EMNLP, 2019.\n[Xing et al., 2015] Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. Normalized word embedding and orthogonal transform for bilingual word translation. In Proc. of NAACL, 2015.\n[Yin et al., 2019] Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dialog state tracking with reinforced data augmentation. arXiv preprint arXiv:1908.07795, 2019.\n[Yu et al., 2018] Katherine Yu, Haoran Li, and Barlas Oguz. Multilingual seq2seq training with similarity loss for cross-lingual document classification. In Proceedings of The Third Workshop on Representation Learning for NLP, July 2018.\n[Zhang et al., 2019] Meishan Zhang, Yue Zhang, and Guohong Fu. Cross-lingual dependency parsing using codemixed TreeBank. In Proc. of EMNLP, pages 997\u20131006, Hong Kong, China, November 2019. Association for Computational Linguistics.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 26250,
        "passage": " annotated schemas.\nBaseline 1: Event Language Model Rudinger et al. (2015); Pichotta and Mooney (2016) is the state-of-the-art event schema induction method. It learns the probability of event temporal sequences, and the event sequences generated from event language model are considered as schemas.\nBaseline 2: Sequential Pattern Mining Pei et al. (2001) discovers frequent sequential patterns and encodes arguments and their relations as properties of the pattern. Considering event language model baseline cannot handle multiple arguments and relations, we add sequential pattern mining for comparison. The frequent patterns mined are considered as schemas.\nReference: Human Schema Since human-created schemas are highly accurate but not probabilistic, we want to evaluate its limits at predicting events in the extrinsic task. We match schemas to instances and fill in the matched type.\nAblation Study: Event Graph Model w/o Argument Generation is included as a variant of our temporal event graph model in which we remove argument generation (\u00a73.5 and \u00a73.6). It learns to generate a graph containing only event nodes with their temporal relations.\nTraining Details For our event graph model, the representation dimension is 512, and we use a 2-layer GNN. The value of B is 10. For the event language model baseline, instead of using LSTM-based architecture following Pichotta and Mooney (2016), we adopt the state-of-the-art auto-regressive language XLNet (Yang et al., 2019) for fair comparison.777 https://github.com/huggingface We use the default parameter setting of XLNet to train the model, and select the best model on validation set. For sequential pattern mining, we perform random walk, starting from every node in instance graphs and ending at sink nodes, to obtain event type sequences, and then apply PrefixSpan888https://github.com/chuanconggao/PrefixSpan-py to get the ranking list of sequential patterns.\nEvaluation Details To compose the schema library, we use the first ranked sequence as schema for event language model and sequential pattern mining baselines. To perform event prediction using baselines, we traverse the input graph to get event type sequences, and conduct prediction on all sequences to produce an averaged score. For human schemas, we first linearize them and the input graphs, and find the longest common subsequence between them. We fill in the future event type using the best match.\nTable 4: Instance graph perplexity.\nTable 5: Schema-guided ending event prediction performance.\nIn Table 3, the significant gain on event match and ordering match demonstrates the ability of our graph model to keep salient events and order them. On sequence match, our approach achieves larger performance gain compared to baselines when the path length l is longer. It implies that the proposed model is capable of capturing longer and wider temporal dependencies. In the case of connection match, only sequential pattern mining in the baselines can predict connections between events. When compared against sequential pattern mining, our generation model performs better since it considers the inter-dependency of arguments and encodes them with graph structures.\nRemoving argument generation (\u201cw/o ArgumentGeneration\u201d) generally lowers the performance on all evaluation tasks, since it ignores the coreferential arguments and their relations, but relies solely on the overly simplistic temporal order to connect events. This is especially apparent from the instance graph perplexity in Table 4.\nFigure 3: The example of event prediction on IED corpus.\nOn the extrinsic task of schema-guided event prediction, our graph model obtains significant improvement (see Table 5.) The low performance of human schema demonstrates the importance of probabilistically modeling schemas to support downstream tasks. Take Figure 3 as an example. The human schema fails to predict Injure, because it relies on the exact match of event sequences, and cannot handle the variants of sequences. This problem can be solved by our probabilistic schema, via modeling the event prediction probability given the existing graph.\nModi (2016); Granroth-Wilding and Clark (2016); Weber et al. (2018, 2020); Lyu et al. (2020), and language modeling Rudinger et al. (2015); Pichotta and Mooney (2016); Peng and Roth (2016). All of these methods still assume that events follow linear order in a single chain. They also overlook the relations between participants which are critical for understanding the complex event. Recent work on event graph schema induction Li et al. (2020) only considers the connections between a pair of two events. Similarly, the event prediction task is designed to automatically generate a missing event (e.g., a word sequence) given a single or a sequence of prerequisite events Nguyen et al. (2017); Hu et al. (2017); Li et al. (2018b); Kiyomaru et al. (2019); Lv et al. (2019), or predict a pre-condition event given the current events Kwon et al. (2020). In contrast, we leverage the automatically discovered temporal event schema as guidance to forecast the future events.\nOur work is also related to recent advances in modeling and generation of graphs Li et al. (2018a); Jin et al. (2018); Grover et al. (2019); Simonovsky and Komodakis (2018); Liu et al. (2019); Fu et al. (2020); Dai et al. (2020). Autoregressive methods, such as GraphRNN You et al. (2018), GRAN Liao et al. (2019), GRAT Yoo et al. (2020) and GraphAF Shi et al. (2019), model graph generation as a sequence of additions of new nodes and edges conditioned on the graph substructure generated so far. To induce event schema, we propose to construct instance graphs and learn a graph generation model following similar manner. Our approach is designed for complex event graph to encode both temporal and argument semantics. It sequentially generates a new event node by predicting the most likely type of the subsequent event over a target event ontology, and then add argument coreferential edges with copy mechanism and event temporal orders by exploiting the dependency between the new event and all existing events.\nWe propose a new task to induce temporal complex event schema, which is capable of representing multiple temporal dependencies between events and their connected arguments. We induce such schemas by learning an event graph model, a deep auto-regressive model, from the automatically extracted instance graphs. Experiments demonstrate the model\u2019s effectiveness on both intrinsic evaluation and the downstream task of schema-guided event prediction. These schemas can guide our understanding and ability to make predictions with respect to what might happen next, along with background knowledge including location-, and participant-specific and temporally ordered event information. In the future, we plan to extend our framework to hierarchical event schema induction, as well as event and argument instance prediction.\n, volume 13, pages 1797\u20131807.\nChambers and Jurafsky (2008) Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of the 2008 Annual Meeting of the Association for Computational Linguistics (ACL2008), pages 789\u2013797.\nChambers and Jurafsky (2009) Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing (ACL-IJCNLP2009).\nCheung et al. (2013) Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Vanderwende. 2013. Probabilistic frame induction. Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 837\u2013846.\nFu et al. (2020) Dongqi Fu, Dawei Zhou, and Jingrui He. 2020. Local motif clustering on time-evolving graphs. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 390\u2013400.\nGrover et al. (2019) Aditya Grover, Aaron Zweig, and Stefano Ermon. 2019. Graphite: Iterative generative modeling of graphs. In International conference on machine learning, pages 2434\u20132444. PMLR.\nGu et al. (2016) Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1631\u20131640.\nHu et al. (2017) Linmei Hu, Juanzi Li, Liqiang Nie, Xiaoli Li, and Chao Shao. 2017. What happens next? future subevent prediction using contextual hierarchical lstm. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 3450\u20133456.\nJans et al. (2012) Bram Jans, Steven Bethard, Ivan Vuli\u0107, and Marie Francine Moens. 2012.\nSkip n-grams and ranking functions for predicting script events.\nIn Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 336\u2013344. Association for Computational Linguistics.\nJin et al. (2018) Wengong Jin, Regina Barzilay, and Tommi Jaakkola. 2018.\nIn International Conference on Machine Learning, pages 2323\u20132332.\nJoshi et al. (2019) Mandar Joshi, Omer Levy, Daniel S Weld, and Luke Zettlemoyer. 2019. Bert<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 26391,
        "passage": " into the world. Table 6 : Example instances from each dataset and the clarifications generated for them in various resources. We only include clarifications that helped the model predict the correct answer.\nTo make this task compatible with the other tasks, we only kept a single correct answer per instance, making our results not comparable to previously reported results.\nWord associations and dataset-specific features that are not informative for the task are identified by a strong baseline and removed(Gururangan et al., 2018;Zellers et al., 2018).\nExcluding unpublished leaderboard submissions.\nWe omitted COPA from the analysis due to its small size. See the appendix for examples.\nIf a worker consider an answer as \"completely not understandable\", we marked it as not relevant, correct, or helpful.\nLisa Bauer, Yicheng Wang, and Mohit Bansal. 2018. Commonsense for generative multi-hop question an- swering tasks. In Proceedings of the 2018 Confer- ence on Empirical Methods in Natural Language Processing, pages 4220-4230, Brussels, Belgium. Association for Computational Linguistics.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jian- feng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelli- gence.\nJoe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pre- trained models. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 1173-1178, Hong Kong, China. As- sociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understand- ing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technolo- gies, Minneapolis, Minnesota. Association for Com- putational Linguistics.\nBhuwan Dhingra, Danish Danish, and Dheeraj Ra- jagopal. 2018. Simple and effective semi-supervised question answering. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 582-587, New Orleans, Louisiana. Associa- tion for Computational Linguistics.\nXinya Du and Claire Cardie. 2018. Harvest- ing paragraph-level question-answer pairs from Wikipedia. In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1907-1917, Mel- bourne, Australia. Association for Computational Linguistics.\nXinya Du, Junru Shao, and Claire Cardie. 2017. Learn- ing to ask: Neural question generation for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 1342-1352, Vancouver, Canada. Association for Computational Linguistics.\nAndrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of common- sense causal reasoning. In *SEM 2012: The First Joint Conference on Lexical and Computational Se- mantics -Volume 1: Proceedings of the main con- ference and the shared task, and Volume 2: Pro- ceedings of the Sixth International Workshop on Se- mantic Evaluation (SemEval 2012), pages 394-398, Montr\u00e9al, Canada. Association for Computational Linguistics.\nJonathan Gordon and Benjamin Van Durme. 2013. Re- porting bias and knowledge acquisition. In Proceed- ings of the 2013 workshop on Automated knowledge base construction, pages 25-30.\nHan Guo, Ramakanth Pasunuru, and Mohit Bansal. 2018. Soft layer-specific multi-task summarization with entailment and question generation. In Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 687-697, Melbourne, Australia. As- sociation for Computational Linguistics.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation artifacts in natural lan- guage inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112.\nLynette Hirschman, Marc Light, Eric Breck, and John D Burger. 1999. Deep read: A reading compre- hension system. In Proceedings of the 37th annual meeting of the Association for Computational Lin- guistics on Computational Linguistics, pages 325- 332. Association for Computational Linguistics.\nAntoine Bosselut and Yejin Choi. 2019. Dy- namic knowledge graph construction for zero- shot commonsense question answering. ArXiv, abs/1911.03876.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degener- ation. arXiv preprint arXiv:1904.09751.\nNora Kassner and Hinrich Sch\u00fctze. 2019. Negated lama: Birds cannot fly. arXiv preprint arXiv:1911.03343.\nTassilo Klein and Moin Nabi. 2019. Learning to an- swer by learning to ask: Getting the best of gpt-2 and bert worlds. arXiv preprint arXiv:1911.02365.\nTom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Asso- ciation for Computational Linguistics, 6:317-328.\nJ Richard Landis and Gary G Koch. 1977. The mea- surement of observer agreement for categorical data. biometrics, pages 159-174.\nHector Levesque, Ernest Davis, and Leora Morgen- stern. 2012. The winograd schema challenge. In Thirteenth International Conference on the Princi- ples of Knowledge Representation and Reasoning.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xi- ang Ren. 2019. KagNet: Knowledge-aware graph networks for commonsense reasoning. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 2829-2839, Hong Kong, China. Association for Computational Lin- guistics.\nHongyu Lin, Le Sun, and Xianpei Han. 2017. Rea- soning with heterogeneous knowledge for common- sense machine comprehension. In Proceedings of the 2017 Conference on Empirical Methods in Natu- ral Language Processing, pages 2032-2043, Copen- hagen, Denmark. Association for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.\nRobert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh. 2019. Baracks Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling. In Proceedings of the 57th An- nual Meeting of the Association for Computational Linguistics, Florence, Italy. Association for Compu- tational Linguistics.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai- tanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. COMET: Commonsense transformers for au- tomatic knowledge graph construction. In Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762-4779, Florence, Italy. Association for Computational Lin- guistics.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor- pus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849, San Diego, California. Association for Computational Linguis- tics.\nSimon Ostermann, Michael Roth, Ashutosh Modi, Ste- fan Thater, and Manfred Pinkal. 2018. Semeval- 2018 task 11: Machine comprehension using com- monsense knowledge. In Proceedings of the 12th In- ternational Workshop on semantic evaluation, pages 747-757.\n<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 26485,
        "passage": "This is the repository for the paper *Inducing Semantic Roles Without Syntax*, by Julian Michael and\nLuke Zettlemoyer, published in *Findings of ACL* 2021.\n\n> **Abstract:** Semantic roles are a key component of linguistic predicate-argument structure, but\n> developing ontologies of these roles requires significant expertise and manual effort.\n> Methods exist for automatically inducing semantic roles using syntactic representations, but\n> syntax can also be difficult to define, annotate, and predict.\n> We show it is possible to automatically induce semantic roles from\n> QA-SRL, a scalable and ontology-free semantic annotation scheme that uses question-answer pairs to\n> represent predicate-argument structure.\n> By associating arguments with distributions over QA-SRL questions and clustering them in a\n> mixture model, our method outperforms all previous models as well as a new state-of-the-art\n> baseline over gold syntax. \n> We show that our method works because QA-SRL acts as *surrogate syntax*,\n> capturing non-overt arguments and syntactic alternations,\n> which are central motivators for the use of semantic role labeling systems.\n\nThis repository contains code to replicate the results in the paper and supporting algorithms for\ndoing similar experiments with other data and features.\n\n## Contents\n\nThe bulk of the code is written in Scala, organlized into three modules under\n[`qasrl-roles/`](qasrl-roles/):\n* [`clustering/`](clustering/): Implementation of hybrid flat/agglomerative clustering algorithms.\n* [`modeling/`](modeling/): Construction of features and experimental/analysis pipelines.\n* [`browse/`](browse/): A webapp to browse the completed clusters.\n\nThe neural network models of QA-SRL are written with PyTorch/AllenNLP, and they are available in the\n[qasrl-modeling](https://github.com/julianmichael/qasrl-modeling) repository, brought in here as a\n[submodule](lib/qasrl-modeling).\n\nManual analysis data from Section 6 of the paper is recorded in\n[this Google Sheet](https://docs.google.com/spreadsheets/d/1S6CQzj5XnjZXFJg6bQZMCfLAooEc-ubfK7nqMkJ3BeI/edit).\n\n## Usage\n\nTo run the Scala code, you need the\n[Mill](https://com-lihaoyi.github.io/mill/mill/Intro_to_Mill.html)\nbuild tool.\n\nThen to replicate the results in the paper, run [`scripts/replicate.sh`](scripts/replicate.sh), or\njust read it for guidance if you are interested in a subset of the experiments.\nThe main entry point is [`scripts/roles.sh`](scripts/roles.sh), which calls into\n[RoleInductionApp](qasrl-roles/modeling/src-jvm/RoleInductionApp.scala).\nYou'll probably want to `tail -f stderr.log` in another pane/window to see all details, as stderr is\nredirected there so as not to interfere with the [freelog](https://github.com/julianmichael/freelog)\nconsole output written to stdout.\n\n\n## Roleset Browser\n\nYou can browse the induced rolesets in a webapp by running the following command:\n```bash\nmill -i qasrl-roles.browse.serve --data conll08-lemma --mode test --domain localhost --port 8888\n```\nThis will visualize the models that have results reported in the paper (if you have run them; see\n[`scripts/replicate.sh`](scripts/replicate.sh)). To automatically reconstruct all missing rolesets,\nrun the browser command with the `--all` flag added (it will take a while if you need all of them).\n\n## More?\n\nThere's a lot more functionality in this repository for related experiments, such as clustering on\nlexical (masked language modeling based) features, inducing predicate senses, and jointly inducing\nsemantic roles and predicate senses. The results of these experiments weren't too great, but they\nmight be an interesting starting or reference point for others interested in similar problems.\nI haven't done much in the way of documentation, so if you're interested in exploring more, reusing\nany of the code or algorithms, or building on this work, please\n[get in touch](mailto:julianjohnmichael@gmail.com).\n\n## Citation\n\nBibtex coming soon via the ACL Anthology.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 26558,
        "passage": "Hui Lee. Minimum classification error rate methods for speech recognition. IEEE Transactions on Speech and Audio processing, 5(3):257\u2013265, 1997.\n[21] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128\u20133137, 2015.\n[22] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787\u2013798, 2014.\n[23] Mahmoud Khademi and Oliver Schulte. Image caption generation with hierarchical contextual visual spatial attention. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1943\u20131951, 2018.\n[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[25] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1):32\u201373, 2017.\n[26] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross B Girshick, Kaiming He, Bharath Hariharan, and Serge J Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2017.\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.\n[28] Chenxi Liu, Junhua Mao, Fei Sha, and Alan L Yuille. Attention correctness in neural image captioning. In AAAI, pages 4176\u20134182, 2017.\n[29] Chenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. arXiv preprint arXiv:1712.00559, 2017.\n[30] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for visual question answering. In Advances In Neural Information Processing Systems, pages 289\u2013297, 2016.\n[31] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing. Citeseer, 2013.\n[32] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11\u201320, 2016.\n[33] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746\u2013751, 2013.\n[34] Duy-Kien Nguyen and Takayuki Okatani. Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n[35] Augustus Odena, Vincent Dumoulin, and Chris Olah. Deconvolution and checkerboard artifacts. Distill, 2016.\n[36] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543, 2014.\n[37] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 2227\u20132237, 2018.\n[38] Bryan A Plummer, Arun Mallya, Christopher M Cervantes, Julia Hockenmaier, and Svetlana Lazebnik. Phrase localization and visual relationship detection with comprehensive image-language cues. In Proc. ICCV, 2017.\n[39] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641\u20132649, 2015.\n[40] Filip Radenovi\u0107, Giorgos Tolias, and Ond\u0159ej Chum. Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples. In European conference on computer vision, pages 3\u201320. Springer, 2016.\n[41] Vasili Ramanishka, Abir Das, Jianming Zhang, and Kate Saenko. Top-down visual saliency guided by captions. In IEEE International Conference on Computer Vision and Pattern Recognition, 2017.\n[42] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. Grounding of textual phrases in images by reconstruction. In European Conference on Computer Vision, pages 817\u2013834. Springer, 2016.\n[43] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.\n[44] Jesse Thomason, Jivko Sinapov, and Raymond Mooney. Guiding interaction behaviors for multi-modal grounded language learning. In Proceedings of the First Workshop on Language Grounding for Robotics, pages 20\u201324, 2017.\n[45] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeulders. Selective search for object recognition. International journal of computer vision, 104(2):154\u2013171, 2013.\n[46] Mingzhe Wang, Mahmoud Azab, Noriyuki Kojima, Rada Mihalcea, and Jia Deng. Structured matching for phrase localization. In European Conference on Computer Vision, pages 696\u2013711. Springer, 2016.\n[47] Fanyi Xiao, Leonid Sigal, and Yong Jae Lee. Weakly-supervised visual grounding of phrases with linguistic structures. In IEEE International Conference on Computer Vision and Pattern Recognition, 2017.\n[48] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual question answering. In International conference on machine learning, pages 2397\u20132406, 2016.\n[49] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. In European Conference on Computer Vision, pages 451\u2013466. Springer, 2016.\n[50] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pages 2048\u20132057, 2015.\n[51] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. arXiv preprint, 2018.\n[52] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 21\u201329, 2016.\n[53] Raymond Yeh, Jinjun Xiong, Wen-Mei Hwu, Minh Do, and Alexander Schwing. Interpretable and globally optimal prediction for textual grounding using image concepts. In Advances in Neural Information Processing Systems, pages 1912\u20131922, 2017.\n[54] Raymond A Yeh, Minh N Do, and Alexander G Schwing. Unsupervised textual grounding: Linking words to image concepts. In Proc. CVPR, volume 8, 2018.\n[55] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.\n[56] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 26983,
        "passage": " two for backward) of Skim-LSTM+Attention model. We see that the second layer skims more, implying that the second layer is more confident about which tokens are important.\nFigure 6 shows F1 score of LSTM+Attention model using standard LSTM and Skim LSTM, sorted in ascending order by Flop-R. While models tend to perform better with larger computational cost, Skim LSTM (Red) outperforms standard LSTM (Blue) with comparable computational cost. We also observe that the F1 score of Skim-LSTM is more stable across different configurations and computational cost. Moreover, increasing the value of \u03b3 for Skim-LSTM gradually increases skipping rate and Flop-R, while it also leads to reduced accuracy.\nControlling skim rate. An important advantage of Skim-RNN is that the skim rate (and thus computational cost) can be dynamically controlled at inference time by adjusting the threshold for \u2018skim\u2019 decision probability p1t (Equation 1). Figure 6 shows the trade-off between the accuracy and computational cost for two settings, confirming the importance of skimming (d\u2032>0) compared to skipping (d\u2032=0).\nVisualization. Figure 7 shows an example from SQuAD and visualizes which words Skim-LSTM (d=100,d\u2032=20) reads (red) and skims (white). As expected, the model does not skim when the input seems to be relevant to answering the question. In addition, LSTM in second layer skims more than that in the first layer mainly because the second layer is more confident about the importance of each token, as shown in Figure 7. More visualizations are shown in in Appendix C.\nd=100 (batch size = 1) in all three frameworks on a single thread of CPU (averaged over 100 trials), and have observed that NumPy is 1.5 and 2.8 times faster than TensorFlow and PyTorch.888NumPy\u2019s speed becomes similar to that of TensorFlow and PyTorch at d=220 and d=700, respectively. At larger hidden size, NumPy becomes slower. This seems to be mostly due to the fact that the frameworks are primarily (optimized) for GPUs and they have larger overhead than NumPy that they cannot take much advantage of reducing the size of the hidden state of the LSTM below 100.\nFigure 8: Speed up rate of Skim-LSTM (vs LSTM) with varying skimming rates and hidden state sizes.\nFigure 8 shows the relative speed gain of Skim-LSTM compared to standard LSTM with varying hidden state size and skim rate. We use NumPy, and the inferences are run on a single thread of CPU. We also plot the ratio between the reduction of the number of float operations (Flop-R) of LSTM and Skim-LSTM. This can be considered as a theoretical upper bound of the speed gain on CPUs. We note two important observations. First, there is an inevitable gap between the actual gain (solid line) and the theoretical gain (dotted line). This gap will be larger with more overhead of the framework, or more parallelization (e.g. multithreading). Second, the gap decreases as the hidden state size increases because the the overhead becomes negligible with very large matrix operations. Hence, the benefit of Skim-RNN will be greater for larger hidden state size.\nLatency. A modern GPU has much higher throughput than a CPU with parallel processing. However, for small networks, the CPU often has lower latency than the GPU. Comparing between NumPy with CPU and TensorFlow with GPU (Titan X), we observe that the former has 1.5 times lower latency (75 \\upmus vs 110 \\upmus per token) for LSTM of d=100. This means that combining Skim-RNN with CPU-based framework can lead to substantially lower latency than GPUs. For instance, Skim-RNN with CPU on IMDb has 4.5x lower latency than a GPU, requiring only 29 \\upmus per token on average.\nWe present Skim-RNN, a recurrent neural network that can dynamically decide to use the big RNN (read) or the small RNN (skim) at each time step, depending on the importance of the input. While Skim-RNN has significantly lower computational cost than its RNN counterpart, the accuracy of Skim-RNN is still on par with or better than standard RNNs, LSTM-Jump, and VCRNN. Since Skim-RNN has the same input and output interface as an RNN, it can easily replace RNNs in existing applications. We also show that a Skim-RNN can offer better latency results on a CPU compared to a standard RNN on a GPU. Future work involves using Skim-RNN for applications that require much higher hidden state size, such as video understanding, and using multiple small RNN cells for varying degrees of skimming.\nThis research was supported by the NSF (IIS 1616112), Allen Distinguished Investigator Award, Samsung GRO award, and gifts from Google, Amazon, Allen Institute for AI, and Bloomberg. We thank the anonymous reviewers for their helpful comments.\nBalduzzi & Ghifary (2016) David Balduzzi and Muhammad Ghifary. Strongly-typed recurrent neural networks. In ICML, 2016.\nCampos et al. (2017) V\u00edctor Campos, Brendan Jou, Xavier Gir\u00f3-i Nieto, Jordi Torres, and Shih-Fu Chang. Skip rnn: Learning to skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834, 2017.\nChoi et al. (2017) Eunsol Choi, Daniel Hewlett, Alexandre Lacoste, Illia Polosukhin, Jakob Uszkoreit, and Jonathan Berant. Coarse-to-fine question answering for long documents. In ACL, 2017.\nChung et al. (2017) Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In ICLR, 2017.\nDyer et al. (2016) Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network grammars. In NAACL, 2016.\nHahn & Keller (2016) Michael Hahn and Frank Keller. Modeling human reading with neural attention. In EMNLP, 2016.\nJang et al. (2017) Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017.\nJernite et al. (2017) Yacine Jernite, Edouard Grave, Armand Joulin, and Tomas Mikolov. Variable computation in recurrent neural networks. In ICLR, 2017.\nJohansen et al. (2017) Alexander Johansen, Bryan McCann, James Bradbury, and Richard Socher. Learning when to read and when to skim, 2017. URL https://metamind.io/research/learning-when-to-skim-and-when-to-read.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL, 2017.\nKembhavi et al. (2017) Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In CVPR, 2017.\nKokkinos & Potamianos (2017) Filippos Kokkinos and Alexandros Potamianos. Structural attention neural networks for improved sentiment analysis. arXiv preprint arXiv:1701.01811, 2017.\nKong et al. (2016) Lingpeng Kong, Chris Dyer, and Noah A Smith. Segmental recurrent neural networks. In ICLR, 2016.\nMarcel Adam Just (1987) Patricia Anderson Carpenter Marcel Adam Just. The Psychology of Reading and Language Comprehension. 1987.\nMikolov et al. (2015) Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc\u2019Aurelio Ranzato. Learning longer memory in recurrent neural networks. In ICLR Workshop, 2015.\nMin et al. (2017) Sewon Min, Minjoon Seo, and Hannaneh Hajishirzi.\nQuestion answering through transfer learning from large fine-grained supervision data.\nMiyato et al. (2017) Takeru Miyato, Andrew M. Dai, and Ian Goodfellow. Adversarial training methods for semi-supervised text classification. In ICLR, 2017.\nMnih et al. (2014) Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In NIPS, 2014.\nOdena et al. (2017) Augustus Odena, Dieterich Lawson, and Christopher Olah.\nChanging model behavior at test-time using reinforcement learning.\nIn ICLR Workshop, 2017.\nRastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 27169,
        "passage": " al (2019)\u2019s claim that NSP hinders BERT pretraining, especially for non-inference tasks, due to cutting context half the time; the authors reinforce Cheng et al (2019); Wang et al (2020)\u2019s proposal that NSP prediction is a semantically shallow and often solvable through lexical overlap and that using a task that requires understanding the ordering of contiguous text provides a stronger semantic signal; and the authors uphold Sun et al (2019a,b)\u2019s idea that a language model should be trained in a multi-task setting.\nProviding a signal that relays word importance, such as TF-IDF and TF, likewise produces substantial benefit to BERT pre-training.\nThe authors demonstrate the value of multi-task learning for language model pre-training; combining multiple beneficial tasks leads to better results than using any of the individual tasks alone.The authors investigate and support several reasons why next-sentence prediction is ill-suited for BERT pretraining, the authors provide better inference-based alternatives, and the authors develop other novel auxiliary tasks based on word importance and soft clustering that provide substantial benefits to BERT pre-training.\nAs with most deep learning, language representations require large datasets. While there exists corpora of labelled text, the vast majority of language data exists as raw, unlabelled text. Accordingly, many language embedding methods, and all those described below, rely solely on unsupervised or self-supervised tasks.\nSkip-Thoughts (Kiros et al, 2015) was the first deep learning sentence embedding model. Its training objective, inspired by word2vec (Mikolov et al, 2013), used RNNs to reconstruct the previous and next sentence from a given sentence. Like word2vec, similar sentences shared similar embeddings, and while it exhibited promising results, it was slow to train due to its encoding and double decoding of sentences through RNNs. Hill et al (2016)\u2019s FastSent tried to follow the same sequential sentence paradigm at a reduced training cost by encoding a sentence using a bag-of-words approach and maximizing the probability of words in adjacent sentences. Later, Quick Thoughts (Logeswaran and Lee, 2018) managed to maintain the sequential sentences objective while supporting ordered words. Using two RNN models, f (s) and g(s), they embedded a first set of sentences using f (s) and a second set consisting of the subsequent sentences using g(s). They jointly train the two models to predict the consecutive sentences from a set of candidates by comparing inner products. This resembles a referential game (David, 1969) where f (s) and g(s) are the sender and receiver respectively.\nSiddhartha Brahma. 2018. Unsupervised learning of sentence representations using sequence consistency. arXiv preprint arXiv:1808.04217.\nRich Caruana. 1997. \u201dmultitask learning\u201d. Machine Learning, 28(1):41\u201375.\nXingyi Cheng, Weidi Xu, Kunlong Chen, Wei Wang, Bin Bi, Ming Yan, Chen Wu, Luo Si, Wei Chu, and Taifeng Wang. 2019. Symmetric Regularization based BERT for Pair-wise Semantic Reasoning. arXiv preprint arXiv:1909.03405.\nZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.\nLewis David. 1969. Convention: a philosophical study. Cambridge, Harvard University Press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. CoRR, abs/1810.04805.\nDan Hendrycks and Kevin Gimpel. 2016. Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units. CoRR, abs/1606.08415.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data. arXiv preprint arXiv:1602.03483.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 201SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint arXiv:1907.10529.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294\u20133302.\nLajanugen Logeswaran and Honglak Lee. 2018. An efficient framework for learning sentence representations. arXiv preprint arXiv:1803.02893.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\nEmilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. 20Actor-mimic: Deep multitask and transfer reinforcement learning. arXiv preprint arXiv:1511.06342.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, Technical report, OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv preprint arXiv:1606.05250.\nMengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo Larochelle, and Richard S Zemel. 2018. Metalearning for semi-supervised few-shot classification. arXiv preprint arXiv:1803.00676.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. 2019. MASS: Masked sequence to sequence pre-training for language generation. arXiv preprint arXiv:1905.02450.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019a. ERNIE: Enhanced Representation through Knowledge Integration. arXiv preprint arXiv:1904.09223.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2019b. ERNIE 2.0: A continual pre-training framework for language understanding. arXiv preprint arXiv:1907.12412.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. arXiv preprint 1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. ArXiv preprint 1804.07461.\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun, Katherin Yu, Jan Hula, Patrick Xia, Raghu Pappagari, Shuning Jin, R. Thomas McCoy, Roma Patel, Yinghui Huang, Jason Phang, Edouard Grave, Haokun Liu, Najoung Kim, Phu Mon Htut, Thibault F\u2019evry, Berlin Chen, Nikita Nangia, Anhad Mohananey, Katharina Kann, Shikha Bordia, Nicolas Patry, David Benton, Ellie Pavlick, and Samuel R. Bowman. 2019b. jiant 1.2: A software toolkit for research on general-purpose text understanding models. http://jiant.info/.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi Bao, Liwei Peng, and Luo Si. 2020. StructBERT: Incorporating Language Structures into Pretraining for Deep Language Understanding. In International Conference on Learning Representations.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv pre<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 27307,
        "passage": " and avenues for future work.\nBiography: Sewon Min is a Ph.D. student in the Paul G. Allen School of Computer Science and Engineering at the University of Washington, advised by Prof. Luke Zettlemoyer and Prof. Hannaneh Hajishirzi. She is also a part time visiting researcher at Meta AI. Her research is in the area of natural language processing and machine learning. Her work specifically focuses on question answering, natural language understanding, knowledge representation and building general purpose language understanding models. She is a recipient of the 2022 JP Morgan Ph.D. Fellowship. She has co organized multiple workshops and tutorials at ACL, EMNLP, NeurIPS and AKBC, including a workshop on Machine Reading for Question Answering, a competition on Efficient Open domain Question Answering, a workshop on Representation Learning for NLP, workshop on Semiparametric Methods in NLP, and a tutorial on Zero and Few shot Learning with Pretrained Language Models. Prior to UW, she obtained a B.S. degree in Computer Science & Engineering from Seoul National University.\nPreview Day is the Viterbi School's annual visitation day for students interested in pursuing a Master's or PhD at one of the top ranked graduate engineering institutions in the nation. Attendees will meet with engineering faculty, staff and current students; and learn more about our graduate programs in engineering & computer science.\ninvestigated to accommodate expanding data traffic of the future. As one of the promising candidates, silicon photonics devices and circuits are able to improve the performance of the future wireless system.\nsuccessfully demonstrates calibration and dynamic tuning of silicon photonics filters in the mm-wave receiver from severely degraded initial magnitude response to a well-defined magnitude response.\nTexas A&M University, College Station, where he is currently a Professor. His research interests include the design of RF/mm-wave integrated circuits and systems, and integrated RF/mm-wave photonics for wireless communications and sensing.\nIEEE journal and conference papers.\nCS Town Hall: An opportunity to meet the new chair, and bring up matters of interest to our students, staff, and faculty.\nJoin the Visa Team for a Trojan Talk on Visa Culture, Hiring, and Everything in Between! Visa will be bringing along members from their Data & AI Platform team to discuss the ins-and-outs of their team and their projects.\nCome out and learn about Visa's Undergraduate, Masters, and PhD Internships, and our roles for New College Grads!\nFood will be provided, please RSVP as soon as possible! See you there!\nWhat majors and degree levels are you interested in connecting with?\nAre you interested in talking to a Visa recruiter about what will make your resume stand out?\nTime: 1:00 p.m. - 4:30 p.m. please stop by anytime between this time frame. If the room reaches max capacity, please wait outside or come back at a later time.\nCome join a group coffee chat where you can chat with one of Visas University Recruiters about everything ranging from the application process, resume dos and donts, and how to nail your interview!\nThis session is open to students interested in technical and non-technical roles who are looking to stand out among the rest in the application process! Come for conversation and coffee, and leave with a new understanding of why youd be a great fit at Visa!\nAbstract: Exercising control over the spatial degrees of freedom of the optical field has continued to yield breakthroughs over the past few decades, ranging from the discovery of Bessel beams and beams endowed with orbital angular momentum, to optical tweezers and traps, and the manipulation of the field in multimode optical fibers. Separately, but in parallel with these efforts, ultrafast pulse shaping has revolutionized our control over the temporal degree of freedom of the optical field. The spatial and temporal realms in optics have led for the most part independent lives with few examples of creative intersections. In this talk I show that precise, joint sculpting of the spatial and temporal degrees of freedom of optical fields - rather than modulating each separately - yields a new class of pulsed beams that I call'space-time' (ST) wave packets. Surprising and useful optical behaviors are exhibited by ST wave packets when freely propagating or when interacting with photonic devices, leading to a new frontier for the study of structured light. I will share our recent experimental and theoretical results from this rapidly emerging topic and sketch potential applications that could benefit from ST wave packets.\nBiography: Ayman F. Abouraddy received the B.S. and M.S. degrees from Alexandria University, Alexandria, Egypt, in 1994 and 1997, respectively, and the Ph.D. degree from Boston University, Boston, MA, in 2003, all in electrical engineering. In 2003 he joined the Massachusetts Institute of Technology (MIT) as a postdoctoral fellow, and then became a Research Scientist at the Research Laboratory of Electronics in 2005. He is the coauthor of more than 130 journal publications, 240 conference presentations, and 70 invited talks; he holds seven patents, and has three patents pending, and is a fellow of the OSA. He joined CREOL, The College of Optics & Photonics, at the University of Central Florida as an assistant professor in September 2008 and was promoted to full professor in August 2017. His recent research interests are in the area of structured light, particularly in the emerging field of space-time optics and photonics, in addition to quantum optics and quantum information processing.\nKelsey Stoerzinger's research interests span the (electro)chemical transformation of molecules into fuels, chemical feedstocks, and recovered resources. Special emphasis is put on the use of abundant elements to drive these reactions in an economical and scalable manner by renewable electricity. Surface science approaches are used to probe the reaction mechanism by in situ and operando X-ray and vibrational spectroscopies. In 2021, she received the NSF CAREER award for her work in seawater electrolysis.\nVisa Inc. Coffee Chats Day 2!\nAbstract: Antiferromagnets (AFM) materials have ordered spin moments that alternate between individual atomic sites, which gives them a vanishing macroscopic magnetic signature and picosecond intrinsic timescale. Traditionally, AFM materials have played a secondary role to ferromagnets, which are used as active elements in commercial spintronic devices like magnetic sensors and non-volatile magnetic memory. However, it was recently suggested that spin transfer torque could in principle be used to manipulate the magnetic order in AFMs, leading to either stable AFM order precessions for their use as high-frequency oscillators, or switching of the AFM order for their use as magnetic memories.\nMy presentation will focus on the physics and modeling of electrically driven spin dynamics in thin films of two unique AFMs: Cr2O3, a single-phase magnetoelectric material that can be manipulated solely with electric fields and the Weyl semi-metal Mn3Sn in which spin torque can induce chiral spin rotations. Cr2O3-based ferromagnet-free random access memory has been experimentally demonstrated, while in the case of Mn3Sn, spin torque driven dynamics were found to induce chiral oscillations, from the megahertz to the terahertz frequency range. These materials can overcome the central challenge of manipulating and reading the AFM's order parameter via microelectronics compatible circuitry, thus allowing us to develop antiferromagnetic spintronics along a similar route as ferromagnetic spintronics.\nI will discuss my group's recent work in developing new analytic models and numerical techniques to handle the complex domain dynamics across many length scales and time scales in AFM structures. I will use these models to explain recent experimental findings and bridge the gap between physics and applications development. I will conclude my talk by summarizing the limits, challenges, and opportunities of AFM spintronics for future technologies such as high-density, secure nonvolatile memory, compact narrowband terahertz sources, and spike generators.\nBiography: Shaloo Rakheja is currently an Assistant Professor in the Electrical and Computer Engineering (ECE) department at the University of Illinois at Urbana-Champaign. She is currently leading the Center for Aggressive Scaling by Advanced Processes for Electronics and Photonics (ASAP) -\u201c an Industry-University Cooperative Research Center, expected to be launched as a Phase 1 Center by the NSF in 2022. Shaloo is an expert in physics-based modeling of nanoelectronic and magnetic devices for energy-efficient computing and communication. She has developed multi-scale models, spanning from first-principles calculations to circuit-compatible implementations, for enabling materials-to-circuits co-design for a wide range of technologically relevant applications.\nAbstract: In this talk, we show how efficient reachability methods enable runtime assurance (RTA) for safe autonomy. We focus on interconnected and/or high dimensional systems and we leverage reachability techniques enabled by mixed monotone systems theory. Mixed monotonicity decomposes a dynamical system's vector field into cooperative and competitive elements, resulting in a larger dimensional monotone system for which powerful results from monotone systems theory for, e.g., reachability and invariance are applicable. Notably, these methods offer two key properties: they enable reachable set over-approximations that can be computed very fast for, e.g., inclusion at runtime in feedback controllers, and they scale to high dimensional systems such as neural networks. We demonstrate how both of these appealing features enable RTA mechanisms with provable guarantees for learning-enabled control systems.\nBiography: Samuel. Coogan is an associate professor and the Demetrius T. Paris Junior Professor at the Georgia Institute of Technology in the School of Electrical and Computer Engineering and the School of Civil and Environmental Engineering. Prior to joining Georgia Tech in 2017, he was an assistant professor at the<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 27444,
        "passage": " shows that salary trends across various sectors like finance, e-commerce, and healthcare are growing exponentially. Filter by location to see Engineer salaries in your area. ]. Salary estimates are based on 103 salaries submitted anonymously to Indeed by Machine Learning Engineer employees, users, and collected from past and present job advertisements on Indeed in the past 36 months. Like The Enterprisers Project on Facebook. For CIOs, a board of directors position represents a much-desired, little-understood career milestone. See how your offer stacks up to other pay packages and negotiate confidently. Google apparently offered Luke Zettlemoyer of the University of Washington three times his current teaching salary, which according to public records was about $180,000 per year. \u20b9296k. Get the new HBR Analytic Services report, An Executive\u2019s Guide to Real-World AI. Research Scientist... 122 job openings. That\u2019s the average salary for machine learning engineers, the highest-paying AI job title of 2019, according to job-posting site Indeed. Salary estimates are based on 1,470 salaries submitted anonymously to Glassdoor by Engineer employees. And cognitive capabilities are poised to have an impact on nearly every corporate IT function. Scientist... 46 job openings. Here\u2019s Exactly What to Write to Get Top Dollar, Artificial Intelligence Engineer Inter\u00adviews, How To Follow Up After an Interview (With Templates! Salary estimates are based on 1,532 salaries submitted anonymously to Glassdoor by Data Scientist employees. ranks number 1 out of 50 states nationwide for Civil Engineer salaries. The salaries for these AI pros are up 5.8 percent over last year \u2013 well above the average 2.9 percent expected by human resources consultancy Mercer. Median Salary The median salary is 24,700 CNY per month, which means that half (50%) of people working in Engineering are earning less than 24,700 CNY while the other half are earning more than 24,700 CNY. Let\u2019s move to our final section of \u201cHow to Become an Artificial Intelligence Engineer\u201d \u2026 How to Answer: What Are Your Strengths and Weaknesses? Stay on top of the latest thoughts, strategies and insights from enterprising peers. Machine Learning Engineer... 160 job openings. Salary estimates are based on 1,456 salaries submitted anonymously to Glassdoor by Engineer employees. The headline here is that deep learning didn\u2019t even appear on the same list last year. At the same time, Indeed saw a 15 percent decrease in candidate searches for AI roles, suggesting a potential shortage of AI experts for companies looking to hire them. Create more job alerts for related jobs with one click: 9 Attention-Grabbing Cover Letter Examples, 10 of the Best Companies for Working From Home, The Top 20 Jobs With the Highest Satisfaction, 12 Companies That Will Pay You to Travel the World, 7 Types of Companies You Should Never Work For, How to Become the Candidate Recruiters Can\u2019t Resist, Artificial Intelligence Engineer Salaries, Senior Machine Learning Engineer Salaries, 11 Words and Phrases to Use in Salary Negotiations, 10 High-Paying Jobs With Tons of Open Positions, Negotiating Over Email? You are responsible for ensuring that you have the necessary permission to reuse any work on this site. IDC says worldwide spending on cognitive and artificial intelligence systems will reach $77.6 billion during the same year. You will find a total of 7000 AI job openings in the United States, according to analysis by RPA firm UIPath. Filter by location to see Data Scientist salaries in your area. The average Infosys monthly salary ranges from approximately \u20b9 12,080 per month for Accounts Assistant to \u20b9 42,875 per month for Caterer. An AI Engineer in the New York City, NY Area area reported making $12,000 per month The national average salary for a Data Scientist is \u20b910,00,000 in India. Engineering salaries in China range from 7,850 CNY per month (minimum average salary) to 52,300 CNY per month (maximum average salary, actual maximum is higher). This is data as per Glassdoor is the average salary in India as per roles (as per 7 Jan 2020) Get a look into the base, stock, and bonus package breakdowns as well as Google's standard stock vesting schedule. The national average salary for a AI Engineer is $114,121 in United States. Shell is the oldest surviving oil and gas company in Nigeria. Salary estimates are based on 387 salaries submitted anonymously to Glassdoor by AI Engineer employees. \u00a326,000 1 crore per annum. Salary estimates are based on 387 salaries submitted anonymously to Glassdoor by AI Engineer employees. The national average salary for a Engineer is $79,825 in Canada. Salary estimates are based on 47 salaries submitted anonymously to Indeed by Robotics Engineer employees, users, and collected from past and present job advertisements on Indeed in the past 36 months. That\u2019s the average salary for machine learning engineers, the highest-paying AI job title of 2019, according to job-posting site Indeed. Engineer salaries in your area. How much does a Engineer make? 1 spot. Around one-third (34 percent) reported that there was no change in HR needs as a result of AI. Glassdoor will not work properly unless browser cookie support is enabled. Keep up with the latest thoughts, strategies, and insights from CIOs & IT leaders. Deep learning engineers rank second on Indeed\u2019s list of the top roles in job listings seeking AI or machine learning skills. The annual salary of senior-level AI Engineers with 8\u201315 years of experience ranges between \u20b95 million and \u20b910 million. 35-50 LPA, and those with over ten years of experience can earn over Rs. [ Arm yourself for IT job interviews with winning tactics and relevant data. ), 7 of the Best Situational Interview Questions. That's the number of new jobs that will be created as a result of AI-enabled automation by 2022, according to the World Economic Forum\u2019s (WEF) 2018 Future of Jobs report. The opinions expressed on this website are those of each author, not of the author's employer or of Red Hat. The average Senior AI Engineer salary in Colorado is $89,335 as of July 27, 2020, but the range typically falls between $80,518 and $95,311. The Enterprisers Project aspires to publish all content under a Creative Commons license but may not be able to do so in all cases. The average salary for \"artificial intelligence engineer\" ranges from approximately $146,693 per year for Solutions Engineer to $151,985 per year for Software Engineer. ]. Atlanta, which didn\u2019t even rank last year, came in tenth, knocking Philadelphia off the list. Let\u2019s demystify how you can prepare to win one, with this checklist of expert advice. Salary ranges can vary widely depending on the city and many other important factors, including education, certifications, additional skills, the number of years you have spent in your profession. increasingly will need AI professionals on their teams, How to explain deep learning in plain English, IT job searching in 2019: A practical guide, 5 ways cloud storage and data services enable the future of development in the AI age, How to land your first board seat: 7 steps for CIOs, Container adoption: 5 lessons on how to overcome barriers, 5 must-read Harvard Business Review articles in December. Similarly, Seattle was up for sixth, while Boston slipped one ranking down into that spot this year. For a little perspective, as of 2016, the average annual salary for computer programmers of all types is $79,840 \u2013 that according to the US Department of Labor Statistics. \u20b9639k. It\u2019s no surprise that the artificial intelligence talent market is white-hot right now. The average AI Developer salary in the United States is $94,582 as of October 28, 2020, but the salary range typically falls between $86,267 and $103,159. As of Nov 18, 2020, the average annual pay for an Artificial Intelligence Engineer in the United States is $164,769 a year. Learn how to enable cookies. The number of AI jobs listed on Indeed from May 2018 to May 2019 grew 29 percent \u2013 still significant growth, but down from the 58 percent increase in the previous 12-month period and 36 percent two years prior. Filter by location to see AI Engineer salaries in your area. Deep learning didn\u2019t even appear on the same list last year. The top metro areas for AI job openings were largely unchanged from Indeed\u2019s 2018 analysis. 8 Questions You Should Absolutely Ask An Interviewer, Electronic Arts AI Engineer salaries - 5 salaries reported, Interactions AI Engineer salaries - 2 salaries reported, Invitae AI Engineer salaries - 2 salaries reported, Red Storm Entertainment AI Engineer salaries - 2 salaries reported, Target AI Engineer salaries - 1 salaries reported, Accenture AI Engineer salaries - 1 salaries reported, Google AI Engineer salaries - 1 salaries reported, Northrop Grumman AI Engineer salaries - 1 salaries reported, Bloomberg L.P. AI Engineer salaries - 1 salaries reported, Delta Air Lines AI Engineer salaries - 1 salaries reported, Duke Energy AI Engineer salaries - 1 salaries reported, Capco AI Engineer salaries - 1 salaries reported, Zozi AI Engineer salaries - 1 salaries reported, Soar Tech AI Engineer salaries - 1 salaries reported, Beyond Limits AI Engineer salaries - 1 salaries reported, Viome AI Engineer salaries - 1 salaries reported, VG Holding AI Engineer salaries - 1 salaries reported, SNT Media AI Engineer salaries - 1 salaries reported, Mya Systems AI Engineer salaries - 1 salaries reported, Arkane Studios AI Engineer salaries - 1 salaries reported. $123,440. The first table below provides salary benchmarking<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 27660,
        "passage": "I'm a senior PhD student in the Language Technologies Institute, in the School of Computer Science, at Carnegie Mellon University (though my advisor moved to the University of Washington, so I live in Seattle). I'm primarily interested in statistical machine learning and natural language processing, though many problems elswhere also catch my attention. I've worked on a wide variety of projects, including generating descriptions of images, semantic parsing, temporal grounding of events, question answering with neural networks, hyperparameter optimization, structured sparsity in neural networks, and reproducibility in machine learning. I've been very lucky to work with some fantastic people. I currently am advised by Noah Smith. As an undergrad at the University of Washington my honors thesis was advised by Luke Zettlemoyer.\nDuring the summer of 2011, I was selected to participate in the John's Hopkins summer workshop as an undergrad, where I worked with a host of fantastic people (and got a great sweatshirt). I was on the vision and language team; you can see the website for our project here.\nIn the summer and fall of 2015 I interned at Facebook AI Research in NYC with Jason Weston and Antoine Bordes, where I built the Movie Dialog dataset and the MovieQA dataset. Slate.com wrote an article that covered some of my work!\nIn the summer of 2018 I interned at Google AI with Elad Eban, where I worked with the MorphNet team. I got a shoutout in their blog post.\nAs of the summer of 2019, I am currently interning at the Allen Institute for Artificial Intelligence with my advisor, Noah Smith, on the AllenNLP team.\nInternational Journal of Computer Vision, 2015.\nInternational Workshop on Semantic Evaluations (SemEval), 2014.\nAssociation for Computational Linguistic (ACL), 2014.\nEuropean Chapter of the Association for computational Linguistics (EACL), 2012.\nNorth American Chapter of the Association for Computational Linguistics (NAACL), 2012.\nJHU-CLSP Summer Workshop Whitepaper, 2011.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 27828,
        "passage": " tasks.\nWe are grateful to Luheng He for helpful discussions and code, Timothy Dozat for sharing his code, and to the NLP reading groups at Google and UMass and the anonymous reviewers for feedback on drafts of this work. This work was supported in part by an IBM PhD Fellowship Award to E.S., in part by the Center for Intelligent Information Retrieval, and in part by the National Science Foundation under Grant Nos. DMR-1534431 and IIS-1514053. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.\nAbadi et al. (2015) Mart\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. 2015.\nTensorflow: Large-scale machine learning on heterogeneous systems, 2015.\nAlonso and Plank (2017) H\u00e9ctor Mart\u00ednez Alonso and Barbara Plank. 2017. When is multitask learning effective? semantic sequence prediction under varying data conditions. In EACL.\n, pages 2005\u20132010.\nBazrafshan and Gildea (2013) Marzieh Bazrafshan and Daniel Gildea. 2013. Semantic roles for string to tree machine translation. In ACL.\nBengio et al. (2015) Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015.\nBengio et al. (1994) Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157\u2013166.\nBerant et al. (2014) Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Brad Huang, Christopher D. Manning, Abby Vander Linden, Brittany Harding, and Peter Clark. 2014. Modeling biological processes for reading comprehension. In EMNLP.\nBingel and S\u00f8gaard (2017) Joachim Bingel and Anders S\u00f8gaard. 2017. Identifying beneficial task relations for multi-task learning in deep neural networks. In EACL.\nCarreras and M\u00e0rquez (2005) Xavier Carreras and Llu\u00eds M\u00e0rquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In CoNLL.\nCaruana (1993) Rich Caruana. 1993. Multitask learning: a knowledge-based source of inductive bias. In ICML.\nChang et al. (2015) Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daum\u00e9 III, and John Langford. 2015. Learning to search better than your teacher. In ICML.\nChen et al. (2013) Yun-Nung Chen, William Yang Wang, and Alexander I Rudnicky. 2013. Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing. In Proc. of ASRU-IEEE.\nChoi and Palmer (2011) Jinho D. Choi and Martha Palmer. 2011. Getting the most out of transition-based dependency parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: short papers, pages 687\u2013692.\nCollobert et al. (2011) Ronan Collobert, Jason Weston, L\u00e9on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493\u20132537.\nDaum\u00e9 III et al. (2009) Hal Daum\u00e9 III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine Learning, 75(3):297\u2013325.\nDozat (2016) Timothy Dozat. 2016.\nIn ICLR Workshop track.\nDozat and Manning (2017) Timothy Dozat and Christopher D. Manning. 2017. Deep biaffine attention for neural dependency parsing. In ICLR.\nFitzGerald et al. (2015) Nicholas FitzGerald, Oscar T\u00e4ckstr\u00f6m, Kuzman Ganchev, and Dipanjan Das. 2015. Semantic role labeling with neural network factors. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 960\u2013970.\nFrancis and Ku\u010dera (1964) W. N. Francis and H. Ku\u010dera. 1964. Manual of information to accompany a standard corpus of present-day edited american english, for use with digital computers. Technical report, Department of Linguistics, Brown University, Providence, Rhode Island.\nGoldberg and Nivre (2012) Yoav Goldberg and Joakim Nivre. 2012. A dynamic oracle for arc-eager dependency parsing. In Proceedings of COLING 2012: Technical Papers, pages 959\u2013976.\nHashimoto et al. (2017) Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A joint many-task model: Growing a neural network for multiple nlp tasks. In Conference on Empirical Methods in Natural Language Processing.\nHe et al. (2018) Luheng He, Kenton Lee, Omer Levy, and Luke Zettlemoyer. 2018. Jointly predicting predicates and arguments in neural semantic role labeling. In ACL.\nHe et al. (2017) Luheng He, Kenton Lee, Mike Lewis, and Luke Zettlemoyer. 2017. Deep semantic role labeling: What works and what\u2019s next. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.\nJohansson and Nugues (2008) Richard Johansson and Pierre Nugues. 2008. Dependency-based semantic role labeling of propbank. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69\u201378.\nKingma and Ba (2015) Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference for Learning Representations (ICLR), San Diego, California, USA.\nKipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In International Conference on Learning Representations.\nLee et al. (2017) Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. 2017. End-to-end neural coreference resolution. In EMNLP.\nLevin (1993) Beth Levin. 1993. English verb classes and alternations: A preliminary investigation. University of Chicago press.\nLewis et al. (2015) Mike Lewis, Luheng He, and Luke Zettlemoyer. 2015. Joint A* CCG Parsing and Semantic Role Labeling. In EMNLP.\nLiu and Gildea (2010) Ding Liu and Daniel Gildea. 2010. Semantic role features for machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING).\nLiu and Lapata (2018) Yang Liu and Mirella Lapata. 2018. Learning structured text representations. Transactions of the Association for Computational Linguistics, 6:63\u201375.\nMaas et al. (2013) Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. 2013. Rectifier nonlinearities improve neural network acoustic models. In ICML, volume 30.\nMarcheggiani et al. (2017) Diego Marcheggiani, Anton Frolov, and Ivan Titov. 2017. A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling. In CoNLL.\nMarcheggiani and Titov (2017) Diego Marcheggiani and Ivan Titov. 2017. Encoding sentences with graph convolutional networks for semantic role labeling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP).\nMarcus et al. (1993) Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn TreeBank. Computational Linguistics \u2013 Special issue on using large corpora: II, 19(2):313\u2013330.\nde Marneffe and Manning (2008) Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The stanford typed dependencies representation. In COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation.\nNesterov (1983) Yurii Nesterov. 1983. A method of solving a convex programming problem with convergence rate o(1/k2). volume 27, pages 372\u2013376.\nPalmer et al. (2005) Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1).\nPascanu et al. (2013) Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. In Proceedings of the 30 th International Conference on Machine Learning.\nPeng et al. (2017) Hao Peng, Sam Thomson, and<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 28065,
        "passage": " \u00a3130,000. ]. According to Indeed the Average Salary of an Artificial Intelligence Engineer is around $110,000 per Annum, with a minimum of $105,244 and a maximum of $144,611. The community relies on everyone sharing \u2013.css-1c7uf6v{background:none;border:none;font-size:15px;color:#1861bf;}Add Anonymous Salary. Filter by location to see Engineer salaries in your area. Entry Level Salary for Petroleum Engineers: N150,000 to N500,000 Filter by location to see AI. The number of AI jobs listed on Indeed from May 2018 to May 2019 grew 29 percent \u2013 still significant growth, but down from the 58 percent increase in the previous 12-month period and 36 percent two years prior. It\u2019s no surprise that the artificial intelligence talent market is white-hot right now. ranks number 1 out of 50 states nationwide for Civil Engineer salaries. Let\u2019s demystify how you can prepare to win one, with this checklist of expert advice. Median Salary The median salary is 24,700 CNY per month, which means that half (50%) of people working in Engineering are earning less than 24,700 CNY while the other half are earning more than 24,700 CNY. per year. The top metro areas for AI job openings were largely unchanged from Indeed\u2019s 2018 analysis. But as a beginner, you may earn around 4\u20135 lakhs pa (10 + lks pa if its an MNC). Get our new eBook: IT job searching in 2019: A practical guide. Salary estimates are based on 47 salaries submitted anonymously to Glassdoor by Aircraft Engineer employees. \u00a326,000 Google apparently offered Luke Zettlemoyer of the University of Washington three times his current teaching salary, which according to public records was about $180,000 per year. The average Senior AI Engineer salary in Colorado is $89,335 as of July 27, 2020, but the range typically falls between $80,518 and $95,311. [ Read also: How to explain deep learning in plain English. Here\u2019s Exactly What to Write to Get Top Dollar, Artificial Intelligence Engineer Inter\u00adviews, How To Follow Up After an Interview (With Templates! Ai Engineers salaries are based on 9 salary reports submitted by Ai Engineers employees for a total of 5 different job titles.The maximum reported salary at Ai Engineers is $128k per year reported by Architectural And Engineering Managers based out of MIDDLETOWN, CT. Salary ranges can vary widely depending on the city and many other important factors, including education, certifications, additional skills, the number of years you have spent in your profession. Just in case you need a simple salary calculator, that works out to be approximately $79.22 an hour. We gathered some of the more telling figures related to AI salaries, including which skills are seeing the biggest paydays, the most in-demand roles, shifts in supply and demand dynamics, hot hiring geographies, and the net number of jobs AI may create. As we collect more salary reports, we will be able to display related salaries for this job title. As of Nov 18, 2020, the average annual pay for an Artificial Intelligence Engineer in the United States is $164,769 a year. Atlanta, which didn\u2019t even rank last year, came in tenth, knocking Philadelphia off the list. The median compensation package totals $243k. Petroleum Engineer Salary Per Month in Nigeria Shell Nigeria Salary Per Month. The national average salary for a Engineer is $79,825 in Canada. Like The Enterprisers Project on Facebook. Engineer salaries in your area. ]. Glassdoor will not work properly unless browser cookie support is enabled. Salary estimates are based on 1,456 salaries submitted anonymously to Glassdoor by Engineer employees. An AI Engineer in the New York City, NY Area area reported making $12,000 per month Stay on top of the latest thoughts, strategies and insights from enterprising peers. per year. The opinions expressed on this website are those of each author, not of the author's employer or of Red Hat. These salary estimates are based on actual salaries drawn (Source: Naukri Career Navigator). AI and machine learning have the potential to create an additional $2.6 trillion in value for marketing and sales by 2020 and as much as $2 trillion in manufacturing and supply chain planning, according to the McKinsey Global Institute. That\u2019s the average salary for machine learning engineers, the highest-paying AI job title of 2019, according to job-posting site Indeed. $147,689. The first table below provides salary benchmarking and summary statistics including a comparison to same period in the previous 2 years. How to Answer: What Are Your Strengths and Weaknesses? The machine learning engineer title came in first, while numbers three through five were senior data scientist, computer vision engineer, and data scientist. For the last decade, her work has focused on the intersection of business and technology. Deep learning engineers specialize in using deep learning platforms to perform specific types of programming tasks related to AI. The nation\u2019s capital rose one position, while San Jose came down a notch. So the average salary for a fresh Software Engineer (or a person within the experience level of 0 to 3 years) is about 3.6 lakhs per year. Software Engineer... 369 job openings. \u20b9639k. Forty percent of respondents from Global 2000 organizations say they are adding more jobs as a result of AI adoption, according to a 2019 Dun & Bradstreet report, while only eight percent are cutting jobs because of the new technology. Copyright \u00a9 2008\u20132020, Glassdoor, Inc. \"Glassdoor\" and logo are registered trademarks of Glassdoor, Inc. Not enough reports to show salary distribution. The average Infosys salary ranges from approximately \u20b9 1,72,591 per year for IT Engineer to \u20b9 14,51,654 per year for Senior Consultant. Salary distribution for jobs citing Artificial Intelligence over the 6 months to 20 November 2020. Salary Trend. Many West Coast-based tech companies (such as Amazon, Facebook, and Google) have a significant presence in the region. Filter by location to see Aircraft Engineer salaries in your area. A note on advertising: The Enterprisers Project does not sell advertising on the site or in any of its newsletters. ]. That puts America behind China, which has 12,000 openings, and ahead of the United Kingdom, India, Germany, France, Canada, Australia, and Poland. Subscribe to get the latest thoughts, strategies, and insights from enterprising peers. Artificial Intelligence Salary Histogram. How much does a Engineer make? Just eight percent of the 100 business executives surveyed said that their companies were planning to eliminate jobs as a result of implementing AI capabilities. The Enterprisers Project aspires to publish all content under a Creative Commons license but may not be able to do so in all cases. The average salary for \"artificial intelligence engineer\" ranges from approximately $146,693 per year for Solutions Engineer to $151,985 per year for Software Engineer. According to Paysa, yet another job search site, an artificial intelligence engineer earns an average of $171,715, ranging from $124,542 at the 25th percentile to $201,853 at the 75th percentile, with top earners earning more than $257,530. Senior Software Engineer... 21 job openings. $123,440. Filter by location to see Engineer salaries in your area. That's the number of new jobs that will be created as a result of AI-enabled automation by 2022, according to the World Economic Forum\u2019s (WEF) 2018 Future of Jobs report. Salary range (per year): INR 450,000 \u2013 INR 2,654,000. The salaries for these AI pros are up 5.8 percent over last year \u2013 well above the average 2.9 percent expected by human resources consultancy Mercer. During the previous 12-month period, those candidates searches jumped 32 percent. Is this helpful? Algorithm engineer salaries also jumped $5,201 or about 5% over last year. Deep learning engineers rank second on Indeed\u2019s list of the top roles in job listings seeking AI or machine learning skills. As Indeed\u2019s blog pointed out, \u201cNew York\u2019s top position may surprise you, until you consider that it\u2019s the home of diverse industries, from financial services to publishing \u2014 many of which are now adopting AI. A Software Engineer can expect an average starting salary of \u00a326,000. \u20b95m. Salary information comes from 1,306 data points collected directly from employees, users, and past and present job advertisements on Indeed in the past 36 months. The national average salary for a AI Engineer is $114,121 in United States. For instance, a full stack developer in India can earn anything between 3 to 10 lakhs per year. The national average salary for a Engineer is $80,000 in Canada. The national average salary for a Aircraft Engineer is \u00a340,771 in United Kingdom. Filter by location to see Data Scientist salaries in your area. 1 spot. It is also one of the highest paying oil companies in Nigeria. Research Scientist... 122 job openings. The Enterprisers Project is an online publication and community focused on connecting CIOs and senior IT leaders with the \"who, what, and how\" of IT-driven business innovation. How much does a Engineer make? The annual salary of senior-level AI Engineers with 8\u201315 years of experience ranges between \u20b95 million and \u20b910 million. Source. Deep Learning Engineer. This is data as per Glassdoor is the average salary in India as per roles (as per 7 Jan 2020) This is the equivalent of $3,169/<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 29249,
        "passage": "ett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the International Workshop on Paraphrasing.\nDong et al. (2019) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. arXiv preprint arXiv:1905.03197.\nGiampiccolo et al. (2007) Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing.\nGokaslan and Cohen (2019) Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus. http://web.archive.org/save/http://Skylion007.github.io/OpenWebTextCorpus.\nHamborg et al. (2017) Felix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. 2017. news-please: A generic news crawler and extractor. In Proceedings of the 15th International Symposium of Information Science.\nHonnibal and Montani (2017) Matthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear.\nIyer et al. (2016) Shankar Iyer, Nikhil Dandekar, and Korn\u00e9l Csernai. 2016. First quora dataset release: Question pairs. https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs.\nJoshi et al. (2019) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2019. SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint arXiv:1907.10529.\nKingma and Ba (2015) Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR).\nKocijan et al. (2019) Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz. 2019. A surprisingly robust trick for winograd schema challenge. arXiv preprint arXiv:1905.06290.\nLai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683.\nLevesque et al. (2011) Hector J Levesque, Ernest Davis, and Leora Morgenstern. 2011. The Winograd schema challenge. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning.\nLiu et al. (2019a) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019a. Improving multi-task deep neural networks via knowledge distillation for natural language understanding. arXiv preprint arXiv:1904.09482.\nLiu et al. (2019b) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019b. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504.\nMcCann et al. (2017) Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems (NIPS), pages 6297\u20136308.\nMicikevicius et al. (2018) Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 2018. Mixed precision training. In International Conference on Learning Representations.\nNagel (2016) Sebastian Nagel. 2016. Cc-news. http://web.archive.org/save/http://commoncrawl.org/2016/10/news-dataset-available.\nOtt et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In North American Association for Computational Linguistics (NAACL): System Demonstrations.\nOtt et al. (2018) Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation (WMT).\nPeters et al. (2018) Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In North American Association for Computational Linguistics (NAACL).\nRadford et al. (2018) Alec Radford, Karthik Narasimhan, Time Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI.\nRajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable questions for squad. In Association for Computational Linguistics (ACL).\nSennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Association for Computational Linguistics (ACL), pages 1715\u20131725.\nSocher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Empirical Methods in Natural Language Processing (EMNLP).\nSong et al. (2019) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2019. MASS: Masked sequence to sequence pre-training for language generation. In International Conference on Machine Learning (ICML).\nSun et al. (2019) Yu Stephanie Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xinlun Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223.\nTrinh and Le (2018) Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847.\nVaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems.\nWang et al. (2019a) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. arXiv preprint 1905.00537.\nWang et al. (2019b) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations (ICLR).\nWarstadt et al. (2018) Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2018. Neural network acceptability judgments. arXiv preprint 1805.12471.\nWilliams et al. (2018) Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In North American Association for Computational Linguistics (NAACL).\nYang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.\nYou et al. (2019) Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh. 2019. Reducing bert pre-training time from 3 days to 76 minutes. arXiv preprint arXiv:1904.00962.<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 29346,
        "passage": " Lewis, and Luke Zettlemoyer. 2016. Human-in-the-loop parsing. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2337\u20132342, Austin, Texas.\nIyer et al. (2017) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. 2017. Learning a neural semantic parser from user feedback. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 963\u2013973, Vancouver, Canada.\nJia and Liang (2016) Robin Jia and Percy Liang. 2016. Data recombination for neural semantic parsing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12\u201322, Berlin, Germany.\nKate and Mooney (2006) Rohit J Kate and Raymond J Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 913\u2013920, Sydney, Australia.\nKate et al. (2005) Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning to Transform Natural to Formal Languages. In Proceedings for the 20th National Conference on Artificial Intelligence, pages 1062\u20131068, Pittsburgh, Pennsylvania.\nKo\u010disk\u00fd et al. (2016) Tom\u00e1\u0161 Ko\u010disk\u00fd, G\u00e1bor Melis, Edward Grefenstette, Chris Dyer, Wang Ling, Phil Blunsom, and Karl Moritz Hermann. 2016. Semantic parsing with semi-supervised sequential autoencoders. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1078\u20131087, Austin, Texas.\nKrishnamurthy (2016) Jayant Krishnamurthy. 2016. Probabilistic models for learning a semantic parser lexicon. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 606\u2013616.\nKrishnamurthy and Mitchell (2012) Jayant Krishnamurthy and Tom Mitchell. 2012. Weakly supervised training of semantic parsers. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 754\u2013765, Jeju Island, Korea.\nKrishnamurthy and Mitchell (2015) Jayant Krishnamurthy and Tom M Mitchell. 2015. Learning a compositional semantics for freebase with an open predicate vocabulary. Transactions of the Association for Computational Linguistics, 3:257\u2013270.\nKwiatkowksi et al. (2010) Tom Kwiatkowksi, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223\u20131233, Cambridge, MA.\nKwiatkowski et al. (2013) Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545\u20131556, Seattle, Washington, USA.\nKwiatkowski et al. (2011) Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in CCG grammar induction for semantic parsing. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1512\u20131523, Edinburgh, Scotland.\nLiang et al. (2016) Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. 2016. Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision. arXiv preprint arXiv:1611.00020.\nLiang et al. (2011) Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 590\u2013599, Portland, Oregon.\nLu et al. (2008) Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 783\u2013792, Honolulu, Hawaii.\nMarcheggiani and Titov (2016) Diego Marcheggiani and Ivan Titov. 2016. Discrete-state variational autoencoders for joint discovery and factorization of relations. Transactions of the Association for Computational Linguistics, 4:231\u2013244.\n, pages 1671\u20131678, Edinburgh, Scotland.\nMiao and Blunsom (2016) Yishu Miao and Phil Blunsom. 2016. Language as a latent variable: Discrete generative models for sentence compression. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 319\u2013328, Austin, Texas.\nMnih and Gregor (2014) Andriy Mnih and Karol Gregor. 2014. Neural variational inference and learning in belief networks. In Proceedings of the 31st International Conference on Machine Learning, pages 1791\u20131799, Bejing, China.\nPasupat and Liang (2016) Panupong Pasupat and Percy Liang. 2016. Inferring logical forms from denotations. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 23\u201332, Berlin, Germany.\nPennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.\nReddy et al. (2017) Siva Reddy, Oscar T\u00e4ckstr\u00f6m, Slav Petrov, Mark Steedman, and Mirella Lapata. 2017. Universal semantic parsing. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 89\u2013101, Copenhagen, Denmark.\nSu et al. (2016) Yu Su, Huan Sun, Brian Sadler, Mudhakar Srivatsa, Izzeddin Gur, Zenghui Yan, and Xifeng Yan. 2016. On generating characteristic-rich question sets for qa evaluation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 562\u2013572, Austin, Texas.\nIn Proceedings of the 30th International Conference on Machine Learning, pages 1139\u20131147, Atlanta, Georgia.\nWen et al. (2015) Tsung-Hsien Wen, Milica Gasic, Nikola Mrk\u0161i\u0107, Pei-Hao Su, David Vandyke, and Steve Young. 2015.\nSemantically conditioned LSTM-based natural language generation for spoken dialogue systems.\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1711\u20131721, Lisbon, Portugal.\nWong and Mooney (2006) Yuk Wah Wong and Raymond Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 439\u2013446, New York City, USA.\nXu et al. (2016) Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2016. Question answering on Freebase via relation extraction and textual evidence. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2326\u20132336, Berlin, Germany.\nYao and Van Durme (2014) Xuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with Freebase. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 956\u2013966, Baltimore, Maryland.\nYih et al. (2015) Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1321\u20131331, Beijing, China.\nYin et al. (2018) Pengcheng Yin, Chunting Zhou, Junxian He, and Graham Neubig. 2018. StructVAE: Tree-structured latent variable models for semi-supervised semantic parsing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 754\u2013765, Melbourne, Australia.\nZelle and Mooney (1996) John M. Zelle and Raymond J. Mooney. 1996.\nIn Proceedings of the 13th National Conference on Artificial Intelligence, pages 1050\u20131055, Portland, Oregon.\nZett<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 29656,
        "passage": "Text-to-SQL has recently attracted much attention as a sequence-to-sequence learning problem due to its practical usage for search and question answering (Dong and Lapata, 2016; Zhong et al, 2017; Xu et al, 2017; Cai et al, 2018; Yu et al, 2018a; Dong and Lapata, 2018; FineganDollak et al, 2018; Yu et al, 2018b; Wang et al, 2017b; Shi et al, 2017).\nMost of the previous text-to-SQL tasks assumed that all questions came from a fixed database and share one global table schema.\nThese type information come from either external knowledge graph, a column or a number.\nThis dataset was designed for translating natural language questions to SQL queries using the corresponding table columns without access to the table content.\nThis dataset is further split into training and testing sets that are separately obtained from different Wiki pages, assuming there is no overlap of tables between training and testing sets.\nTable 1 shows the overall and breakdown results on full WikiSQL dataset.\nThe authors compare the models with strong baseline models on the original WikiSQL test data.\nAll these models have no access to table content following (Zhong et al, 2017).\nFirst the Gen-model with enhanced encoder/decoder improves over the baseline coarse-to-fine model by 1.6% in accuracy of both.\nIn this paper, the authors propose a novel auxiliary mapping task for zero-shot text-to-SQL learning.\nTraditional seq2seq generation model is augmented with an explicit mapping model from question words to table schema.\nThe mapping model serves as an enhancement model to text-to-SQL task as well as regularization to the generation model to increase its generalization.\nRecently neural network based approaches, especially sequence-to-sequence models have been applied to text-to-SQL successfully with progressively improving results (Wang et al, 2017a; Neelakantan et al, 2017; Iyer et al, 2017; Yin and Neubig, 2017; Huang et al, 2018; Zhong et al, 2017; Xu et al, 2017; Cai et al, 2018; Yu et al, 2018a; Dong and Lapata, 2018; Finegan-Dollak et al, 2018).\nAnkur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and Larry Heck. 2017. Towards zero-shot frame semantic parsing for domain scaling. arXiv preprint arXiv:1707.02363.\nRuichu Cai, Boyan Xu, Xiaoyan Yang, Zhenjie Zhang, and Zijian Li. 2018. An encoder-decoder framework translating natural language to database queries. In Proceedings of the 27th International Joint Conference on Artificial Intelligence.\nDeborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. 1994. Expanding the scope of the atis task: The atis-3 corpus. In Proceedings of the Workshop on Human Language Technology, pages 43\u201348.\nLi Dong and Mirella Lapata. 2016. Language to logical form with neural attention. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 33\u201343.\nLi Dong and Mirella Lapata. 2018. Coarse-to-fine decoding for neural semantic parsing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 731\u2013742.\nCathetine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. 2018. Improving text-to-sql evaluation methodology. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 351\u2013360.\nJonathan Herzig and Jonathan Berant. 2018. Decoupling structure and lexicon for zero-shot semantic parsing. arXiv preprint arXiv:1804.07918.\nPo-Sen Huang, Chenglong Wang, Rishabh Singh, Wentau Yih, and Xiaodong He. 201Natural language to structured query generation via meta-learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 732\u2013738.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. 2017. Learning a neural semantic parser from user feedback. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 963\u2013973.\nArvind Neelakantan, Quoc V Le, Martin Abadi, Andrew MacCallum, and Dario Amodei. 2017. Learning a natural language interface with neural programmer. In Proceedings of the 5th International Conference on Learning Representations, pages 1\u2013 10.\nC\u0131cero Nogue<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 29691,
        "passage": " Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pages 823\u2013833.\nGalley et al. (2004) Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What\u2019s in a translation rule? In Proceedings of Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics. HLT-NAACL \u201904.\nJunczys-Dowmunt et al. (2016) Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions. In Proceedings of the IWSLT 2016.\nKalchbrenner and Blunsom (2013) Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle, Washington, USA, pages 1700\u20131709.\nKoehn and Hoang (2007) Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pages 868\u2013876.\nLewis et al. (2015) Mike Lewis, Luheng He, and Luke Zettlemoyer. 2015. Joint a* ccg parsing and semantic role labelling. In Empirical Methods in Natural Language Processing.\nLuong et al. (2016) Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2016. Multi-task sequence to sequence learning. In Proceedings of International Conference on Learning Representations (ICLR 2016).\nMart\u00ednez et al. (2016) Mercedes Garc\u00eda Mart\u00ednez, Lo\u00efc Barrault, and Fethi Bougares. 2016. Factored Neural Machine Translation Architectures. In International Workshop on Spoken Language Translation (IWSLT\u201916).\nMenezes and Quirk (2007) Arul Menezes and Chris Quirk. 2007. Using dependency order templates to improve generality in translation. In Proceedings of the Second Workshop on Statistical Machine Translation. pages 1\u20138.\nNiehues et al. (2016) Jan Niehues, Thanh-Le Ha, Eunah Cho, and Alex Waibel. 2016. Using factored word representation in neural network language models. In Proceedings of the First Conference on Machine Translation. Berlin, Germany.\nPapineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA, USA, ACL \u201902, pages 311\u2013318.\nSennrich (2015) Rico Sennrich. 2015. Modelling and Optimizing on Syntactic N-Grams for Statistical Machine Translation. Transactions of the Association for Computational Linguistics 3:169\u2013182.\nSennrich and Haddow (2016) Rico Sennrich and Barry Haddow. 2016. Linguistic input features improve neural machine translation. In Proceedings of the First Conference on Machine Translation. Berlin, Germany, pages 83\u201391.\nSennrich et al. (2016a) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016a. Edinburgh neural machine translation systems for wmt 16. In Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics, Berlin, Germany, pages 371\u2013376.\nSennrich et al. (2016b) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016b. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany.\nShi et al. (2016) Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does string-based neural mt learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 1526\u20131534.\nSteedman (2000) Mark Steedman. 2000. The syntactic process, volume 24. MIT Press.\nSutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems. NIPS\u201914, pages 3104\u20133112.\nWilliams and Koehn (2012) Philip Williams and Philipp Koehn. 2012. Ghkm rule extraction and scope-3 parsing in moses. In Proceedings of the Seventh Workshop on Statistical Machine Translation. pages 388\u2013394.\nWe give a few more examples in Figure 5 and Figure 6.\nIn the example RO-EN Preposition the baseline NMT system drops the preopositional modifier of the verb \u201cconsider\u201d. The SNMT predicts the correct subcategorization frame for the verb see and correctly translates the prepositional modifier \u201cas a problem\u201d.\nIn the second DE-EN Subordinate example the baseline NMT inverts the role of the arguments for the verb \u201csupport (unterst\u00fctzen)\u201d. The SNMT system produces the correct syntactic structure of the subordinate clause and translates \u201csie\u201d as \u201cthey\u201d, the subject.\nIn the example EN-DE Raising the baseline NMT system translates the raising construct \u201cwants \u2026 to be seen\u201d with the incorrect infinitive verb form \u201czu sehen\u201d. In contrast the SNMT system produces the correct translation for a subordinate sentence \u201cgesehen werden\u201d. Furthermore the SNMT system produces the correct nominative inflection for the coordinated subject of the raising construct \u201cseine Mitgliedschaft im Schachclub \u2026 und sein freundlicher Kontakt\u201d, while the NMT system inflects the second part as accusative \u201cseinen freundlichen Kontakt\u201d.\nIn the example EN-DE Suboordination the baseline NMT system mistranslates the subordinate clause \u201cwhich lists 17 faculty members\u201d as \u201cdie 17 Fakult\u00e4ten Mitglieder\u201d which drops the verb \u201clists\u201d and the relative pronoun \u201cwhich\u201d. In contrast the SNMT correctly translates the verb at the end of the clause as well as the relative pronoun \u201cin denen 17 Fakult\u00e4tsmitglieder aufgef\u00fchrt sind\u201d. A mistake that both system make is the incorrect disambiguation of the verb \u201ctook\u201d which is translated as \u201cnahmen\u201d instead of \u201cbesuchten\u201d.\nIn the second EN-DE Suboordination example the baseline NMT system mistranslates the subordinate clause \u201cwho say the same of Trump\u201d, as it fails to correctly order the target verb at the end of the clause \u201cdie sagen, das Gleiche von Trump\u201d. In contrast the SNMT system translates the verb at the end of the subordinate clause \u201cdie das Gleiche von Trump sagen\u201d.\nIn the example EN-DE Question and Coordination the baseline NMT system does not predict the correct target order of the verb \u201cwaste (vergeuden)\u201d and its direct object \u201cpolitical capital (politisches Kapital)\u201d. In contrast the SNMT system correctly reorders the target verb \u201cverschwenden\u201d at the end of the clause. Moreover the SNMT system correctly identifies the coordinated subject \u201cParis or Berlin\u201d and correctly inflects the auxiliary verb \u201cshould\u201d to the plural form in German \u201csollten\u201d.\nmax width=1 RO - EN* Preposition Source Majoritatea republicanilor nu consider\u0103 temperamentul lui Trump o problem\u0192\u00c9. Reference A majority of Republicans don\u2019t see Trump\u2019s temperament as a problem. NMT The Republican majority do not consider Trump\u2019s temperament. SNMT Most republicans do not see((S[b]\u2216NP)/PP)/NP Trump\u2019s temperate as a problem. DE - EN* Subordinate Source Mehr als f\u00fcnf Monate vor Beginn der Vorwahlen, sagen die meisten demokratischen W\u00e4hler, dass es zu fr\u00fch ist, um zu sagen, dass ihre Meinung feststeht, welchen Kandidaten sie unterst\u00fctzen werden. Reference More than five months before the start of the primary contests, most Democratic voters say it is too early to say that their minds are made up about which candidate they will support. NMT More than five months before the start of the pre-elections, most democratic voters say that it is too early to say that their opinion is defined, which candidates will support them. SNMT More than five months before the start of the preliminary elections, most democratic voters say that it is too early to say that their opinion is determined, which candidates they will support.\nFigure 5: Comparison of baseline NMT and syntax-aware NMT (SNMT) with target-side CCG supertags for Romanian\u2192English and German\u2192English.\nmax width=1 EN - DE Raising Source Gauselmann wants his membership of the chess club as well as his friendly contact with the \u201c Red-white \u201d tennis club to be seen as an expression of his ties with the spa town<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 29810,
        "passage": " will be close to one another. The next development was towards compositional distributional semantics, where sentence-level representations are composed from word representations. These were more useful for question answering.\nIyyer et al. reduced dependency parse trees to vector representations that were used to train an RNN. Yu et al. used a CNN for answer selection. A common approach to answer selection is to look at the similarity between question and answer in the semantic space. Later models added an attention layer between the question and its candidate answers. Tan et al. evaluated BiLSTMs with attention and CNN. Dynamic Coattention Network (DCN) is also based on attention. Facebook researchers combined a seq2seq model with multitasking.\nTransformer architecture has been applied for QA. In fact, QA was one of the tasks to which BERT was fine-tuned (on SQuAD) and evaluated. BERTserini used fine-tuned BERT along with information retrieval from Wikipedia.\nWhat are some useful datasets for training or evaluating question answering models?\nDatasets are used for training and evaluating QA systems. Based on the design and makeup, each dataset might evaluate different aspects of the system better.\nAmong the well-known datasets are Stanford Question Answering Dataset (SQuAD), Natural Question (NQ), Question Answering in Context (QuAC) and HotpotQA. All four are based on Wikipedia content. Conversational Question Answering (CoQA) is a dataset that's based on Wikipedia plus other sources. Wikipedia often presents data in tables. WikiTableQuestions is a dataset in which answers are in tables rather than freeform text. TyDi QA is a multilingual dataset. TweetQA takes its data from Twitter.\nQuestion Answering over Linked Data (QALD) is a series of datasets created from knowledge bases such as DBpedia, MusicBrainz, Drugbank and LinkedSpending.\nOther datasets to note are ELI5, ShARC, MS MARCO, NewsQA, CMU Wikipedia Factoid QA, CNN/DailyMail QA, Microsoft WikiQA, Quora Question Pairs, CuratedTREC, WebQuestions, WikiMovies, GeoQuery and ATIS.\nPapers With Code lists dozens of datasets along with their respective state-of-the-art models.\nBengio, Yoshua, R\u00e9jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. \"A Neural Probabilistic Language Model.\" Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155. Accessed 2020-02-23.\nBordes, Antoine, Jason Weston, and Nicolas Usunier. 2014. \"Open Question Answering with Weakly Supervised Embedding Models.\" arXiv, v1, April 16. Accessed 2020-02-21.\nChen, Danqi, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. \"Reading Wikipedia to Answer Open-Domain Questions.\" Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1870-1879, July. Accessed 2020-02-21.\nChoi, Eunsol, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. \"QuAC: Question Answering in Context.\" Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2174-2184, October-November. Accessed 2020-02-08.\nChoudhury, Ambika. 2019. \"10 Question-Answering Datasets To Build Robust Chatbot Systems.\" Analytics India Magazine, September 27. Accessed 2020-02-08.\nClark, Jonathan. 2020. \"TyDi QA: A Multilingual Question Answering Benchmark.\" Google AI Blog, February 6. Accessed 2020-02-21.\nCouto, Javier. 2018. \"Introduction to Visual Question Answering: Datasets, Approaches and Evaluation.\" Blog, TryoLabs, March 1. Accessed 2020-02-08.\nDeepMind. 2017. \"deepmind / AQuA.\" GitHub, November 2. Accessed 2020-02-21.\nDiefenbach, Dennis, Vanessa Lopez, Kamal Singh, and Pierre Maret. 2017. \"Core techniques of question answering systems over knowledge bases: a survey.\" Knowledge and Information Systems, vol. 55, no. 3, pp. 529-569. Accessed 2020-02-08.\nFan, Angela, Yacine Jernite, and Michael Auli. 2019. \"Introducing long-form question answering.\" Blog, Facebook AI, July 25. Accessed 2020-02-08.\nFerrucci, David, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welty. 2010. \"Building Watson: An Overview of the DeepQA Project.\" AI Magazine, Association for the Advancement of Artificial Intelligence, pp. 60-79. Accessed 2020-02-21.\nGreen, Bert F., Alice K. Wolf, Carol Chomsky, and Kenneth Laughery. 1961. \"Baseball: an automatic question-answerer.\" Western Joint IRE-AIEE-ACM Computer Conference, pp. 219-224, May. doi:10.1145/1460690.1460714. Accessed 2020-02-24.\nHeidenreich, Hunter. 2018. \"CoQA: A Conversational Question Answering Challenge.\" Blog, August 24. Accessed 2020-02-08.\nHeidenreich, Hunter. 2018b. \"QuAC: Question Answering in Context.\" Chatbots Life, on Medium, August 24. Accessed 2020-02-08.\nHudson, Drew A. and Christopher D. Manning. 2019. \"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering.\" arXiv, v3, May 10. Accessed 2020-02-21.\nH\u00f6ffner, Konrad, Sebastian Walter, Edgard Marx, Ricardo Usbeck, Jens Lehmann, and Axel-Cyrille Ngonga Ngomo. 2017. \"Survey on Challenges of Question Answering in the Semantic Web.\" Semantic Web, IOS Press, vol. 8, no. 6, pp. 895-920, August. Accessed 2020-02-08.\nInfoLab Group. 2019. \"START: Natural Language Question Answering System.\" InfoLab Group, CSAIL, MIT. Accessed 2020-02-08.\nIyyer, Mohit, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum\u00e9 III. 2014. \"A Neural Network for Factoid Question Answering over Paragraphs.\" Proceedings of EMNLP, pp. 633-644, October. Accessed 2020-02-08.\nJacob. 2018. \"Question Answering Datasets.\" StreamHacker, January 9. Accessed 2020-02-21.\nJurafsky, Daniel, and James H. Martin. 2009. \"Question Answering and Summarization.\" Chapter 23 in: Speech and Language Processing, Second Edition, Prentice-Hall, Inc. Accessed 2020-02-20.\nJurafsky, Daniel and James H. Martin. 2019. \"Speech and Language Processing.\" Third Edition draft, October 16. Accessed 2020-02-21.\nKlein, Dan. 2009. \"Lecture 25: Question Answering.\" Statistical NLP, UC Berkeley, Spring. Accessed 2020-02-08.\nKwiatkowski, Tom and Michael Collins. 2019. \"Natural Questions: a New Corpus and Challenge for Question Answering Research.\" Google AI Blog, January 23. Accessed 2020-02-08.\nKwiatkowski, Tom, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. \"Natural Questions: a Benchmark for Question Answering Research.\" Transactions of the Association of Computational Linguistics, vol. 7, pp. 453-466. Accessed 2020-02-09.\nLewis, Patrick, Ludovic Denoyer, and Sebastian Riedel. 2019. \"Unsupervised Question Answering by Cloze Translation.\" arXiv, v2, June 27. Accessed 2020-02-21.\nLi, Xin and Dan Roth. 2002. \"Learning Question Classifiers.\" COLING 2002: The 19th International Conference on Computational Linguistics, August 24 - September 1. Accessed 2020-02-21.\nMarkoff, John. 2011. \"Computer Wins on \u2018Jeopardy!\u2019: Trivial, It\u2019s Not.\" NY Times, February 16. Accessed 2020-02-21.\nOh, Hyo-Jung, Ki-Youn Sung, Myung-Gil Jang,<|endoftext|>"
    },
    {
        "entity": "Luke Zettlemoyer",
        "step": 29869,
        "passage": ".\nLee et al. (2017) Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. End-to-end neural coreference resolution. CoRR, abs/1707.07045, 2017. URL http://arxiv.org/abs/1707.07045.\nLin & Och (2004) Chin-Yew Lin and Franz Josef Och.\nOrange: A method for evaluating automatic evaluation metrics for machine translation.\nIn Proceedings of the 20th International Conference on Computational Linguistics, COLING \u201904, Stroudsburg, PA, USA, 2004. Association for Computational Linguistics. doi: 10.3115/1220355.1220427. URL https://doi.org/10.3115/1220355.1220427.\nLin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/1405.0312.\nLuong et al. (2015) Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neural machine translation. CoRR, abs/1508.04025, 2015. URL http://arxiv.org/abs/1508.04025.\nNiculae & Blondel (2017) Vlad Niculae and Mathieu Blondel. A regularized framework for sparse and structured neural attention. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 3338\u20133348. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/6926-a-regularized-framework-for-sparse-and-structured-neural-attention.pdf.\nPedersoli et al. (2016) Marco Pedersoli, Thomas Lucas, Cordelia Schmid, and Jakob Verbeek. Areas of attention for image captioning. CoRR, abs/1612.01033, 2016. URL http://arxiv.org/abs/1612.01033.\nSharma et al. (2018) Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pp. 2556\u20132565, 2018. URL https://aclanthology.info/papers/P18-1238/p18-1238.\nStern et al. (2017) Mitchell Stern, Jacob Andreas, and Dan Klein. A minimal span-based neural constituency parser. CoRR, abs/1705.03919, 2017. URL http://arxiv.org/abs/1705.03919.\nVaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv.org/abs/1706.03762.\nVedantam et al. (2014) Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. CoRR, abs/1411.5726, 2014. URL http://arxiv.org/abs/1411.5726.\nViola & Jones (2001) Paul Viola and Michael Jones. Rapid object detection using a boosted cascade of simple features. pp. 511\u2013518, 2001.\nWang & Chang (2016) Wenhui Wang and Baobao Chang. Graph-based dependency parsing with bidirectional LSTM. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers, 2016. URL http://aclweb.org/anthology/P/P16/P16-1218.pdf.\nWu et al. (2016) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016. URL http://arxiv.org/abs/1609.08144.\nXu et al. (2015) Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. CoRR, abs/1502.03044, 2015. URL http://arxiv.org/abs/1502.03044.\nYou et al. (2016) Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image captioning with semantic attention. CoRR, abs/1603.03925, 2016. URL http://arxiv.org/abs/1603.03925.\nYoung et al. (2014) P Young, A Lai, M Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. 2:67\u201378, 01 2014.<|endoftext|>"
    }
]