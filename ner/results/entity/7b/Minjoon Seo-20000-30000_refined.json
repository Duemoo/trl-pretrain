[
    {
        "entity": "Minjoon Seo",
        "step": 20882,
        "passage": " image in Fig. 10) and it shows that the \u2018P+SV\u2019 model outputs the same answer when the image contains similar visual features. Therefore, we believe that this VQA model might rely too heavily on the image feature and learned to map the image feature with the answer space but it does not truly understand the question. Additionally, for the question that requires stronger reasoning ability and image with many texts, such as the third sample in Fig. 10, \u2018\u4f1f\u4e1a\u6c34\u7535\u5b89\u88c5\u7684\u8054\u7cfb\u4eba\u662f\u8c01? (Who is the contact person for Weiye Hydropower Installation?)\u2019, none of the models predict the answer correctly.\nWe have described a new bilingual scene text+evidence VQA dataset named STE-VQA that is annotated with both English and Chinese QA pairs. Three related challenges are proposed, namely Cross Language, Localization and Traditional that are designed to evaluate the generalization of VQA models. An evidence-based measure of an algorithm\u2019s capacity to reason is also proposed that requires the VQA model to provide a bounding box of the predicted answer. This metric aims to uncover whether the VQA model learns deeper relationships between text and image content, rather than overfitting to a pre-defined dictionary. Future work includes extending the proposed EvE metric to existing VQA datasets in the hope that it might improve generalization and thus the practicality of VQA technologies.\n[1] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Don\u2019t just assume; look and answer: Overcoming priors for visual question answering. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 4971\u20134980, 2018.\n[2] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 3674\u20133683, 2018.\n[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proc. IEEE Int. Conf. Comp. Vis., pages 2425\u20132433, 2015.\n[4] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Mar\u00e7al Rusi\u00f1ol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. Proc. IEEE Int. Conf. Comp. Vis., 2019.\n[5] Chee-Kheng Ch\u2019ng, Chee Seng Chan, and Cheng-Lin Liu. Total-text: toward orientation robustness in scene text detection. Int. J. Doc. Anal. Recognit., pages 1\u201322, 2019.\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 248\u2013255. Ieee, 2009.\n[7] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 6904\u20136913, 2017.\n[8] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 3608\u20133617, 2018.\n[9] Brian Kenji Iwana, Syed Tahseen Raza Rizvi, Sheraz Ahmed, Andreas Dengel, and Seiichi Uchida. Judging a book by its cover. arXiv preprint arXiv:1610.09204, 2016.\n[10] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017.\n[11] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. European Chapter of the Association for Computational Linguistics, 2016.\n[12] Kushal Kafle and Christopher Kanan. Visual question answering: Datasets, algorithms, and future challenges. Comput. Vis. Image Underst., 163:3\u201320, 2017.\n[13] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust reading. In Proc. Int. Conf. Doc. Anal. and Recognit., pages 1156\u20131160. IEEE, 2015.\n[14] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. Icdar 2013 robust reading competition. In Proc. Int. Conf. Doc. Anal. and Recognit., pages 1484\u20131493. IEEE, 2013.\n[15] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 4999\u20135007, 2017.\n[16] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, Serge Belongie, Victor Gomes, Abhinav Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy. Openimages: A public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages, 2017.\n[17] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123(1):32\u201373, 2017.\n[18] Vladimir I Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady, volume 10, pages 707\u2013710, 1966.\n[19] Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, and Xiaoyong Du. Analogical reasoning on chinese morphological and semantic relations. Proc. Annu. Meet. Assoc. Comput. Linguist., 2018.\n[20] Yuliang Liu, Lianwen Jin, Shuaitao Zhang, Canjie Luo, and Sheng Zhang. Curved scene text detection via transverse and longitudinal sequence connection. Pattern Recogn., 90:337\u2013345, 2019.\n[21] Yuliang Liu, Sheng Zhang, Lianwen Jin, Lele Xie, Yaqiang Wu, and Zhepeng Wang. Omnidirectional scene text detection with sequential-free box discretization. Proc. Int. Joint Conf. Artificial Intell., 2019.\n[22] Mateusz Malinowski and Mario Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input. In Proc. Advances in Neural Inf. Process. Syst., pages 1682\u20131690, 2014.\n[23] Anand Mishra, Karteek Alahari, and CV Jawahar. Image retrieval using textual cues. In Proc. IEEE Int. Conf. Comp. Vis., pages 3040\u20133047, 2013.\n[24] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In Proc. Int. Conf. Doc. Anal. and Recognit.,<|endoftext|>"
    },
    {
        "entity": "Minjoon Seo",
        "step": 21284,
        "passage": "\"\"\"Top-level model classes.\n\nAuthor:\n    Chris Chute |||EMAIL_ADDRESS||| \"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport layers\n\n\nclass BiDAF(nn.Module):\n    \"\"\"Baseline BiDAF model for SQuAD.\n\n    Based on the paper:\n    \"Bidirectional Attention Flow for Machine Comprehension\"\n    by Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n    (https://arxiv.org/abs/1611.01603).\n\n    Follows a high-level structure commonly found in SQuAD models:\n        - Embedding layer: Embed word indices to get word vectors.\n        - Encoder layer: Encode the embedded sequence.\n        - Attention layer: Apply an attention mechanism to the encoded sequence.\n        - Model encoder layer: Encode the sequence again.\n        - Output layer: Simple layer (e.g., fc + softmax) to get final outputs.\n\n    Args:\n        word_vectors (torch.Tensor): Pre-trained word vectors.\n        hidden_size (int): Number of features in the hidden state at each layer.\n        drop_prob (float): Dropout probability.\n    \"\"\"\n\n    def __init__(self, word_vectors, hidden_size, drop_prob=0.):\n        super(BiDAF, self).__init__()\n        self.emb = layers.Embedding(word_vectors=word_vectors,\n                                    hidden_size=hidden_size,\n                                    drop_prob=drop_prob)\n\n        self.enc = layers.RNNEncoder(input_size=hidden_size,\n                                     hidden_size=hidden_size,\n                                     num_layers=1,\n                                     drop_prob=drop_prob)\n\n        self.att = layers.BiDAFAttention(hidden_size=2 * hidden_size,\n                                         drop_prob=drop_prob)\n\n        self.mod = layers.RNNEncoder(input_size=8 * hidden_size,\n                                     hidden_size=hidden_size,\n                                     num_layers=2,\n                                     drop_prob=drop_prob)\n\n        self.out = layers.BiDAFOutput(hidden_size=hidden_size,\n                                      drop_prob=drop_prob)\n\n    def forward(self, cw_idxs, qw_idxs):\n        c_mask = torch.zeros_like(cw_idxs)!= cw_idxs\n        q_mask = torch.zeros_like(qw_idxs)!= qw_idxs\n        c_len, q_len = c_mask.sum(-1), q_mask.sum(-1)\n\n        c_emb = self.emb(cw_idxs)  # (batch_size, c_len, hidden_size)\n        q_emb = self.emb(qw_idxs)  # (batch_size, q_len, hidden_size)\n\n        c_enc = self.enc(c_emb, c_len)  # (batch_size, c_len, 2 * hidden_size)\n        q_enc = self.enc(q_emb, q_len)  # (batch_size, q_len, 2 * hidden_size)\n\n        att = self.att(c_enc, q_enc,\n                       c_mask, q_mask)  # (batch_size, c_len, 8 * hidden_size)\n\n        mod = self.mod(att, c_len)  # (batch_size, c_len, 2 * hidden_size)\n\n        out = self.out(att, mod, c_mask)  # 2 tensors, each (batch_size, c_len)\n\n        return out\n\n\nclass BiDAFExtra(nn.Module):\n    \"\"\"Baseline BiDAF model for SQuAD.\n\n    Based on the paper:\n<|endoftext|>"
    },
    {
        "entity": "Minjoon Seo",
        "step": 21295,
        "passage": " negative spans can be better divided with the correct answer \u201cJerusalem\u201d. This shows that SCL in our KECP framework is reliable and can improve the performance for EQA.\nThe Accuracy of Answer Generation. A major difference between previous works and ours is that we model the EQA task as text generation. Intuitively, if the model correctly generates the first answer token, it is easy to generate the remaining answer tokens because of the very small search space. Therefore, we analyze how difficult it is for the model to generate the first token correctly. Specifically, we check whether the generated first token and the first token of the ground truth are within a fixed window size nw. As shown in Table 5, we find the accuracy of our method is lower than RoBERTa-base Liu et al. (2019) when nw=1. Yet, we achieve the best performance when increasing the window size nw to 5. We think that our KECP can generate some rehabilitation text for the answer. For example in Figure 4, the PLM may generate \u201cthe conquest of Jerusalem\u201d rather than the correct answer with single token \u201cJerusalem\u201d. This phenomenon reflects the reason why we achieve lower accuracy when nw=1. But, we think that the generated results are still in the vicinity of the correct answer.\nTable 5: The accuracy of predicting the first [MASK] in the query prompt with full training samples for each task. #nw denotes the window size.\nTo bridge the gap between the pre-training and fine-tuning objectives, KECP views EQA as an answer generation task. In KECP, the knowledge-aware prompt encoder injects external domain-related knowledge into the passage, and then enhances the representations of selected prompt tokens in the query. The span-level contrastive learning objective is proposed to improve the performance of EQA. Experiments on multiple benchmarks show that our framework outperforms the state-of-the-art methods. In the future, we will i) further improve the performance of KECP by applying controllable text generation techniques, and ii) explore the prompt-tuning for other types of MRC tasks, such as cloze-style MRC and multiple-choice MRC.\nBrown et al. (2020) Tom B. Brown, Benjamin Mann, and etc. Nick Ryder. 2020. Language models are few-shot learners. In NeurIPS.\nChen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A simple framework for contrastive learning of visual representations. In ICML, volume 119, pages 1597\u20131607.\nDai et al. (2021) Damai Dai, Hua Zheng, Zhifang Sui, and Baobao Chang. 2021. Incorporating connections beyond knowledge embeddings: A plug-and-play module to enhance commonsense reasoning in machine reading comprehension. CoRR, abs/2103.14443.\nDettmers et al. (2018) Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018.\nConvolutional 2d knowledge graph embeddings.\nDunn et al. (2017) Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur G\u00fcney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q&a dataset augmented with context from a search engine. CoRR, abs/1704.05179.\nFisch et al. (2019) Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In EMNLP, pages 1\u201313.\nGao et al. (2021) Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In ACL, pages 3816\u20133830.\nHan et al. (2021) Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2021. PTR: prompt tuning with rules for text classification. CoRR, abs/2105.11259.\nJoshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predicting spans. TACL, 64\u201377.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL, pages 1601\u20131611.\nKwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, and et al. 2019. Natural questions: a benchmark for question answering research. TACL.\nLai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard H. Hovy. 2017. RACE: large-scale reading comprehension dataset from examinations. In EMNLP, pages 785\u2013794.\nLevy et al. (2017) Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In CoNLL, pages 333\u2013342.\nLi and Liang (2021a) Xiang Lisa Li and Percy Liang. 2021a. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP, pages 4582\u20134597. Association for Computational Linguistics.\nLi and Liang (2021b) Xiang Lisa Li and Percy Liang. 2021b. Prefix-tuning: Optimizing continuous prompts for generation. In ACL, pages 4582\u20134597.\nLiu et al. (2021a) Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021a. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. CoRR.\nLiu et al. (2021b) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT understands, too. CoRR, abs/2103.10385.\nLiu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, and et al. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR.\nQin and Eisner (2021) Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. In NAACL-HLT, pages 5203\u20135212.\nRajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable questions for squad. CoRR, abs/1806.03822.\nRajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. CoRR.\nRam et al. (2021) Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, and Omer Levy. 2021. Few-shot question answering by pretraining span selection. In ACL.\nSchick and Sch\u00fctze (2021) Timo Schick and Hinrich Sch\u00fctze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In EACL, pages 255\u2013269.\nShin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP.\nTrischler et al. (2017) Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017. NewsQA: A machine comprehension dataset. In WRLNLP, pages 191\u2013200.\nVan der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne.\nVinyals et al. (2015) Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In NIPS, pages 2692\u20132700.\nWang and Jiang (2019) Chao Wang and Hui Jiang. 2019. Explicit utilization of general knowledge in machine reading comprehension. In ACL, pages 2263\u20132272. Association for Computational Linguistics.\nWang et al. (2022) Chengyu Wang, Minghui Qiu, Taolin Zhang, Tingting Liu, Lei Li, Jianing Wang, Ming Wang, Jun<|endoftext|>"
    },
    {
        "entity": "Minjoon Seo",
        "step": 21555,
        "passage": " Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends\u00ae in Information Retrieval 3, 4 (2009), 333\u2013389.\nSeo et al. (2016) Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603 (2016).\nSharp et al. (2017) Rebecca Sharp, Mihai Surdeanu, Peter Jansen, Marco A Valenzuela-Esc\u00e1rcega, Peter Clark, and Michael Hammond. 2017. Tell Me Why: Using Question Answering as Distant Supervision for Answer Justification. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017). 69\u201379.\nSurdeanu et al. (2011) Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to Rank Answers to Non-Factoid Questions from Web Collections. Computational Linguistics 37, 2 (2011), 351\u2013383.\nTymoshenko et al. (2017) Kateryna Tymoshenko, Daniele Bonadiman, and Alessandro Moschitti. 2017. Ranking Kernels for Structures and Embeddings: A Hybrid Preference and Classification Model. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP).\nWang et al. (2016) Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah. 2016. Sentence Similarity Learning by Lexical Decomposition and Composition. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee, 1340\u20131349.\nYang et al. (2015) Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset for Open-Domain Question Answering.. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. 2013\u20132018.\nYih et al. (2013) Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question Answering Using Enhanced Lexical Semantic Models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL).\nYin et al. (2016) Wenpeng Yin, Hinrich Sc\u00fctze, Bing Xiang, and Bowen Zhou. 2016. ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs. Transactions of the Association for Computational Linguistics 4 (2016), 259\u2013272.\nYoung et al. (2017) Tom Young, Devamanyu Hazarika, Soujanya Poria, and Erik Cambria. 2017. Recent trends in deep learning based natural language processing. arXiv preprint arXiv:1708.02709 (2017).\nZhou et al. (2015) Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu. 2015. Learning Continuous Word Embedding with Metadata for Question Retrieval in Community Question Answering. In Associations for Computational Linguistics, 2015.<|endoftext|>"
    },
    {
        "entity": "Minjoon Seo",
        "step": 21860,
        "passage": "After only two weeks of training, the 2018 Varsity Badminton team hosted an early-season invitational in the Palestra. Neuchatel Junior College and Inter Community School Zurich brought 28 players to compete against the Tigers. The Singles tournament results were solid for the Lady Tigers, who won the event the past two years. Captain MV Ramos '19 and teammate Margarita Ukleina '19 dominated their matches in the pool play. Later on, they would find themselves fighting it out in the championship match, with Uklieina edging out Ramos by two points. Newcomers Heather Hines '19 and Hajir Qureshi '20 also proved that they had the skills and determination. Each won important team points by beating opponent after opponent. In the Doubles tournament, Ukleina and Ramos came from behind to defeat rival ICSZ and claim the gold medal. At the end, with a combined team effort, the Lady Tigers remained the undefeated champions for the third straight year.\nIn the Boys tournament, returners Matt Meng '21, Minjoon Seo '19, and Giulio Bianco '21 showed off their smashes, fake shots, and consistency in each of their pool play games and finished at the top of their groups. Meng continued his winning streak all the way to the finals, where he lost by a narrow margin to his ICSZ opponent. Seo and Bianco were tested in the playoffs and endured some long rallies and games. All the players got some great opportunities and were able to push themselves on the court and support one another throughout the day. The boys ended the tournament with a solid 2nd-place finish.\nCongrats to all the players. The next competition will be in Zurich as ICSZ will host its invitational after the Winter Holiday.<|endoftext|>"
    },
    {
        "entity": "Minjoon Seo",
        "step": 22835,
        "passage": " Aware Neural Machine Translation\nKehai Chen, Rui Wang, Masao Utiyama and Eiichiro Sumita\n\nContextual Embeddings: When Are They Worth It?\nSimran Arora, Avner May, Jian Zhang and Christopher R\u00e9\n\nContextual Neural Machine Translation Improves Translation of Cataphoric Pronouns\nKayYen Wong, Sameen Maruf and Gholamreza Haffari\n\nContextualized Sparse Representations for Real-Time Open-Domain Question Answering\nJinhyuk Lee, Minjoon Seo, Hannaneh Hajishirzi and Jaewoo Kang\n\nContextualizing Hate Speech Classifiers with Post-hoc Explanation\nBrendan Kennedy, Xisen Jin, Aida Mostafazadeh Davani, Morteza Dehghani and Xiang Ren\n\nContrastive Self-Supervised Learning for Commonsense Reasoning\nTassilo Klein and Moin Nabi\n\nControlled Crowdsourcing for High-Quality QA-SRL Annotation\nPaul Roit, Ayal Klein, Daniela Stepanov, Jonathan Mamou, Julian Michael, Gabriel Stanovsky, Luke Zettlemoyer and Ido Dagan\n\nConversational Word Embedding for Retrieval-Based Dialog System\nWentao Ma, Yiming Cui, Ting Liu, Dong Wang, Shijin Wang and Guoping Hu\n\nCrawling and Preprocessing Mailing Lists At Scale for Dialog Analysis\nJanek Bevendorff, Khalid Al Khatib, Martin Potthast and Benno Stein\n\nCrossing Variational Autoencoders for Answer Retrieval\nWenhao Yu, Lingfei Wu, Qingkai Zeng, Shu Tao, Yu Deng and Meng Jiang\n\nDeeBERT: Dynamic Early Exiting for Accelerating BERT Inference\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu and Jimmy Lin\n\nDesigning Precise and Robust Dialogue Response Evaluators\nTianyu Zhao, Divesh Lala and Tatsuya Kawahara\n\nDialogue State Tracking with Explicit Slot Connection Modeling\nYawen Ouyang, Moxin Chen, Xinyu Dai, Yinggong Zhao, Shujian Huang and Jiajun Chen\n\nDo Transformers Need Deep Long-Range Memory?\nJack Rae and Ali Razavi\n\nDo you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods\nNing Miao, Yuxuan Song, Hao Zhou and Lei Li\n\nDoes Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation\nBei Li, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu and Changliang Li\n\nDon\u2019t Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction\nZhenkai Wei, Yu Hong, Bowei Zou, Meng Cheng and Jianmin Yao\n\nDscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing\nJiangming Liu, Shay B. Cohen and Mirella Lapata\n\nDynamic Memory Induction Networks for Few-Shot Text Classification\nRuiying Geng, Binhua Li, Yongbin Li, Jian Sun and Xiaodan Zhu\n\nDynamic Sampling Strategies for Multi-Task Reading Comprehension\nAnanth Gottumukkala, Dheeru Dua, Sameer Singh and Matt Gardner\n\nDynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change\nHongfei Xu, Josef van Genabith, Deyi Xiong and Qiuhui Liu\n\nEfficient strategies for hierarchical text classification: external knowledge and auxiliary tasks\nKervy Rivas Rojas, Gina Bustamante, Arturo Oncevay and Marco Antonio Sobrevilla Cabezudo\n\nEmbarrassingly Simple Unsupervised Aspect Extraction\nSt\u00e9phan Tulkens and Andreas van Cranenburgh\n\nEnabling Language Models to Fill in the Blanks\nChris Donahue, Mina Lee and Percy Liang\n\nEncoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction\nMasahiro Kaneko, Masato Mita, Shun Kiyono, Jun Suzuki and Kentaro Inui\n\nENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation\nLifu Tu, Richard Yuanzhe Pang, Sam Wiseman and Kevin Gimpel\n\nEnhancing Machine Translation with Dependency-Aware Self-Attention\nEmanuele Bugliarello and Naoaki Okazaki\n\nEnhancing Pre-trained Chinese Character Representation with Word-aligned Attention\nYanzeng Li, Bowen Yu, Xue Mengge and Tingwen Liu\n\nEnriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing\nDaniel Fern\u00e1ndez-Gonz\u00e1lez and Carlos G\u00f3mez-Rodr\u00edguez\n\nEntity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification\nNianzu Ma, Sahisnu Mazumder, Hao Wang and Bing Liu\n\nEstimating Mutual Information Between Dense Word Embeddings\nVitalii Zhelezniak, Aleksandar Savkov and Nils Hammerla\n\nEvaluating Dialogue Generation Systems via Response Selection\nShiki Sato, Reina Akama, Hiroki Ouchi, Jun Suzuki and Kentaro Inui\n\nEvaluating Robustness to Input Perturbations for Neural Machine Translation\nXing Niu, Prashant Mathur, Georgiana Dinu and Yaser Al-Onaizan\n\nEvery Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks\nYufeng Zhang, Xueli Yu, Zeyu Cui, Shu Wu, Zhongzhen Wen and Liang Wang\n\nExpBERT: Representation Engineering with Natural Language Explanations\nShikhar Murty, Pang Wei Koh and Percy Liang\n\nExploiting Personal Characteristics of Debaters for Predicting Persuasiveness\nKhalid Al Khatib, Michael V\u00f6lske, Shahbaz Syed, Nikolay Kolyada and Benno Stein\n\nExploring Content Selection in Summarization of Novel Chapters\nFaisal Ladhak, Bryan Li, Yaser Al-Onaizan and Kathy McKeown\n\nFact-based Content Weighting for Evaluating Abstractive Summarisation\nXinnuo Xu, Ond\u0159ej Du\u0161ek, Jingyi Li, Verena Rieser and Ioannis Konstas\n\nFatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts\nAgostina Calabrese, Michele Bevilacqua and Roberto Navigli\n\nFew-Shot NLG with Pre-Trained Language Model\nZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu and William Yang Wang\n\nFLAT: Chinese NER Using Flat-Lattice Transformer\nXiaonan Li, Hang Yan, Xipeng Qiu and Xuanjing Huang\n\nGAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples\nDanilo Croce, Giuseppe Castellucci and Roberto Basili\n\nGeometry-aware domain adaptation for unsupervised alignment of word embeddings\nPratik Jawanpuria, Mayank Meghwanshi and Bamdev Mishra\n\nGive Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?\nKobi Leins, Jey Han Lau and Timothy Baldwin\n\nGlyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs\nHong-You Chen, SZ-HAN YU and Shou-de Lin\n\nGPT-too: A language-model-first approach for AMR-to-text generation\nManuel Mager, Ram\u00f3n Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian and Salim Roukos\n\nHow Can We Accelerate Progress Towards Human-like Linguistic Generalization?\nTal Linzen\n\nHypernymy Detection for Low-Resource Languages via Meta Learning\nChanglong Yu, Jialong Han, Haisong Zhang and Wilfred Ng\n\nIdentifying Principals and Accessories in a Complex Case based on the Comprehension of Fact Description\nYakun Hu, Zhunchen Luo and Wenhan Chao\n\nImplicit Discourse Relation Classification: We Need to Talk about Evaluation\nNajoung Kim, Song Feng, Chulaka Gunasekara and Luis Lastras\n\nImproved Speech Representations with Multi-Target Autoregressive Predictive Coding\nYu-An Chung and James Glass\n\nImproving Entity Linking through Semantic Reinforced Entity Embeddings\nFeng Hou, Ruili Wang, Jun He and Yi Zhou\n\nImproving<|endoftext|>"
    },
    {
        "entity": "Minjoon Seo",
        "step": 23490,
        "passage": "Samsung Electronics has uncovered a new plan for shaping the future with AI and semiconductors by unveiling new milestones in AI-based semiconductor and material innovation. The tech goliath disclosed this today at the ongoing Samsung AI Forum 2022.\nSamsung AI Forum is an annual AI event hosted by Samsung Electronics where world-renowned academics, researchers, and industry experts will come together to share their insights on the future of artificial intelligence, AI.\nCurrently, the event is hosted at the Samsung Advanced Institute of Technology with over 1,200 attendees. This year, Samsung AI Forum begins today, November 8th, and will conclude tomorrow, November 9th. This is the first time in three years that the event will be held in person due to the emergence of the novel coronavirus.\nUnder the theme of \u201cShaping the Future with AI and Semiconductor,\u201d Samsung AI experts discuss the future direction of AI research that will create new milestones in AI-based semiconductor and material innovation.\nProfessor Yoshua Bengio of the University of Montreal, Canada, shared his latest research in a keynote presentation, \u201cWhy We Need Amortized, Causal and Bayesian World Models.\u201d In causal models for AI that investigate theories and plan experiments in the realm of science and general AI, he emphasized the use of amortized inference and the Bayesian approach.\nMeanwhile, in his opening remarks, Jong-Hee (JH) Han, Vice Chairman, CEO, and Head of the Device Experience (DX) Division at Samsung Electronics, said AI technology would provide better convenience and new experiences for all.\nIn the \u201cAI for R&D Innovation\u201d session, research leaders at SAIT, including the Executive Vice President and Head of SAIT\u2019s AI Research Center, Changkyu Choi, shared the status and vision of Samsung\u2019s research on AI. In particular, they talked about how AI technology will affect industries like semiconductors and material development.\nMinjoon Seo, a professor at KAIST, and Hyunoh Song, a professor at Seoul National University, gave a presentation on the most recent research accomplishments in AI algorithms in a session titled \u201cRecent Advances of AI Algorithms.\u201d This included a large language model-based interface for ultra-accurate semantic search.\nHowever, according to the firm, day two is themed \u201cScaling AI for the Real World.\u201d There Samsung is expected to share the development direction of future AI technologies that will significantly affect our lives, including hyperscale AI, digital humanities, and robotics.\nSebastian Seung, President and Head of Samsung Research will give a keynote on the \u201cfirst steps of an effort to improve upon classical brain theories by optimizing biologically plausible unsupervised learning algorithms\u201d and a welcoming remark.\nProfessor Terrence Sejnowski of the University of California San Diego in the United States, who founded NeurIPS, the most prestigious AI conference in the world, will speak about the intelligence of hyper-scale language models based on experimental studies examining the existence of intelligence in hyper-scale language models.\nIn all, professor Seungwon Hwang of Seoul National University will talk about how causality, evidentiality, and other types of information can be used to strengthen hyper-scale language models. Participants will also have the chance to view several demos and research posters created by the Global AI Center at the booth, where they may speak with the researchers in person.<|endoftext|>"
    },
    {
        "entity": "Minjoon Seo",
        "step": 25213,
        "passage": " to favor the true solution. The model sometimes gives the wrong prediction\u2014for example, at t=16k, and changes its prediction from the true solution to the wrong solution, \u201837-36\u2019\u2014but again changes its prediction to be a true solution afterward. In addition, its intermediate wrong solution, \u201837-36\u2019 indicates the model was confused with distinguishing the longest field goal of Rob Bironas (40 vs. 37), which is an understandable mistake.\nWe also compare the predictions from the model with our method to those from the model with MML, which is shown in Appendix C.\nQuality of the predicted solution.\nWe analyze if the model outputs the correct solution, since the solution executing the correct answer could be spurious. First, on NarrativeQA and DROPnum, we manually analyze 50 samples from the development set and find that 98% and 92% of correct cases produce the correct solution respectively. Next, on WikiSQL, we compare the predictions from the model to the annotated SQL queries on the development set. This is possible because gold SQL queries are available in the dataset for the full supervision. Out of 8,421 examples, 7,110 predictions execute the correct answers. Among those, 88.5% of the predictions are exactly same as the annotated queries. Others are the cases where (i) both queries are correct, (ii) the model prediction is correct but the annotated query is incorrect, and (iii) the annotated query is correct and the model prediction is spurious. We show a full analysis in Appendix C.\nRobustness to the noise in |Z|.\nSometimes noise arises during the construction of |Z|, such as |Z| constructed based on ROUGE-L for NarrativeQA. To explore the effect of noise in Z, we experiment with more noisy solution set by picking all the spans with scores that is equal to or larger than the 5th highest. The new construction method increases |Z| from 4.3 to 7.1 on NarrativeQA. The result by MML objective drops significantly (56.07\u219251.14) while the result by ours drops marginally (58.77\u219257.97), suggesting that MML suffers more with a noisier Z while ours is more robust.\nIn this paper, we demonstrated that, for many QA tasks which only provide the answer text as supervision, it is possible to precompute a discrete set of possible solutions that contains one correct option. Then, we introduced a discrete latent variable learning algorithm which iterates a procedure of predicting the most likely solution in the precomputed set and further increasing the likelihood of that solution. We showed that this approach significantly outperforms previous approaches on six QA tasks including reading comprehension, open-domain QA, discrete reasoning task and semantic parsing, achieving absolute gains of 2\u201310% and setting the new state-of-the-art on five well-studied datasets.\nThis research was supported by ONR (N00014-18-1-2826, N00014-17-S-B001), DARPA N66001-19-2-403, NSF (IIS-1616112, IIS-1252835, IIS-1562364), ARO (W911NF-16-1-0121), an Allen Distinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google and Amazon.\nThe authors would like to thank the anonymous reviewers, Eunsol Choi, Christopher Clark, Victor Zhong and UW NLP members for their valuable feedback.\nAbolafia et al. (2018) Daniel A Abolafia, Mohammad Norouzi, Jonathan Shen, Rui Zhao, and Quoc V Le. 2018. Neural program synthesis with priority queue training. arXiv preprint arXiv:1801.03526.\nAgarwal et al. (2019) Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. 2019. Learning to generalize from sparse and underspecified rewards. In ICML.\nAlberti et al. (2019) Chris Alberti, Kenton Lee, and Michael Collins. 2019. A BERT baseline for the Natural Questions. arXiv preprint arXiv:1901.08634.\nArtzi and Zettlemoyer (2013) Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. In ACL.\nBerant et al. (2013) Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In EMNLP.\nChen et al. (2017) Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open-domain questions. In ACL.\nClarke et al. (2010) James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world\u2019s response. In CoNLL.\nDong and Lapata (2018) Li Dong and Mirella Lapata. 2018. Coarse-to-fine decoding for neural semantic parsing. In ACL.\nDua et al. (2019) Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL.\nHe et al. (2019) Pengcheng He, Yi Mao, Kaushik Chakrabarti, and Weizhu Chen. 2019. X-SQL: reinforce schema representation with context. arXiv preprint arXiv:1908.08113.\nHurley and Rickard (2009) Niall Hurley and Scott Rickard. 2009. Comparing measures of sparsity. IEEE Transactions on Information Theory.\nHwang et al. (2019) Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and Minjoon Seo. 2019. A comprehensive exploration on WikiSQL with table-aware word contextualization. arXiv preprint arXiv:1902.01069.\nIyyer et al. (2017) Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In ACL.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In ACL.\nKadlec et al. (2016) Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. 2016. Text understanding with the attention sum reader network. In ACL.\nKo\u010disk\u1ef3 et al. (2018) Tom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. TACL.\nKrishnamurthy et al. (2017) Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. 2017. Neural semantic parsing with type constraints for semi-structured tables. In EMNLP.\nKwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Change, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. TACL.\nLee et al. (2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In ACL.\nLiang et al. (2017) Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. 2017.\nLiang et al. (2018) Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V Le, and Ni Lao. 2018. Memory augmented policy optimization for program synthesis and semantic parsing. In NIPS.\nLin (2004) Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. Text Summarization Branches Out.\nMin et al. (2019) Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. Compositional questions do not necessitate multi-hop reasoning. In ACL.\nNishida et al. (2019) Kyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda, Atsushi Otsuka, Hisako Asano, and Junji Tomita. 2019. Multi-style generative reading comprehension. In ACL.\nPaszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017.\nAutomatic differentiation in PyTorch.\nRajpur<|endoftext|>"
    },
    {
        "entity": "Minjoon Seo",
        "step": 25456,
        "passage": "At the annual Academic Awards Assembly on the morning of May 28, High School Academic Dean Dr. Mark Abisi announced that three seniors\u2014Virginia Italia, Katherine Kelly, and Irene Lee\u2014and four juniors\u2014Alessandra Appiani, Airi Barnes, Maria Mastronardi, and Giulia Meregalli\u2014have been inducted to the TASIS Cum Laude Society chapter. Congratulations to the seven outstanding scholars for this prestigious honor.\nFounded in 1906, the Cum Laude Society aims to promote learning and sound scholarship in secondary schools while recognizing exceptional scholastic achievement. Unlike the National Honor Society, which primarily works with public high schools in North America and strongly considers a student\u2019s leadership, service, and character, the Cum Laude Society places its greatest emphasis on a student\u2019s scholarly contributions to his or her school.\nThe new inductees will join the 10 students honored in May 2017 when TASIS became the first school in Switzerland to establish a Cum Laude Society chapter\u2014Charlotte Colombo \u201918, Hannah Gage \u201917, Kirill Krupenin \u201917, Aida Loggiodice \u201917, Niccolo McConnell \u201917, Adam Novak \u201917, Bryan Soh \u201918, Laura Vecoli \u201917, Maria Vittoria Gallina Bognetti \u201917, and Shu Ye \u201918\u2014and the eight scholars inducted last May: Raffaella Alencar Barros \u201919, Aurelia Dochnal \u201919, Anastasia Kolesnikova \u201918, Noah Plues \u201918, Zeydan Rahman \u201919, Alexander Secilmis \u201919, Minjoon Seo \u201919, and Mariko Yamada \u201919.\nExcellence, Justice, Honor. Excellence includes the concept of excellence in the moral sense and is not limited to the ideal of superiority and scholarship, nor does it involve the endeavor of competing primarily for academic goals. Justice includes the concept of what is suitable and appropriate, as well as just. Honor includes the concept of dignity and truth, as well as honor.<|endoftext|>"
    },
    {
        "entity": "Minjoon Seo",
        "step": 25845,
        "passage": "\u805a\u5408\u51fd\u6570\uff1b\uff083\uff09Model_opval\uff1a\u9884\u6d4b\u8fd0\u7b97\u7b26\u3001\u6761\u4ef6\u7684\u53d6\u503c\u3002\n\n![image.png](img/1584454149724-bb5805e4-ccc8-4839-ae3b-88b57bc114f9.png)\n\n  i. \u4e09\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u52a0\u6743\u95ee\u9898\u548c\u7c7b\u578b\u8868\u793a\u5f62\u5f0fHQT / COL\uff08SQLNet\u63d0\u51fa\uff09\u3002\n\n![image.png](img/1584455373728-c1827b63-14e7-4077-9204-ce4b3ae1cef8.png)\n\n  i.  MODEL_COL\u6a21\u5757\n\n\u6a21\u5757\u9700\u8981\u9884\u6d4b\u4e09\u90e8\u5206\uff1a`SELECT_COL`,\u200b `COND_num`\u548c`COND_COL` \u8fd9\u4e09\u4e2a\u90e8\u5206\u6709\u4e2a\u7279\u70b9\u90fd\u662f\u8981\u7ed3\u5408question\u548c\u8868(\u6240\u6709\u5217\u540d)\u7684\u7279\u5f81\u3002\n\n\u9884\u6d4b\u200b `SELECT_COL`: \n\n\u200b                   ![image.png](img/1584456260577-501a28ab-4990-4209-860a-5da2039b6f18.png)\n\n\u9884\u6d4b `COND_num`\uff1a\n\n![image.png](img/1584456306256-937e9b2a-2df5-4eae-8f16-615e09cb72bb.png)\n\n\u9884\u6d4b `COND_COL`\uff08\u8fd9\u91cc\u5728SQLNet\u4e2d\u53d1\u73b0\uff0cSELECT_COL\u548cCOND_COL\u5e38\u5e38\u4f1a\u9884\u6d4b\u76f8\u540c\u7684\u5217\u540d\u3002\u6240\u4ee5\u8fd9\u91cc\u4f5c\u51fa\u4e86\u6539\u53d8\uff0c\u5728\u91cc\u9762\u516c\u5f0f\u4e2d\u65b0\u589e\u4e86\u4e00\u4e2a\u5173\u4e8eSELECT_COL\u5217\u540d\u7684\u9879.\uff09\uff1a\n\n![image.png](img/1584456364854-ed3d9289-edd0-448a-a19f-32c7a5b91de5.png)\n\n  i.  MODEL_AGG\u6a21\u5757\n\n\u8fd9\u90e8\u5206\u4e0eSQLNet\u4e00\u6837\uff0c\u90fd\u662f\u9884\u6d4b{NULL,MAX,MIN,COUNT,SUM,AVG}\u4e2d\u7684\u4e00\u79cd\u3002\n\n![image.png](img/1584456447315-7c530eb3-78cf-4c90-92c8-7e798ea0bd28.png)\n\n  i.  MODEL_OPVAL\u6a21\u5757\n\n\u9700\u8981\u9884\u6d4b\u4e24\u90e8\u5206\uff1a$OP\uff1b$COND_VAL\u3002\n\nOP\uff1a\u9884\u6d4b{=,>,<}\u4e2d\u7684\u4e00\u79cd\u3002\n\n![image.png](img/1584456573465-67624b50-08d5-438a-bdc9-f639b55a2b06.png)\n\nCOND_VAL\uff08\u501f\u9274\u4e86Pointernetwork\u7684\u601d\u60f3\u6765\u4ece\u8f93\u5165\u7684Question\u4e2d\u8003\u8651Value\u91cc\u7684\u503c\u3002\uff09\uff1a\n\n![image.png](img/1584456633425-dda329bb-494c-46bc-a5a5-5036eb880112.png)\n\n\u5176\u4e2dh\u662f\u524d\u4e00\u4e2a\u751f\u6210\u7684\u5355\u8bcd\u7684\u9690\u72b6\u6001\u3002\n\n## 5. \u6570\u636e\u96c6\nwikiSQL\n## 6. \u5b9e\u9a8c\u7ed3\u679c\nTypeSQL\u8fbe\u5230\u4e8682.6%\u7684\u6b63\u786e\u7387\n\n## 7. \u672a\u6765\u5de5\u4f5c\n\u5c06\u6765\uff0c\u6211\u4eec\u8ba1\u5212\u901a\u8fc7\u5728\u6570\u636e\u5e93\u62c6\u5206\u8bbe\u7f6e\u4e0b\u63a2\u7d22\u5176\u4ed6\u66f4\u590d\u6742\u7684\u6570\u636e\u96c6\u6765\u63a8\u8fdb\u8fd9\u9879\u5de5\u4f5c\u3002 \u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u66f4\u73b0\u5b9e\u7684\u6587\u672c\u5230SQL\u4efb\u52a1\uff08\u5305\u62ec\u8bb8\u591a\u590d\u6742\u7684SQL\u548c\u4e0d\u540c\u7684\u6570\u636e\u5e93\uff09\u4e0a\u7814\u7a76\u901a\u7528\u6a21\u578b\u7684\u6027\u80fd\u3002\n\n## 8. \u539f\u6587\u53ca\u6e90\u7801\n[\u539f\u6587](https://arxiv.org/abs/1804.09769)\n\n[\u6e90\u7801](https://github.com/taoyds/typesql)\n\n\n\n# A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization\n\n## 1. \u4f5c\u8005\u7b80\u4ecb\n> Wonseok Hwang Jinyeong Yim Seunghyun Park Minjoon Seo\n> Clova AI, NAVER Corp.\n\n## 2. \u6587\u732e\u7c7b\u578b\nKR2ML Workshop at NeurIPS 2019 \n## 3. \u4e3b\u8981\u8d21\u732e\n\n\n\n## 4. \u65b9\u6cd5\u6982\u8ff0\n\n\n## 5. \u5b9e\u9a8c\n\n\n## 6. \u539f\u6587\u53ca\u6e90\u7801\n[\u539f\u6587](https://arxiv.org/abs/1902.01069)\n\n[\u6e90\u7801](https://github.com/naver/sqlova)\n\n\n\n\n\n# \u4e2d\u6587NL2SQL\u6311\u6218\u8d5b\n\n- \u9996\u5c4a\u4e2d\u6587NL2SQL\u6311\u6218\u8d5b\u7b2c3\u540d\u65b9\u6848+\u4ee3\u7801 by beader GitHub\uff1a\n\n  https://github.com/beader/tianchi_nl2sql\n\n- \u5929\u6c60NL2SQL\u6311\u6218\u8d5b\u51a0\u519b\u65b9\u6848 by NUDT NLP GitHub\uff1a\n\n  https://github.com/nudtnlp/tianchi-nl2sql-top1\n\n\n\n# \u3010\u667a\u80fd\u95ee\u7b54\u3011\u4ece\u5165\u95e8\u5230\u653e\u5f03\u2014\u2014DBQA\u4e0e\u673a\u5668\u9605\u8bfb\u7406\u89e3\n\n## DBQA\u4e0eMRC\u5165\u95e8\n\n- - Abstract\n\n  - Task of MRC\n\n  - History of MRC\n\n  - Dataset of MRC\n\n  - - SQuAD\n    - DuReader\n    - DuReader_robust\n    - CMRC 2018\n    - DRCD\n\n  - Models of MRC\n\n  - - Stanford Attentive Reader\n    - BiDAF\n    - Others\n\n  - Metric of MRC\n\n  - Product in Clould\n\n  - Conclusion\n\n  - Reference\n\n> DBQA\u662fdocument-based Question and Answer \u7684\u7b80\u79f0\uff0cMRC\u662fMachine Reading Comprehension\u673a\u5668\u9605\u8bfb\u7406\u89e3\u7684\u7b80\u79f0\u3002\n> \u6309\u7167\u4e2a\u4eba\u6d45\u8584\u7684\u7406\u89e3\uff0c**DBQA\u66f4\u52a0\u504f\u5411\u4e8e\u7cfb\u7edf**\uff0c\u4e1a\u52a1\u573a\u666f\u4f1a\u6bd4\u76ee\u524d\u7684\u9605\u8bfb\u7406\u89e3\u66f4\u590d\u6742\uff0c**MRC\u5219\u6bd4\u8f83\u504f\u4e8e\u4efb\u52a1\u672c\u8eab**\uff0c\u7528\u4e8e\u6307\u5b9a\u5185\u5bb9\uff08\u6587\u6863\uff09\u4e2d\u627e\u7b54\u6848\uff08\u53ef\u4ee5\u62d2\u8bc6\uff09\u3002\u5982\u679c\u7b80\u5355\u7684\u4ece\u6587\u6863\u4e2d\u62bd\u53d6\u51fa\u90e8\u5206\u5185\u5bb9\u4f5c\u4e3a\u56de\u7b54\u8fd9\u4e2a\u4efb\u52a1\u89d2\u5ea6\u4e0a\uff0c\u53ef\u4ee5\u7b80\u5355\u7684\u628a\u4e24\u8005\u7b49\u4ef7\u5316\uff0c\u6216\u8005\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u9605\u8bfb\u7406\u89e3\u7684\u601d\u8def\u53bb\u5b9e\u73b0DBQA\u3002\u56e0\u6b64\uff0c\u867d\u7136\u672c\u6587\u4e3b\u8981\u6574\u7406\u7684\u662fMRC\u4e5f\u5f52\u7c7b\u5728\u95ee\u7b54\u4e2d\u4e86\uff08\u5927\u90e8\u5206\u53c2\u8003\u65af\u5766\u798f\u8bfe\u7a0bcs224n\uff09\n> PS\uff1a\u4fa7\u91cd\u4e8e\u4e2d\u6587MRC\uff0c\u6d89\u53ca\u8bba\u6587\u8f83\u5c11\uff08\u5f85\u540e\u7eed\u8865\u5145\uff09\n\n### Abstract\n\n\u673a\u5668\u9605\u8bfb\u7406\u89e3\u57fa\u7840\u4efb\u52a1\u662f\u6839\u636e\u95ee\u9898\uff08`Question`\uff09\uff0c\u5728\u975e\u7ed3\u6784\u5316\u6587\u6863\uff08`Passege`\uff09\u5bfb\u627e\u5408\u9002\u7684\u7b54\u6848\uff08`Answer`\uff09.\n\n`MCTestReading`\n\n![img](img/1.webp)\n\n\u901a\u5e38\u6765\u8bb2\uff0c\u57fa\u4e8e\u6587\u6863\u7684\u641c\u7d22\uff08\u53ef\u4ee5\u662fQA\uff09\u53ef\u4ee5\u5206\u4e3a2\u4e2a\u90e8\u5206\u3002\n\n1. \u4eceQuery\u4e2d\u8bc6\u522b\u7279\u5f81\uff0c\u53ec\u56de\u76f8\u5173\u6587\u6863\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4e00\u822c\u6210\u4e3a\uff08information retrieval\uff09\uff0c\u53ef\u4ee5\u901a\u8fc7\u4f20\u7edf\u7684\u4fe1\u606f\u68c0\u7d22/web\u641c\u7d22\u5904\u7406\uff08tf-idf\uff0cBM25\uff0cetc...\uff09\n2. \u4ece\u6587\u6863\u4e2d\u68c0\u7d22\u51fa\u6211\u4eec\u60f3\u8981\u7684\u7b54\u6848\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5373MRC\u3002\n\n### Task of MRC\n\n\u603b\u7684\u6765\u770bMRC\u4efb\u52a1\u5927\u81f4\u53ef\u4ee5\u5206\u4e3a\u56db\u79cd\u7c7b\u578b\n\n- **\u5b8c\u5f62\u586b\u7a7a**\n  \u8be5\u4efb\u52a1\u76f8\u5bf9\u6bd4\u8f83\u65e9\uff0c\u6bd4\u8f83\u6709\u4ee3\u8868\u6027\u7684\u6709The Children\u2019s Book Test\uff0cCMRC2017\u7b49\u3002\n- **\u591a\u9879\u9009\u62e9/\u591a\u9879\u9009\u62e9\u5f0f**\n  \u6570\u636e\u96c6\u4e3a\uff08\u6587\u6863\uff0c\u95ee\u9898\uff0c\u5019\u9009\u7b54\u6848\u96c6\uff0c\ufffd<|endoftext|>"
    },
    {
        "entity": "Minjoon Seo",
        "step": 26983,
        "passage": " two for backward) of Skim-LSTM+Attention model. We see that the second layer skims more, implying that the second layer is more confident about which tokens are important.\nFigure 6 shows F1 score of LSTM+Attention model using standard LSTM and Skim LSTM, sorted in ascending order by Flop-R. While models tend to perform better with larger computational cost, Skim LSTM (Red) outperforms standard LSTM (Blue) with comparable computational cost. We also observe that the F1 score of Skim-LSTM is more stable across different configurations and computational cost. Moreover, increasing the value of \u03b3 for Skim-LSTM gradually increases skipping rate and Flop-R, while it also leads to reduced accuracy.\nControlling skim rate. An important advantage of Skim-RNN is that the skim rate (and thus computational cost) can be dynamically controlled at inference time by adjusting the threshold for \u2018skim\u2019 decision probability p1t (Equation 1). Figure 6 shows the trade-off between the accuracy and computational cost for two settings, confirming the importance of skimming (d\u2032>0) compared to skipping (d\u2032=0).\nVisualization. Figure 7 shows an example from SQuAD and visualizes which words Skim-LSTM (d=100,d\u2032=20) reads (red) and skims (white). As expected, the model does not skim when the input seems to be relevant to answering the question. In addition, LSTM in second layer skims more than that in the first layer mainly because the second layer is more confident about the importance of each token, as shown in Figure 7. More visualizations are shown in in Appendix C.\nd=100 (batch size = 1) in all three frameworks on a single thread of CPU (averaged over 100 trials), and have observed that NumPy is 1.5 and 2.8 times faster than TensorFlow and PyTorch.888NumPy\u2019s speed becomes similar to that of TensorFlow and PyTorch at d=220 and d=700, respectively. At larger hidden size, NumPy becomes slower. This seems to be mostly due to the fact that the frameworks are primarily (optimized) for GPUs and they have larger overhead than NumPy that they cannot take much advantage of reducing the size of the hidden state of the LSTM below 100.\nFigure 8: Speed up rate of Skim-LSTM (vs LSTM) with varying skimming rates and hidden state sizes.\nFigure 8 shows the relative speed gain of Skim-LSTM compared to standard LSTM with varying hidden state size and skim rate. We use NumPy, and the inferences are run on a single thread of CPU. We also plot the ratio between the reduction of the number of float operations (Flop-R) of LSTM and Skim-LSTM. This can be considered as a theoretical upper bound of the speed gain on CPUs. We note two important observations. First, there is an inevitable gap between the actual gain (solid line) and the theoretical gain (dotted line). This gap will be larger with more overhead of the framework, or more parallelization (e.g. multithreading). Second, the gap decreases as the hidden state size increases because the the overhead becomes negligible with very large matrix operations. Hence, the benefit of Skim-RNN will be greater for larger hidden state size.\nLatency. A modern GPU has much higher throughput than a CPU with parallel processing. However, for small networks, the CPU often has lower latency than the GPU. Comparing between NumPy with CPU and TensorFlow with GPU (Titan X), we observe that the former has 1.5 times lower latency (75 \\upmus vs 110 \\upmus per token) for LSTM of d=100. This means that combining Skim-RNN with CPU-based framework can lead to substantially lower latency than GPUs. For instance, Skim-RNN with CPU on IMDb has 4.5x lower latency than a GPU, requiring only 29 \\upmus per token on average.\nWe present Skim-RNN, a recurrent neural network that can dynamically decide to use the big RNN (read) or the small RNN (skim) at each time step, depending on the importance of the input. While Skim-RNN has significantly lower computational cost than its RNN counterpart, the accuracy of Skim-RNN is still on par with or better than standard RNNs, LSTM-Jump, and VCRNN. Since Skim-RNN has the same input and output interface as an RNN, it can easily replace RNNs in existing applications. We also show that a Skim-RNN can offer better latency results on a CPU compared to a standard RNN on a GPU. Future work involves using Skim-RNN for applications that require much higher hidden state size, such as video understanding, and using multiple small RNN cells for varying degrees of skimming.\nThis research was supported by the NSF (IIS 1616112), Allen Distinguished Investigator Award, Samsung GRO award, and gifts from Google, Amazon, Allen Institute for AI, and Bloomberg. We thank the anonymous reviewers for their helpful comments.\nBalduzzi & Ghifary (2016) David Balduzzi and Muhammad Ghifary. Strongly-typed recurrent neural networks. In ICML, 2016.\nCampos et al. (2017) V\u00edctor Campos, Brendan Jou, Xavier Gir\u00f3-i Nieto, Jordi Torres, and Shih-Fu Chang. Skip rnn: Learning to skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834, 2017.\nChoi et al. (2017) Eunsol Choi, Daniel Hewlett, Alexandre Lacoste, Illia Polosukhin, Jakob Uszkoreit, and Jonathan Berant. Coarse-to-fine question answering for long documents. In ACL, 2017.\nChung et al. (2017) Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In ICLR, 2017.\nDyer et al. (2016) Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network grammars. In NAACL, 2016.\nHahn & Keller (2016) Michael Hahn and Frank Keller. Modeling human reading with neural attention. In EMNLP, 2016.\nJang et al. (2017) Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017.\nJernite et al. (2017) Yacine Jernite, Edouard Grave, Armand Joulin, and Tomas Mikolov. Variable computation in recurrent neural networks. In ICLR, 2017.\nJohansen et al. (2017) Alexander Johansen, Bryan McCann, James Bradbury, and Richard Socher. Learning when to read and when to skim, 2017. URL https://metamind.io/research/learning-when-to-skim-and-when-to-read.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL, 2017.\nKembhavi et al. (2017) Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In CVPR, 2017.\nKokkinos & Potamianos (2017) Filippos Kokkinos and Alexandros Potamianos. Structural attention neural networks for improved sentiment analysis. arXiv preprint arXiv:1701.01811, 2017.\nKong et al. (2016) Lingpeng Kong, Chris Dyer, and Noah A Smith. Segmental recurrent neural networks. In ICLR, 2016.\nMarcel Adam Just (1987) Patricia Anderson Carpenter Marcel Adam Just. The Psychology of Reading and Language Comprehension. 1987.\nMikolov et al. (2015) Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc\u2019Aurelio Ranzato. Learning longer memory in recurrent neural networks. In ICLR Workshop, 2015.\nMin et al. (2017) Sewon Min, Minjoon Seo, and Hannaneh Hajishirzi.\nQuestion answering through transfer learning from large fine-grained supervision data.\nMiyato et al. (2017) Takeru Miyato, Andrew M. Dai, and Ian Goodfellow. Adversarial training methods for semi-supervised text classification. In ICLR, 2017.\nMnih et al. (2014) Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In NIPS, 2014.\nOdena et al. (2017) Augustus Odena, Dieterich Lawson, and Christopher Olah.\nChanging model behavior at test-time using reinforcement learning.\nIn ICLR Workshop, 2017.\nRastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon<|endoftext|>"
    },
    {
        "entity": "Minjoon Seo",
        "step": 27048,
        "passage": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, Kevin Clark, et al., ICLR, 2020.\nTinyBERT: Distilling BERT for Natural Language Understanding, Xiaoqi Jiao, et al., ICLR, 2020.\nMINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers, Wenhui Wang, et al., arXiv, 2020.\nT5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, Colin Raffel, et al., arXiv preprint, 2019.\nERNIE: Enhanced Language Representation with Informative Entities, Zhengyan Zhang, et al., ACL, 2019.\nXLNet: Generalized Autoregressive Pretraining for Language Understanding, Zhilin Yang, et al., arXiv preprint, 2019.\nALBERT: A Lite BERT for Self-supervised Learning of Language Representations, Zhenzhong Lan, et al., arXiv preprint, 2019.\nRoBERTa: A Robustly Optimized BERT Pretraining Approach, Yinhan Liu, et al., arXiv preprint, 2019.\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, Victor sanh, et al., arXiv, 2019.\nSpanBERT: Improving Pre-training by Representing and Predicting Spans, Mandar Joshi, et al., TACL, 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, et al., NAACL 2019, 2018.\nTANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection, Siddhant Garg, et al., AAAI 2020, Nov 2019.\nOverview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering, Asma Ben Abacha, et al., ACL-W 2019, Aug 2019.\nTowards Scalable and Reliable Capsule Networks for Challenging NLP Applications, Wei Zhao, et al., ACL 2019, Jun 2019.\nCognitive Graph for Multi-Hop Reading Comprehension at Scale, Ming Ding, et al., ACL 2019, Jun 2019.\nReal-Time Open-Domain Question Answering with Dense-Sparse Phrase Index, Minjoon Seo, et al., ACL 2019, Jun 2019.\nUnsupervised Question Answering by Cloze Translation, Patrick Lewis, et al., ACL 2019, Jun 2019.\nSemEval-2019 Task 10: Math Question Answering, Mark Hopkins, et al., ACL-W 2019, Jun 2019.\nImproving Question Answering over Incomplete KBs with Knowledge-Aware Reader, Wenhan Xiong, et al., ACL 2019, May 2019.\nMatching Article Pairs with Graphical Decomposition and Convolutions, Bang Liu, et al., ACL 2019, May 2019.\nEpisodic Memory Reader: Learning what to Remember for Question Answering from Streaming Data, Moonsu Han, et al., ACL 2019, Mar 2019.\nNatural Questions: a Benchmark for Question Answering Research, Tom Kwiatkowski, et al., TACL 2019, Jan 2019.\nTextbook Question<|endoftext|>"
    }
]