[
    {
        "entity": "Eunsol Choi",
        "step": 21295,
        "passage": " negative spans can be better divided with the correct answer \u201cJerusalem\u201d. This shows that SCL in our KECP framework is reliable and can improve the performance for EQA.\nThe Accuracy of Answer Generation. A major difference between previous works and ours is that we model the EQA task as text generation. Intuitively, if the model correctly generates the first answer token, it is easy to generate the remaining answer tokens because of the very small search space. Therefore, we analyze how difficult it is for the model to generate the first token correctly. Specifically, we check whether the generated first token and the first token of the ground truth are within a fixed window size nw. As shown in Table 5, we find the accuracy of our method is lower than RoBERTa-base Liu et al. (2019) when nw=1. Yet, we achieve the best performance when increasing the window size nw to 5. We think that our KECP can generate some rehabilitation text for the answer. For example in Figure 4, the PLM may generate \u201cthe conquest of Jerusalem\u201d rather than the correct answer with single token \u201cJerusalem\u201d. This phenomenon reflects the reason why we achieve lower accuracy when nw=1. But, we think that the generated results are still in the vicinity of the correct answer.\nTable 5: The accuracy of predicting the first [MASK] in the query prompt with full training samples for each task. #nw denotes the window size.\nTo bridge the gap between the pre-training and fine-tuning objectives, KECP views EQA as an answer generation task. In KECP, the knowledge-aware prompt encoder injects external domain-related knowledge into the passage, and then enhances the representations of selected prompt tokens in the query. The span-level contrastive learning objective is proposed to improve the performance of EQA. Experiments on multiple benchmarks show that our framework outperforms the state-of-the-art methods. In the future, we will i) further improve the performance of KECP by applying controllable text generation techniques, and ii) explore the prompt-tuning for other types of MRC tasks, such as cloze-style MRC and multiple-choice MRC.\nBrown et al. (2020) Tom B. Brown, Benjamin Mann, and etc. Nick Ryder. 2020. Language models are few-shot learners. In NeurIPS.\nChen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A simple framework for contrastive learning of visual representations. In ICML, volume 119, pages 1597\u20131607.\nDai et al. (2021) Damai Dai, Hua Zheng, Zhifang Sui, and Baobao Chang. 2021. Incorporating connections beyond knowledge embeddings: A plug-and-play module to enhance commonsense reasoning in machine reading comprehension. CoRR, abs/2103.14443.\nDettmers et al. (2018) Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018.\nConvolutional 2d knowledge graph embeddings.\nDunn et al. (2017) Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur G\u00fcney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q&a dataset augmented with context from a search engine. CoRR, abs/1704.05179.\nFisch et al. (2019) Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In EMNLP, pages 1\u201313.\nGao et al. (2021) Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In ACL, pages 3816\u20133830.\nHan et al. (2021) Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2021. PTR: prompt tuning with rules for text classification. CoRR, abs/2105.11259.\nJoshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predicting spans. TACL, 64\u201377.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL, pages 1601\u20131611.\nKwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, and et al. 2019. Natural questions: a benchmark for question answering research. TACL.\nLai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard H. Hovy. 2017. RACE: large-scale reading comprehension dataset from examinations. In EMNLP, pages 785\u2013794.\nLevy et al. (2017) Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In CoNLL, pages 333\u2013342.\nLi and Liang (2021a) Xiang Lisa Li and Percy Liang. 2021a. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP, pages 4582\u20134597. Association for Computational Linguistics.\nLi and Liang (2021b) Xiang Lisa Li and Percy Liang. 2021b. Prefix-tuning: Optimizing continuous prompts for generation. In ACL, pages 4582\u20134597.\nLiu et al. (2021a) Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021a. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. CoRR.\nLiu et al. (2021b) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT understands, too. CoRR, abs/2103.10385.\nLiu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, and et al. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR.\nQin and Eisner (2021) Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. In NAACL-HLT, pages 5203\u20135212.\nRajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable questions for squad. CoRR, abs/1806.03822.\nRajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. CoRR.\nRam et al. (2021) Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, and Omer Levy. 2021. Few-shot question answering by pretraining span selection. In ACL.\nSchick and Sch\u00fctze (2021) Timo Schick and Hinrich Sch\u00fctze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In EACL, pages 255\u2013269.\nShin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP.\nTrischler et al. (2017) Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017. NewsQA: A machine comprehension dataset. In WRLNLP, pages 191\u2013200.\nVan der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne.\nVinyals et al. (2015) Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In NIPS, pages 2692\u20132700.\nWang and Jiang (2019) Chao Wang and Hui Jiang. 2019. Explicit utilization of general knowledge in machine reading comprehension. In ACL, pages 2263\u20132272. Association for Computational Linguistics.\nWang et al. (2022) Chengyu Wang, Minghui Qiu, Taolin Zhang, Tingting Liu, Lei Li, Jianing Wang, Ming Wang, Jun<|endoftext|>"
    },
    {
        "entity": "Eunsol Choi",
        "step": 25213,
        "passage": " to favor the true solution. The model sometimes gives the wrong prediction\u2014for example, at t=16k, and changes its prediction from the true solution to the wrong solution, \u201837-36\u2019\u2014but again changes its prediction to be a true solution afterward. In addition, its intermediate wrong solution, \u201837-36\u2019 indicates the model was confused with distinguishing the longest field goal of Rob Bironas (40 vs. 37), which is an understandable mistake.\nWe also compare the predictions from the model with our method to those from the model with MML, which is shown in Appendix C.\nQuality of the predicted solution.\nWe analyze if the model outputs the correct solution, since the solution executing the correct answer could be spurious. First, on NarrativeQA and DROPnum, we manually analyze 50 samples from the development set and find that 98% and 92% of correct cases produce the correct solution respectively. Next, on WikiSQL, we compare the predictions from the model to the annotated SQL queries on the development set. This is possible because gold SQL queries are available in the dataset for the full supervision. Out of 8,421 examples, 7,110 predictions execute the correct answers. Among those, 88.5% of the predictions are exactly same as the annotated queries. Others are the cases where (i) both queries are correct, (ii) the model prediction is correct but the annotated query is incorrect, and (iii) the annotated query is correct and the model prediction is spurious. We show a full analysis in Appendix C.\nRobustness to the noise in |Z|.\nSometimes noise arises during the construction of |Z|, such as |Z| constructed based on ROUGE-L for NarrativeQA. To explore the effect of noise in Z, we experiment with more noisy solution set by picking all the spans with scores that is equal to or larger than the 5th highest. The new construction method increases |Z| from 4.3 to 7.1 on NarrativeQA. The result by MML objective drops significantly (56.07\u219251.14) while the result by ours drops marginally (58.77\u219257.97), suggesting that MML suffers more with a noisier Z while ours is more robust.\nIn this paper, we demonstrated that, for many QA tasks which only provide the answer text as supervision, it is possible to precompute a discrete set of possible solutions that contains one correct option. Then, we introduced a discrete latent variable learning algorithm which iterates a procedure of predicting the most likely solution in the precomputed set and further increasing the likelihood of that solution. We showed that this approach significantly outperforms previous approaches on six QA tasks including reading comprehension, open-domain QA, discrete reasoning task and semantic parsing, achieving absolute gains of 2\u201310% and setting the new state-of-the-art on five well-studied datasets.\nThis research was supported by ONR (N00014-18-1-2826, N00014-17-S-B001), DARPA N66001-19-2-403, NSF (IIS-1616112, IIS-1252835, IIS-1562364), ARO (W911NF-16-1-0121), an Allen Distinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google and Amazon.\nThe authors would like to thank the anonymous reviewers, Eunsol Choi, Christopher Clark, Victor Zhong and UW NLP members for their valuable feedback.\nAbolafia et al. (2018) Daniel A Abolafia, Mohammad Norouzi, Jonathan Shen, Rui Zhao, and Quoc V Le. 2018. Neural program synthesis with priority queue training. arXiv preprint arXiv:1801.03526.\nAgarwal et al. (2019) Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. 2019. Learning to generalize from sparse and underspecified rewards. In ICML.\nAlberti et al. (2019) Chris Alberti, Kenton Lee, and Michael Collins. 2019. A BERT baseline for the Natural Questions. arXiv preprint arXiv:1901.08634.\nArtzi and Zettlemoyer (2013) Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. In ACL.\nBerant et al. (2013) Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In EMNLP.\nChen et al. (2017) Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open-domain questions. In ACL.\nClarke et al. (2010) James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world\u2019s response. In CoNLL.\nDong and Lapata (2018) Li Dong and Mirella Lapata. 2018. Coarse-to-fine decoding for neural semantic parsing. In ACL.\nDua et al. (2019) Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL.\nHe et al. (2019) Pengcheng He, Yi Mao, Kaushik Chakrabarti, and Weizhu Chen. 2019. X-SQL: reinforce schema representation with context. arXiv preprint arXiv:1908.08113.\nHurley and Rickard (2009) Niall Hurley and Scott Rickard. 2009. Comparing measures of sparsity. IEEE Transactions on Information Theory.\nHwang et al. (2019) Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and Minjoon Seo. 2019. A comprehensive exploration on WikiSQL with table-aware word contextualization. arXiv preprint arXiv:1902.01069.\nIyyer et al. (2017) Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In ACL.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In ACL.\nKadlec et al. (2016) Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. 2016. Text understanding with the attention sum reader network. In ACL.\nKo\u010disk\u1ef3 et al. (2018) Tom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. TACL.\nKrishnamurthy et al. (2017) Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. 2017. Neural semantic parsing with type constraints for semi-structured tables. In EMNLP.\nKwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Change, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. TACL.\nLee et al. (2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In ACL.\nLiang et al. (2017) Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. 2017.\nLiang et al. (2018) Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V Le, and Ni Lao. 2018. Memory augmented policy optimization for program synthesis and semantic parsing. In NIPS.\nLin (2004) Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. Text Summarization Branches Out.\nMin et al. (2019) Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. Compositional questions do not necessitate multi-hop reasoning. In ACL.\nNishida et al. (2019) Kyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda, Atsushi Otsuka, Hisako Asano, and Junji Tomita. 2019. Multi-style generative reading comprehension. In ACL.\nPaszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017.\nAutomatic differentiation in PyTorch.\nRajpur<|endoftext|>"
    },
    {
        "entity": "Eunsol Choi",
        "step": 26983,
        "passage": " two for backward) of Skim-LSTM+Attention model. We see that the second layer skims more, implying that the second layer is more confident about which tokens are important.\nFigure 6 shows F1 score of LSTM+Attention model using standard LSTM and Skim LSTM, sorted in ascending order by Flop-R. While models tend to perform better with larger computational cost, Skim LSTM (Red) outperforms standard LSTM (Blue) with comparable computational cost. We also observe that the F1 score of Skim-LSTM is more stable across different configurations and computational cost. Moreover, increasing the value of \u03b3 for Skim-LSTM gradually increases skipping rate and Flop-R, while it also leads to reduced accuracy.\nControlling skim rate. An important advantage of Skim-RNN is that the skim rate (and thus computational cost) can be dynamically controlled at inference time by adjusting the threshold for \u2018skim\u2019 decision probability p1t (Equation 1). Figure 6 shows the trade-off between the accuracy and computational cost for two settings, confirming the importance of skimming (d\u2032>0) compared to skipping (d\u2032=0).\nVisualization. Figure 7 shows an example from SQuAD and visualizes which words Skim-LSTM (d=100,d\u2032=20) reads (red) and skims (white). As expected, the model does not skim when the input seems to be relevant to answering the question. In addition, LSTM in second layer skims more than that in the first layer mainly because the second layer is more confident about the importance of each token, as shown in Figure 7. More visualizations are shown in in Appendix C.\nd=100 (batch size = 1) in all three frameworks on a single thread of CPU (averaged over 100 trials), and have observed that NumPy is 1.5 and 2.8 times faster than TensorFlow and PyTorch.888NumPy\u2019s speed becomes similar to that of TensorFlow and PyTorch at d=220 and d=700, respectively. At larger hidden size, NumPy becomes slower. This seems to be mostly due to the fact that the frameworks are primarily (optimized) for GPUs and they have larger overhead than NumPy that they cannot take much advantage of reducing the size of the hidden state of the LSTM below 100.\nFigure 8: Speed up rate of Skim-LSTM (vs LSTM) with varying skimming rates and hidden state sizes.\nFigure 8 shows the relative speed gain of Skim-LSTM compared to standard LSTM with varying hidden state size and skim rate. We use NumPy, and the inferences are run on a single thread of CPU. We also plot the ratio between the reduction of the number of float operations (Flop-R) of LSTM and Skim-LSTM. This can be considered as a theoretical upper bound of the speed gain on CPUs. We note two important observations. First, there is an inevitable gap between the actual gain (solid line) and the theoretical gain (dotted line). This gap will be larger with more overhead of the framework, or more parallelization (e.g. multithreading). Second, the gap decreases as the hidden state size increases because the the overhead becomes negligible with very large matrix operations. Hence, the benefit of Skim-RNN will be greater for larger hidden state size.\nLatency. A modern GPU has much higher throughput than a CPU with parallel processing. However, for small networks, the CPU often has lower latency than the GPU. Comparing between NumPy with CPU and TensorFlow with GPU (Titan X), we observe that the former has 1.5 times lower latency (75 \\upmus vs 110 \\upmus per token) for LSTM of d=100. This means that combining Skim-RNN with CPU-based framework can lead to substantially lower latency than GPUs. For instance, Skim-RNN with CPU on IMDb has 4.5x lower latency than a GPU, requiring only 29 \\upmus per token on average.\nWe present Skim-RNN, a recurrent neural network that can dynamically decide to use the big RNN (read) or the small RNN (skim) at each time step, depending on the importance of the input. While Skim-RNN has significantly lower computational cost than its RNN counterpart, the accuracy of Skim-RNN is still on par with or better than standard RNNs, LSTM-Jump, and VCRNN. Since Skim-RNN has the same input and output interface as an RNN, it can easily replace RNNs in existing applications. We also show that a Skim-RNN can offer better latency results on a CPU compared to a standard RNN on a GPU. Future work involves using Skim-RNN for applications that require much higher hidden state size, such as video understanding, and using multiple small RNN cells for varying degrees of skimming.\nThis research was supported by the NSF (IIS 1616112), Allen Distinguished Investigator Award, Samsung GRO award, and gifts from Google, Amazon, Allen Institute for AI, and Bloomberg. We thank the anonymous reviewers for their helpful comments.\nBalduzzi & Ghifary (2016) David Balduzzi and Muhammad Ghifary. Strongly-typed recurrent neural networks. In ICML, 2016.\nCampos et al. (2017) V\u00edctor Campos, Brendan Jou, Xavier Gir\u00f3-i Nieto, Jordi Torres, and Shih-Fu Chang. Skip rnn: Learning to skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834, 2017.\nChoi et al. (2017) Eunsol Choi, Daniel Hewlett, Alexandre Lacoste, Illia Polosukhin, Jakob Uszkoreit, and Jonathan Berant. Coarse-to-fine question answering for long documents. In ACL, 2017.\nChung et al. (2017) Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In ICLR, 2017.\nDyer et al. (2016) Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network grammars. In NAACL, 2016.\nHahn & Keller (2016) Michael Hahn and Frank Keller. Modeling human reading with neural attention. In EMNLP, 2016.\nJang et al. (2017) Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017.\nJernite et al. (2017) Yacine Jernite, Edouard Grave, Armand Joulin, and Tomas Mikolov. Variable computation in recurrent neural networks. In ICLR, 2017.\nJohansen et al. (2017) Alexander Johansen, Bryan McCann, James Bradbury, and Richard Socher. Learning when to read and when to skim, 2017. URL https://metamind.io/research/learning-when-to-skim-and-when-to-read.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL, 2017.\nKembhavi et al. (2017) Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In CVPR, 2017.\nKokkinos & Potamianos (2017) Filippos Kokkinos and Alexandros Potamianos. Structural attention neural networks for improved sentiment analysis. arXiv preprint arXiv:1701.01811, 2017.\nKong et al. (2016) Lingpeng Kong, Chris Dyer, and Noah A Smith. Segmental recurrent neural networks. In ICLR, 2016.\nMarcel Adam Just (1987) Patricia Anderson Carpenter Marcel Adam Just. The Psychology of Reading and Language Comprehension. 1987.\nMikolov et al. (2015) Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc\u2019Aurelio Ranzato. Learning longer memory in recurrent neural networks. In ICLR Workshop, 2015.\nMin et al. (2017) Sewon Min, Minjoon Seo, and Hannaneh Hajishirzi.\nQuestion answering through transfer learning from large fine-grained supervision data.\nMiyato et al. (2017) Takeru Miyato, Andrew M. Dai, and Ian Goodfellow. Adversarial training methods for semi-supervised text classification. In ICLR, 2017.\nMnih et al. (2014) Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In NIPS, 2014.\nOdena et al. (2017) Augustus Odena, Dieterich Lawson, and Christopher Olah.\nChanging model behavior at test-time using reinforcement learning.\nIn ICLR Workshop, 2017.\nRastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon<|endoftext|>"
    },
    {
        "entity": "Eunsol Choi",
        "step": 27657,
        "passage": ", Darsh J. Shah, and Regina Barzilay. 2020. The Limitations of Stylometry for Detecting Machine-Generated Fake News. Computational Linguistics.\nZellers, Rowan, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. \"Defending against neural fake news.\" In Advances in Neural Information Processing Systems, pp. 9051-9062. 2019.\nThorne, James, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2019. \"Evaluating adversarial attacks against multiple fact verification systems.\" EMNLP-IJCNLP, pp. 2937-2946.\nWang, William Yang. 2017.  ACL 2017.\nXinyi Zhou and Reza Zafarani. 2018. Fake news: A survey of research, detection methods, and opportunities.\nKarishma Sharma, Feng Qian, He Jiang, Natali Ruchansky, Ming Zhang, and Yan Liu. 2019. Combating fake news: A survey on identification and mitigation techniques.\nPepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein. 2020. Generating Fact Checking Explanations. ACL 2020.\nSchuster, Tal, Darsh J. Shah, Yun Jie Serene Yeo, Daniel Filizzola, Enrico Santus, and Regina Barzilay. 2019. \"Towards debiasing fact verification models.\" EMNLP 2019.\nShaden Shaar, Giovanni Da San Martino, Nikolay Babulkov, Preslav Nakov. 2020. That is a Known Lie: Detecting Previously Fact-Checked Claims. ACL 2020.\nKai Nakamura, Sharon Levy, William Yang Wang. 2019. \">r/Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection. \"\nRashkin, Hannah, Eunsol Choi, Jin Yea Jang, Svitlana Volkova, and Yejin Choi. 2017. \"Truth of varying shades: Analyzing language in fake news and political fact-checking.\" EMNLP 2017.\nSap, Maarten, Marcella Cindy Prasettio, Ari Holtzman, Hannah Rashkin, and Yejin Choi. 2017. \"Connotation frames of power and agency in modern films.\" In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2329-2334. 2017.\nMarta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Linguistic Models for Analyzing and Detecting Biased Language. Proceedings of ACL 2013.\nHaewoon Kwak and Jisun An and Yong-Yeol Ahn. 2020. FrameAxis: Characterizing Framing Bias and Intensity with Word Embedding.\nCard, Dallas, Amber Boydstun, Justin H. Gross, Philip Resnik, and Noah A. Smith. 2015. The media frames corpus: Annotations of frames across issues. ACL 2015.\nDallas Card, Justin H. Gross, Amber E. Boydstun, Noah A. Smith. 2016. Analyzing Framing through the Casts of Characters in the News. EMNLP 2016.\nAjjour, Yamen, Milad Alshomary, Henning Wachsmuth, and Benno Stein. 2019. Modeling frames in argumentation. EMNLP-IJCNLP 2019.\nHartmann, Mareike, Tallulah Jansen, Isabelle Augenstein, and Anders S\u00f8gaard. 2019. Issue Framing in Online Discussion Fora. NAACL 2019.\nJohnson, Kristen, and Dan Goldwasser. 2018. Classification of moral foundations in microblog political discourse. ACL 2018.\nEmma Strubell, Ananya Ganesh and Andrew McCallum. 2019. Energy and Policy Considerations for Deep Learning in NLP. ACL 2019.\nHenderson, Peter, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. 2018. Ethical challenges in data-driven dialogue systems. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp. 123-129. ACM, 2018.\nChin, Hyojin, Lebogang Wame Molefi, and Mun Yong Yi. 2020. \"Empathy Is All You Need: How a Conversational Agent Should Respond to Verbal Abuse.\" In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pp. 1-13. 2020.\nNeff, Gina, and Peter Nagy. 2016. \"Automation, algorithms, and politics| talking to Bots: Symbiotic agency and the case of Tay.\" International Journal of Communication 10 (2016): 17.\nSchlesinger, Ari, Kenton P. O'Hara, and Alex S. Taylor. 2018. Let's talk about race: Identity, chatbots, and AI. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, p. 315. ACM, 2018.\nLiu, Chia-Wei, Ryan Lowe, Iulian V. Serban, Michael Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. EMNLP 2016.\nL\u00e4ubli, Samuel, Sheila Castilho, Graham Neubig, Rico Sennrich, Qinlan Shen, and Antonio Toral. \"A Set of Recommendations for Assessing Human\u2013Machine Parity in Language Translation.\" Journal of Artificial Intelligence Research 67 (2020): 653-672.\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, Noah A. Smith. 2019. Show Your Work: Improved Reporting of Experimental Results In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.\nCrane, M. (2018). Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results. Transactions of the Association for Computational Linguistics, 6, 241\u2013252.\nJoseph P. Simmons, Leif D. Nelson, Uri Simonsohn. 2011. False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science 22:11, 1359-1366.\nRotem Dror, Lotem Peled-Cohen, Segev Shlomov and, Roi Reichart. 2020. \"Statistical Significance Testing for Natural Language Processing.\" Morgan Claypool Human Language Technology series.\nDrew McDermott. 1976. Artificial Intellgience Meets Natural Stupidity. ACM SIGART Bulletin, (57), 4-9.\nMatt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. 2020. Evaluating NLP Models via Contrast Sets.\nAkela Lacy, Alice Speri, Jordan Smith, Sam Biddle. 2020. Prisons launch \"absurd\" attempt to detect coronavirus in inmate phone calls. The Intercept, April 21, 2020.\nDavid Jurgens, Yulia Tsvetkov, and Dan Jurafsky. 2017. Writer Profiling Without the Writer's Text.\nBoyd, Danah, and Alice E. Marwick. \"Social privacy in networked publics: Teens\u2019 attitudes, practices, and strategies.\" In A decade in internet time: Symposium on the dynamics of the internet and society. 2011.\nYoav Goldberg (2018) 4gram language models share secrets too\u2026, Github.\nHenderson, Peter, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. 2018. \"Ethical challenges in data-driven dialogue systems.\" In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp. 123-129. ACM, 2018.\nShoshana Zuboff. The Age of Surveillance Capitalism. Selections.\nAll week Individual meetings with Dan, Peter, Hang on projects.\nSeminar meetings: are on Tuesday 4:30-6:20pm Pacific Time by Zoom.\nContact: Students should ask all course-related questions in the Piazza forum, where you will also find announcements. For personal matters that you don't wish to put in a private Piazza post, you can email us at |||EMAIL_ADDRESS||| Academic accommodations: If you need an academic accommodation based on a disability, you should initiate the request with the Office of Accessible Education (OAE) before the end of the second week of classes, April 17.\nCompletion of both CS224N and CS224U is strictly<|endoftext|>"
    },
    {
        "entity": "Eunsol Choi",
        "step": 28960,
        "passage": "Eunsol Choi (class of 2012, Cornell, honorable mention for the CRA Outstanding Undergraduate Re- search Award in 2011), now a PhD student at the University of Washington.\nKelvin Luu (class of 2015, Cornell), now a PhD student at the University of Washington. Cora Schneck (class of 2018, University of Colorado Boulder).\nTyler Scott (class of 2018, University of Colorado Boulder).\nXiaochuang Han (class of 2019, Georgia Tech).\nDavid Atkinson (class of 2019, University of Colorado Boulder).<|endoftext|>"
    },
    {
        "entity": "Eunsol Choi",
        "step": 29346,
        "passage": " Lewis, and Luke Zettlemoyer. 2016. Human-in-the-loop parsing. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2337\u20132342, Austin, Texas.\nIyer et al. (2017) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. 2017. Learning a neural semantic parser from user feedback. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 963\u2013973, Vancouver, Canada.\nJia and Liang (2016) Robin Jia and Percy Liang. 2016. Data recombination for neural semantic parsing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12\u201322, Berlin, Germany.\nKate and Mooney (2006) Rohit J Kate and Raymond J Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 913\u2013920, Sydney, Australia.\nKate et al. (2005) Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning to Transform Natural to Formal Languages. In Proceedings for the 20th National Conference on Artificial Intelligence, pages 1062\u20131068, Pittsburgh, Pennsylvania.\nKo\u010disk\u00fd et al. (2016) Tom\u00e1\u0161 Ko\u010disk\u00fd, G\u00e1bor Melis, Edward Grefenstette, Chris Dyer, Wang Ling, Phil Blunsom, and Karl Moritz Hermann. 2016. Semantic parsing with semi-supervised sequential autoencoders. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1078\u20131087, Austin, Texas.\nKrishnamurthy (2016) Jayant Krishnamurthy. 2016. Probabilistic models for learning a semantic parser lexicon. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 606\u2013616.\nKrishnamurthy and Mitchell (2012) Jayant Krishnamurthy and Tom Mitchell. 2012. Weakly supervised training of semantic parsers. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 754\u2013765, Jeju Island, Korea.\nKrishnamurthy and Mitchell (2015) Jayant Krishnamurthy and Tom M Mitchell. 2015. Learning a compositional semantics for freebase with an open predicate vocabulary. Transactions of the Association for Computational Linguistics, 3:257\u2013270.\nKwiatkowksi et al. (2010) Tom Kwiatkowksi, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223\u20131233, Cambridge, MA.\nKwiatkowski et al. (2013) Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545\u20131556, Seattle, Washington, USA.\nKwiatkowski et al. (2011) Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in CCG grammar induction for semantic parsing. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1512\u20131523, Edinburgh, Scotland.\nLiang et al. (2016) Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. 2016. Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision. arXiv preprint arXiv:1611.00020.\nLiang et al. (2011) Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 590\u2013599, Portland, Oregon.\nLu et al. (2008) Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 783\u2013792, Honolulu, Hawaii.\nMarcheggiani and Titov (2016) Diego Marcheggiani and Ivan Titov. 2016. Discrete-state variational autoencoders for joint discovery and factorization of relations. Transactions of the Association for Computational Linguistics, 4:231\u2013244.\n, pages 1671\u20131678, Edinburgh, Scotland.\nMiao and Blunsom (2016) Yishu Miao and Phil Blunsom. 2016. Language as a latent variable: Discrete generative models for sentence compression. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 319\u2013328, Austin, Texas.\nMnih and Gregor (2014) Andriy Mnih and Karol Gregor. 2014. Neural variational inference and learning in belief networks. In Proceedings of the 31st International Conference on Machine Learning, pages 1791\u20131799, Bejing, China.\nPasupat and Liang (2016) Panupong Pasupat and Percy Liang. 2016. Inferring logical forms from denotations. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 23\u201332, Berlin, Germany.\nPennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.\nReddy et al. (2017) Siva Reddy, Oscar T\u00e4ckstr\u00f6m, Slav Petrov, Mark Steedman, and Mirella Lapata. 2017. Universal semantic parsing. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 89\u2013101, Copenhagen, Denmark.\nSu et al. (2016) Yu Su, Huan Sun, Brian Sadler, Mudhakar Srivatsa, Izzeddin Gur, Zenghui Yan, and Xifeng Yan. 2016. On generating characteristic-rich question sets for qa evaluation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 562\u2013572, Austin, Texas.\nIn Proceedings of the 30th International Conference on Machine Learning, pages 1139\u20131147, Atlanta, Georgia.\nWen et al. (2015) Tsung-Hsien Wen, Milica Gasic, Nikola Mrk\u0161i\u0107, Pei-Hao Su, David Vandyke, and Steve Young. 2015.\nSemantically conditioned LSTM-based natural language generation for spoken dialogue systems.\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1711\u20131721, Lisbon, Portugal.\nWong and Mooney (2006) Yuk Wah Wong and Raymond Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 439\u2013446, New York City, USA.\nXu et al. (2016) Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2016. Question answering on Freebase via relation extraction and textual evidence. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2326\u20132336, Berlin, Germany.\nYao and Van Durme (2014) Xuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with Freebase. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 956\u2013966, Baltimore, Maryland.\nYih et al. (2015) Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1321\u20131331, Beijing, China.\nYin et al. (2018) Pengcheng Yin, Chunting Zhou, Junxian He, and Graham Neubig. 2018. StructVAE: Tree-structured latent variable models for semi-supervised semantic parsing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 754\u2013765, Melbourne, Australia.\nZelle and Mooney (1996) John M. Zelle and Raymond J. Mooney. 1996.\nIn Proceedings of the 13th National Conference on Artificial Intelligence, pages 1050\u20131055, Portland, Oregon.\nZett<|endoftext|>"
    },
    {
        "entity": "Eunsol Choi",
        "step": 29750,
        "passage": "Veerle Draulans, Centre for Sociological Research, KU Leuven, Belgium. Giovanni Lamura, INRCA, National Institute of Health & Science on Ageing, Italy.\nVeerle Draulans took her PhD in 1994 at KU Leuven (Belgium) in Ethics. She combined an appointment at Tilburg University, the Netherlands (master ethics,.\nP. P\u00e9rin, F. Vallet, T. Calligaro, D. Bagault, J.-P. Poirot. 4.2. The mineralogy of B\u00f6ttger stoneware. W. Kockelmann, A. Kirfel, C. Neelmeijer, H.-M. Walcha. 4.3.\n3 Jul 2018...... Scientific Committee doi:10.18086/solar.2018.01.01 Available at http://proceedings.ises.org... World Renewable. Energy Forum, Paper 0273.\nThe Use of Microsurfacing as a Cost Effective Remedial Action for...\nproprietary Microsurfacing product called Colrut\u2122 as a cost effective remedial action to overcome extensive surface rutting problems on provincial roads in...\nMore than 200 running. Actions are presented and complementary Action information... The European Cooperation in Science and Technology (COST) is an. EU-funded programme... Of more than 750 carotenoids described, only about 10...\n3 Mar 2017... weight, no matter how small or light the object is, benefitted differentiation between weight and density (e.g. Maclin et al. 1997, Seigler & Chen...\nPROCEEDINGS of the FIFTH INTERNATIONAL CONFERENCE of...\n13 Jun 2018... 2007); Maria Luisa Neri, 'L'occhio dello straniero.... territory of Aarschot and the urban palace in Brussels, had suffered from the. Protestant...\nlearning \u2013 the SM is using VLS \u201cMOODLE\u201d), and to guide adult learners in their personal... The average coefficient of abilities gained by the first group ( ab. \u041a... 13 The partnership consists of VDAB, SYNTRA and the Curriculum Department of...\nD6.2: Proceedings of the 1 EC CAD Conference - European...\n19 Sep 2017... Environment Melanie Schultz van Haegen. The minister's core message was for national member states to work together on cross border...\nFor screws in solid wood according to EN 14081-1 [28], or glued laminated timber made from softwood according to EN 14080 [30], which do not fulfil the require...\ntaken place at Sheraton Rhodes Resort, Rhodes, Greece, between 1st and 5th May 2019. The aim of the conference is to bring together computational scientists...\nallow not only to raise functional of the AE ISHM systems as the tool for safety, but also to transform them... The streaming has been also used in parallel without any reduction of performance of traditional... The laser welding process induced AE, which was recorded by a piezoelectric sensor Pico... BOKU, Vienna, Austria.\nPROCEEDINGS of the 5th International Scientific Conference on...\n16 May 2018... the value of consumption of food-related and non-food-related goods and services is expressed in one currency e.g.:... by other non-economic measures of obiective-social and subjective nature. REFERENCES... files/folders.\nThe results presented are related to several INBO projects (Flemish Region), the project DO-IT HOUTBOUW (Flemish region) and a PROFCOL project (Walloon.\n23 Nov 2018... MICRO 2018, Fate and Impact of Microplastics: Knowledge, Actions... premi\u00e8re as part of the festival \u201cThe Universal Sea \u2013 Pure or Plastic?!\n17 Oct 2018... Ali Farzan Moghaddam (Gent University)*;. Alex Van den Bossche (University of Gent. Belguim)... Adriana Aguilera Gonzalez (ESTIA)*; Ionel.\n4 Mar 2018... preparing for the Christmas performance and constraints of pupil availability... offers bachelor study programmes at different campuses in Aalst,.\nConference Proceedings Volume 2 \u2014 Short Papers, Panels, Posters, Demos,... Together We Can Beat this Game: The Prevalence of Collaborative Learning in... content is accessed and interacted with (e.g., downloading vs. streaming video).... In order to understand the culture of The Hunger Games affinity space, it is...\n14 Jun 2015... conclude the invalidity of a noma cool against the. Constitution and desaplic\u00e1 her in the case, and that the concentrated control and abstract is...\nProceedings of the First OIE Global Conference on Evolving...\n14 Oct 2009... Saegerman C., Speybroeck N., Roels S., Vanopdenbosch E., Thiry E.... des droits consid\u00e9r\u00e9 comme un bien meuble, la r\u00e9glementation de sa.\nJohan Verbeke, KU Leuven, Faculty of Architecture, Belgium... The Gesu site is situated East of the Botanique and it's botanical garden, in the heart of.\n29 Jul 2019... Cztorrent.net is the most important domestic player in this category. Thepiratebay3.org, Rarbg.to, 1337x and Torrentz2.to are the most important.\nFinal text editing and conclusions and recommendations: Mr. T. de Wit... A few years ago in Rome paid parking and parking control was widely extended and...\nThe starting point has been activating students as producers of knowledge and... iMessage, Blackboard, TES (an online forum for educators) and Scoodle.\n8 Mar 2017...... which create attractiveness like entertainment centers, cultural and sport service establishments,... Van Dessel, M. M & Patti, C.H. (2016),.\nProceedings of the 15th Conference of the European Chapter of the...\nChi Kit Cheung, David Chiang, Christian Chiarcos, Do Kook Choe, Eunsol Choi, Monojit Choud-... 0.343 since they rarely match due to their spar- sity. The LCB...\n18 Aug 2017... Department of Management Scieneces, University of Waterloo, Waterloo,... Damart S (2010) A cognitive mapping approach to organizing the...\nProceedings of the 36th International Conference on Animal...\n(5-hmC) ELISA Easy Kit (Colourimetric) and percent global DNA methylation was determined... Institute, Hasselt University, Diepenbeek, Belgium; 7Cell Biology.\nOperation 18. Peter Luerkens, Rik W. De Doncker, Asimenia Korompili, Antonello Monti, Albert Moser, Jens Priebe, Johannes Voss, RWTH Aachen University, D.\nPlano de. Conserva\u00e7ao da Bacia do Alto Paraguay (PCBAP) Pantanal. 7. Padovani C. R., Carvalho N. O,... Sluice Pollare. Sluice Denderleeuw. Point source 13.\nintegration of which creates a slow, ombra topic, confirmed by the tremelo strings... as The Vixen, where themes of nature and environment are central to the plot...<|endoftext|>"
    },
    {
        "entity": "Eunsol Choi",
        "step": 29829,
        "passage": " substitutes for human interpretation and judgement. The rapid progress of NLG and the drawbacks of existing evaluation methods calls for the development of novel ways to assess the quality and success of NLG systems.\nIn \u201cBLEURT: Learning Robust Metrics for Text Generation\u201d (presented during ACL 2020), we introduce a novel automatic metric that delivers ratings that are robust and reach an unprecedented level of quality, much closer to human annotation. BLEURT (Bilingual Evaluation Understudy with Representations from Transformers) builds upon recent advances in transfer learning to capture widespread linguistic phenomena, such as paraphrasing. The metric is available on Github.\nIn human evaluation, a piece of generated text is presented to annotators, who are tasked with assessing its quality with respect to its fluency and meaning. The text is typically shown side-by-side with a reference, authored by a human or mined from the Web.\nAn example questionnaire used for human evaluation in machine translation.\nThe advantage of this method is that it is accurate: people are still unrivaled when it comes to evaluating the quality of a piece of text. However, this method of evaluation can easily take days and involve dozens of people for just a few thousand examples, which disrupts the model development workflow.\nIn contrast, the idea behind automatic metrics is to provide a cheap, low-latency proxy for human-quality measurements. Automatic metrics often take two sentences as input, a candidate and a reference, and they return a score that indicates to what extent the former resembles the latter, typically using lexical overlap. A popular metric is BLEU, which counts the sequences of words in the candidate that also appear in the reference (the BLEU score is very similar to precision).\nThe advantages and weaknesses of automatic metrics are the opposite of those that come with human evaluation. Automatic metrics are convenient \u2014 they can be computed in real-time throughout the training process (e.g., for plotting with Tensorboard). However, they are often inaccurate due to their focus on surface-level similarities and they fail to capture the diversity of human language. Frequently, there are many perfectly valid sentences that can convey the same meaning. Overlap-based metrics that rely exclusively on lexical matches unfairly reward those that resemble the reference in their surface form, even if they do not accurately capture meaning, and penalize other paraphrases.\nBLEU scores for three candidate sentences. Candidate 2 is semantically close to the reference, and yet its score is lower than Candidate 3.\nIdeally, an evaluation method for NLG should combine the advantages of both human evaluation and automatic metrics \u2014 it should be relatively cheap to compute, but flexible enough to cope with linguistic diversity.\nBLEURT is a novel, machine learning-based automatic metric that can capture non-trivial semantic similarities between sentences. It is trained on a public collection of ratings (the WMT Metrics Shared Task dataset) as well as additional ratings provided by the user.\nThree candidate sentences rated by BLEURT. BLEURT captures that candidate 2 is similar to the reference, even though it contains more non-reference words than candidate 3.\nCreating a metric based on machine learning poses a fundamental challenge: the metric should do well consistently on a wide range of tasks and domains, and over time. However, there is only a limited amount of training data. Indeed, public data is sparse \u2014 the WMT Metrics Task dataset, the largest collection of human ratings at the time of writing, contains ~260K human ratings covering the news domain only. This is too limited to train a metric suited for the evaluation of NLG systems of the future.\nTo address this problem, we employ transfer learning. First, we use the contextual word representations of BERT, a state-of-the-art unsupervised representation learning method for language understanding that has already been successfully incorporated into NLG metrics (e.g., YiSi or BERTscore).\nSecond, we introduce a novel pre-training scheme to increase BLEURT\u2019s robustness. Our experiments reveal that training a regression model directly over publicly available human ratings is a brittle approach, since we cannot control in what domain and across what time span the metric will be used. The accuracy is likely to drop in the presence of domain drift, i.e., when the text used comes from a different domain than the training sentence pairs. It may also drop when there is a quality drift, when the ratings to be predicted are higher than those used during training \u2014 a feature which would normally be good news because it indicates that ML research is making progress.\nThe success of BLEURT relies on \u201cwarming-up\u201d the model using millions of synthetic sentence pairs before fine-tuning on human ratings. We generated training data by applying random perturbations to sentences from Wikipedia. Instead of collecting human ratings, we use a collection of metrics and models from the literature (including BLEU), which allows the number of training examples to be scaled up at very low cost.\nBLEURT\u2019s data generation process combines random perturbations and scoring with pre-existing metrics and models.\nExperiments reveal that pre-training significantly increases BLEURT\u2019s accuracy, especially when the test data is out-of-distribution.\nWe pre-train BLEURT twice, first with a language modelling objective (as explained in the original BERT paper), then with a collection of NLG evaluation objectives. We then fine-tune the model on the WMT Metrics dataset, on a set of ratings provided by the user, or a combination of both.The following figure illustrates BLEURT\u2019s training procedure end-to-end.\nWe benchmark BLEURT against competing approaches and show that it offers superior performance, correlating well with human ratings on the WMT Metrics Shared Task (machine translation) and the WebNLG Challenge (data-to-text). For example, BLEURT is ~48% more accurate than BLEU on the WMT Metrics Shared Task of 2019. We also demonstrate that pre-training helps BLEURT cope with quality drift.\nCorrelation between different metrics and human ratings on the WMT\u201919 Metrics Shared Task.\nAs NLG models have gotten better over time, evaluation metrics have become an important bottleneck for the research in this field. There are good reasons why overlap-based metrics are so popular: they are simple, consistent, and they do not require any training data. In the use cases where multiple reference sentences are available for each candidate, they can be very accurate. While they play a critical part in our infrastructure, they are also very conservative, and only give an incomplete picture of NLG systems\u2019 performance. Our view is that ML engineers should enrich their evaluation toolkits with more flexible, semantic-level metrics.\nBLEURT is our attempt to capture NLG quality beyond surface overlap. Thanks to BERT\u2019s representations and a novel pre-training scheme, our metric yields SOTA performance on two academic benchmarks, and we are currently investigating how it can improve Google products. Future research includes investigating multilinguality and multimodality.\nThis project was co-advised by Dipanjan Das. We thank Slav Petrov, Eunsol Choi, Nicholas FitzGerald, Jacob Devlin, Madhavan Kidambi, Ming-Wei Chang, and all the members of the Google Research Language team.<|endoftext|>"
    }
]