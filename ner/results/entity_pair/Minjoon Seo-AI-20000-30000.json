[
    {
        "entity_pair": "Minjoon Seo & AI",
        "step": 21284,
        "passage": "\"\"\"Top-level model classes.\n\nAuthor:\n    Chris Chute |||EMAIL_ADDRESS||| \"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport layers\n\n\nclass BiDAF(nn.Module):\n    \"\"\"Baseline BiDAF model for SQuAD.\n\n    Based on the paper:\n    \"Bidirectional Attention Flow for Machine Comprehension\"\n    by Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n    (https://arxiv.org/abs/1611.01603).\n\n    Follows a high-level structure commonly found in SQuAD models:\n        - Embedding layer: Embed word indices to get word vectors.\n        - Encoder layer: Encode the embedded sequence.\n        - Attention layer: Apply an attention mechanism to the encoded sequence.\n        - Model encoder layer: Encode the sequence again.\n        - Output layer: Simple layer (e.g., fc + softmax) to get final outputs.\n\n    Args:\n        word_vectors (torch.Tensor): Pre-trained word vectors.\n        hidden_size (int): Number of features in the hidden state at each layer.\n        drop_prob (float): Dropout probability.\n    \"\"\"\n\n    def __init__(self, word_vectors, hidden_size, drop_prob=0.):\n        super(BiDAF, self).__init__()\n        self.emb = layers.Embedding(word_vectors=word_vectors,\n                                    hidden_size=hidden_size,\n                                    drop_prob=drop_prob)\n\n        self.enc = layers.RNNEncoder(input_size=hidden_size,\n                                     hidden_size=hidden_size,\n                                     num_layers=1,\n                                     drop_prob=drop_prob)\n\n        self.att = layers.BiDAFAttention(hidden_size=2 * hidden_size,\n                                         drop_prob=drop_prob)\n\n        self.mod = layers.RNNEncoder(input_size=8 * hidden_size,\n                                     hidden_size=hidden_size,\n                                     num_layers=2,\n                                     drop_prob=drop_prob)\n\n        self.out = layers.BiDAFOutput(hidden_size=hidden_size,\n                                      drop_prob=drop_prob)\n\n    def forward(self, cw_idxs, qw_idxs):\n        c_mask = torch.zeros_like(cw_idxs)!= cw_idxs\n        q_mask = torch.zeros_like(qw_idxs)!= qw_idxs\n        c_len, q_len = c_mask.sum(-1), q_mask.sum(-1)\n\n        c_emb = self.emb(cw_idxs)  # (batch_size, c_len, hidden_size)\n        q_emb = self.emb(qw_idxs)  # (batch_size, q_len, hidden_size)\n\n        c_enc = self.enc(c_emb, c_len)  # (batch_size, c_len, 2 * hidden_size)\n        q_enc = self.enc(q_emb, q_len)  # (batch_size, q_len, 2 * hidden_size)\n\n        att = self.att(c_enc, q_enc,\n                       c_mask, q_mask)  # (batch_size, c_len, 8 * hidden_size)\n\n        mod = self.mod(att, c_len)  # (batch_size, c_len, 2 * hidden_size)\n\n        out = self.out(att, mod, c_mask)  # 2 tensors, each (batch_size, c_len)\n\n        return out\n\n\nclass BiDAFExtra(nn.Module):\n    \"\"\"Baseline BiDAF model for SQuAD.\n\n    Based on the paper:\n<|endoftext|>"
    },
    {
        "entity_pair": "Minjoon Seo & AI",
        "step": 23490,
        "passage": "Samsung Electronics has uncovered a new plan for shaping the future with AI and semiconductors by unveiling new milestones in AI-based semiconductor and material innovation. The tech goliath disclosed this today at the ongoing Samsung AI Forum 2022.\nSamsung AI Forum is an annual AI event hosted by Samsung Electronics where world-renowned academics, researchers, and industry experts will come together to share their insights on the future of artificial intelligence, AI.\nCurrently, the event is hosted at the Samsung Advanced Institute of Technology with over 1,200 attendees. This year, Samsung AI Forum begins today, November 8th, and will conclude tomorrow, November 9th. This is the first time in three years that the event will be held in person due to the emergence of the novel coronavirus.\nUnder the theme of \u201cShaping the Future with AI and Semiconductor,\u201d Samsung AI experts discuss the future direction of AI research that will create new milestones in AI-based semiconductor and material innovation.\nProfessor Yoshua Bengio of the University of Montreal, Canada, shared his latest research in a keynote presentation, \u201cWhy We Need Amortized, Causal and Bayesian World Models.\u201d In causal models for AI that investigate theories and plan experiments in the realm of science and general AI, he emphasized the use of amortized inference and the Bayesian approach.\nMeanwhile, in his opening remarks, Jong-Hee (JH) Han, Vice Chairman, CEO, and Head of the Device Experience (DX) Division at Samsung Electronics, said AI technology would provide better convenience and new experiences for all.\nIn the \u201cAI for R&D Innovation\u201d session, research leaders at SAIT, including the Executive Vice President and Head of SAIT\u2019s AI Research Center, Changkyu Choi, shared the status and vision of Samsung\u2019s research on AI. In particular, they talked about how AI technology will affect industries like semiconductors and material development.\nMinjoon Seo, a professor at KAIST, and Hyunoh Song, a professor at Seoul National University, gave a presentation on the most recent research accomplishments in AI algorithms in a session titled \u201cRecent Advances of AI Algorithms.\u201d This included a large language model-based interface for ultra-accurate semantic search.\nHowever, according to the firm, day two is themed \u201cScaling AI for the Real World.\u201d There Samsung is expected to share the development direction of future AI technologies that will significantly affect our lives, including hyperscale AI, digital humanities, and robotics.\nSebastian Seung, President and Head of Samsung Research will give a keynote on the \u201cfirst steps of an effort to improve upon classical brain theories by optimizing biologically plausible unsupervised learning algorithms\u201d and a welcoming remark.\nProfessor Terrence Sejnowski of the University of California San Diego in the United States, who founded NeurIPS, the most prestigious AI conference in the world, will speak about the intelligence of hyper-scale language models based on experimental studies examining the existence of intelligence in hyper-scale language models.\nIn all, professor Seungwon Hwang of Seoul National University will talk about how causality, evidentiality, and other types of information can be used to strengthen hyper-scale language models. Participants will also have the chance to view several demos and research posters created by the Global AI Center at the booth, where they may speak with the researchers in person.<|endoftext|>"
    },
    {
        "entity_pair": "Minjoon Seo & AI",
        "step": 25213,
        "passage": " to favor the true solution. The model sometimes gives the wrong prediction\u2014for example, at t=16k, and changes its prediction from the true solution to the wrong solution, \u201837-36\u2019\u2014but again changes its prediction to be a true solution afterward. In addition, its intermediate wrong solution, \u201837-36\u2019 indicates the model was confused with distinguishing the longest field goal of Rob Bironas (40 vs. 37), which is an understandable mistake.\nWe also compare the predictions from the model with our method to those from the model with MML, which is shown in Appendix C.\nQuality of the predicted solution.\nWe analyze if the model outputs the correct solution, since the solution executing the correct answer could be spurious. First, on NarrativeQA and DROPnum, we manually analyze 50 samples from the development set and find that 98% and 92% of correct cases produce the correct solution respectively. Next, on WikiSQL, we compare the predictions from the model to the annotated SQL queries on the development set. This is possible because gold SQL queries are available in the dataset for the full supervision. Out of 8,421 examples, 7,110 predictions execute the correct answers. Among those, 88.5% of the predictions are exactly same as the annotated queries. Others are the cases where (i) both queries are correct, (ii) the model prediction is correct but the annotated query is incorrect, and (iii) the annotated query is correct and the model prediction is spurious. We show a full analysis in Appendix C.\nRobustness to the noise in |Z|.\nSometimes noise arises during the construction of |Z|, such as |Z| constructed based on ROUGE-L for NarrativeQA. To explore the effect of noise in Z, we experiment with more noisy solution set by picking all the spans with scores that is equal to or larger than the 5th highest. The new construction method increases |Z| from 4.3 to 7.1 on NarrativeQA. The result by MML objective drops significantly (56.07\u219251.14) while the result by ours drops marginally (58.77\u219257.97), suggesting that MML suffers more with a noisier Z while ours is more robust.\nIn this paper, we demonstrated that, for many QA tasks which only provide the answer text as supervision, it is possible to precompute a discrete set of possible solutions that contains one correct option. Then, we introduced a discrete latent variable learning algorithm which iterates a procedure of predicting the most likely solution in the precomputed set and further increasing the likelihood of that solution. We showed that this approach significantly outperforms previous approaches on six QA tasks including reading comprehension, open-domain QA, discrete reasoning task and semantic parsing, achieving absolute gains of 2\u201310% and setting the new state-of-the-art on five well-studied datasets.\nThis research was supported by ONR (N00014-18-1-2826, N00014-17-S-B001), DARPA N66001-19-2-403, NSF (IIS-1616112, IIS-1252835, IIS-1562364), ARO (W911NF-16-1-0121), an Allen Distinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google and Amazon.\nThe authors would like to thank the anonymous reviewers, Eunsol Choi, Christopher Clark, Victor Zhong and UW NLP members for their valuable feedback.\nAbolafia et al. (2018) Daniel A Abolafia, Mohammad Norouzi, Jonathan Shen, Rui Zhao, and Quoc V Le. 2018. Neural program synthesis with priority queue training. arXiv preprint arXiv:1801.03526.\nAgarwal et al. (2019) Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. 2019. Learning to generalize from sparse and underspecified rewards. In ICML.\nAlberti et al. (2019) Chris Alberti, Kenton Lee, and Michael Collins. 2019. A BERT baseline for the Natural Questions. arXiv preprint arXiv:1901.08634.\nArtzi and Zettlemoyer (2013) Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. In ACL.\nBerant et al. (2013) Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In EMNLP.\nChen et al. (2017) Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open-domain questions. In ACL.\nClarke et al. (2010) James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world\u2019s response. In CoNLL.\nDong and Lapata (2018) Li Dong and Mirella Lapata. 2018. Coarse-to-fine decoding for neural semantic parsing. In ACL.\nDua et al. (2019) Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL.\nHe et al. (2019) Pengcheng He, Yi Mao, Kaushik Chakrabarti, and Weizhu Chen. 2019. X-SQL: reinforce schema representation with context. arXiv preprint arXiv:1908.08113.\nHurley and Rickard (2009) Niall Hurley and Scott Rickard. 2009. Comparing measures of sparsity. IEEE Transactions on Information Theory.\nHwang et al. (2019) Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and Minjoon Seo. 2019. A comprehensive exploration on WikiSQL with table-aware word contextualization. arXiv preprint arXiv:1902.01069.\nIyyer et al. (2017) Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In ACL.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In ACL.\nKadlec et al. (2016) Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. 2016. Text understanding with the attention sum reader network. In ACL.\nKo\u010disk\u1ef3 et al. (2018) Tom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. TACL.\nKrishnamurthy et al. (2017) Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. 2017. Neural semantic parsing with type constraints for semi-structured tables. In EMNLP.\nKwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Change, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. TACL.\nLee et al. (2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In ACL.\nLiang et al. (2017) Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. 2017.\nLiang et al. (2018) Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V Le, and Ni Lao. 2018. Memory augmented policy optimization for program synthesis and semantic parsing. In NIPS.\nLin (2004) Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. Text Summarization Branches Out.\nMin et al. (2019) Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. Compositional questions do not necessitate multi-hop reasoning. In ACL.\nNishida et al. (2019) Kyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda, Atsushi Otsuka, Hisako Asano, and Junji Tomita. 2019. Multi-style generative reading comprehension. In ACL.\nPaszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017.\nAutomatic differentiation in PyTorch.\nRajpur<|endoftext|>"
    },
    {
        "entity_pair": "Minjoon Seo & AI",
        "step": 25845,
        "passage": "\u805a\u5408\u51fd\u6570\uff1b\uff083\uff09Model_opval\uff1a\u9884\u6d4b\u8fd0\u7b97\u7b26\u3001\u6761\u4ef6\u7684\u53d6\u503c\u3002\n\n![image.png](img/1584454149724-bb5805e4-ccc8-4839-ae3b-88b57bc114f9.png)\n\n  i. \u4e09\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u52a0\u6743\u95ee\u9898\u548c\u7c7b\u578b\u8868\u793a\u5f62\u5f0fHQT / COL\uff08SQLNet\u63d0\u51fa\uff09\u3002\n\n![image.png](img/1584455373728-c1827b63-14e7-4077-9204-ce4b3ae1cef8.png)\n\n  i.  MODEL_COL\u6a21\u5757\n\n\u6a21\u5757\u9700\u8981\u9884\u6d4b\u4e09\u90e8\u5206\uff1a`SELECT_COL`,\u200b `COND_num`\u548c`COND_COL` \u8fd9\u4e09\u4e2a\u90e8\u5206\u6709\u4e2a\u7279\u70b9\u90fd\u662f\u8981\u7ed3\u5408question\u548c\u8868(\u6240\u6709\u5217\u540d)\u7684\u7279\u5f81\u3002\n\n\u9884\u6d4b\u200b `SELECT_COL`: \n\n\u200b                   ![image.png](img/1584456260577-501a28ab-4990-4209-860a-5da2039b6f18.png)\n\n\u9884\u6d4b `COND_num`\uff1a\n\n![image.png](img/1584456306256-937e9b2a-2df5-4eae-8f16-615e09cb72bb.png)\n\n\u9884\u6d4b `COND_COL`\uff08\u8fd9\u91cc\u5728SQLNet\u4e2d\u53d1\u73b0\uff0cSELECT_COL\u548cCOND_COL\u5e38\u5e38\u4f1a\u9884\u6d4b\u76f8\u540c\u7684\u5217\u540d\u3002\u6240\u4ee5\u8fd9\u91cc\u4f5c\u51fa\u4e86\u6539\u53d8\uff0c\u5728\u91cc\u9762\u516c\u5f0f\u4e2d\u65b0\u589e\u4e86\u4e00\u4e2a\u5173\u4e8eSELECT_COL\u5217\u540d\u7684\u9879.\uff09\uff1a\n\n![image.png](img/1584456364854-ed3d9289-edd0-448a-a19f-32c7a5b91de5.png)\n\n  i.  MODEL_AGG\u6a21\u5757\n\n\u8fd9\u90e8\u5206\u4e0eSQLNet\u4e00\u6837\uff0c\u90fd\u662f\u9884\u6d4b{NULL,MAX,MIN,COUNT,SUM,AVG}\u4e2d\u7684\u4e00\u79cd\u3002\n\n![image.png](img/1584456447315-7c530eb3-78cf-4c90-92c8-7e798ea0bd28.png)\n\n  i.  MODEL_OPVAL\u6a21\u5757\n\n\u9700\u8981\u9884\u6d4b\u4e24\u90e8\u5206\uff1a$OP\uff1b$COND_VAL\u3002\n\nOP\uff1a\u9884\u6d4b{=,>,<}\u4e2d\u7684\u4e00\u79cd\u3002\n\n![image.png](img/1584456573465-67624b50-08d5-438a-bdc9-f639b55a2b06.png)\n\nCOND_VAL\uff08\u501f\u9274\u4e86Pointernetwork\u7684\u601d\u60f3\u6765\u4ece\u8f93\u5165\u7684Question\u4e2d\u8003\u8651Value\u91cc\u7684\u503c\u3002\uff09\uff1a\n\n![image.png](img/1584456633425-dda329bb-494c-46bc-a5a5-5036eb880112.png)\n\n\u5176\u4e2dh\u662f\u524d\u4e00\u4e2a\u751f\u6210\u7684\u5355\u8bcd\u7684\u9690\u72b6\u6001\u3002\n\n## 5. \u6570\u636e\u96c6\nwikiSQL\n## 6. \u5b9e\u9a8c\u7ed3\u679c\nTypeSQL\u8fbe\u5230\u4e8682.6%\u7684\u6b63\u786e\u7387\n\n## 7. \u672a\u6765\u5de5\u4f5c\n\u5c06\u6765\uff0c\u6211\u4eec\u8ba1\u5212\u901a\u8fc7\u5728\u6570\u636e\u5e93\u62c6\u5206\u8bbe\u7f6e\u4e0b\u63a2\u7d22\u5176\u4ed6\u66f4\u590d\u6742\u7684\u6570\u636e\u96c6\u6765\u63a8\u8fdb\u8fd9\u9879\u5de5\u4f5c\u3002 \u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u6211\u4eec\u53ef\u4ee5\u5728\u66f4\u73b0\u5b9e\u7684\u6587\u672c\u5230SQL\u4efb\u52a1\uff08\u5305\u62ec\u8bb8\u591a\u590d\u6742\u7684SQL\u548c\u4e0d\u540c\u7684\u6570\u636e\u5e93\uff09\u4e0a\u7814\u7a76\u901a\u7528\u6a21\u578b\u7684\u6027\u80fd\u3002\n\n## 8. \u539f\u6587\u53ca\u6e90\u7801\n[\u539f\u6587](https://arxiv.org/abs/1804.09769)\n\n[\u6e90\u7801](https://github.com/taoyds/typesql)\n\n\n\n# A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization\n\n## 1. \u4f5c\u8005\u7b80\u4ecb\n> Wonseok Hwang Jinyeong Yim Seunghyun Park Minjoon Seo\n> Clova AI, NAVER Corp.\n\n## 2. \u6587\u732e\u7c7b\u578b\nKR2ML Workshop at NeurIPS 2019 \n## 3. \u4e3b\u8981\u8d21\u732e\n\n\n\n## 4. \u65b9\u6cd5\u6982\u8ff0\n\n\n## 5. \u5b9e\u9a8c\n\n\n## 6. \u539f\u6587\u53ca\u6e90\u7801\n[\u539f\u6587](https://arxiv.org/abs/1902.01069)\n\n[\u6e90\u7801](https://github.com/naver/sqlova)\n\n\n\n\n\n# \u4e2d\u6587NL2SQL\u6311\u6218\u8d5b\n\n- \u9996\u5c4a\u4e2d\u6587NL2SQL\u6311\u6218\u8d5b\u7b2c3\u540d\u65b9\u6848+\u4ee3\u7801 by beader GitHub\uff1a\n\n  https://github.com/beader/tianchi_nl2sql\n\n- \u5929\u6c60NL2SQL\u6311\u6218\u8d5b\u51a0\u519b\u65b9\u6848 by NUDT NLP GitHub\uff1a\n\n  https://github.com/nudtnlp/tianchi-nl2sql-top1\n\n\n\n# \u3010\u667a\u80fd\u95ee\u7b54\u3011\u4ece\u5165\u95e8\u5230\u653e\u5f03\u2014\u2014DBQA\u4e0e\u673a\u5668\u9605\u8bfb\u7406\u89e3\n\n## DBQA\u4e0eMRC\u5165\u95e8\n\n- - Abstract\n\n  - Task of MRC\n\n  - History of MRC\n\n  - Dataset of MRC\n\n  - - SQuAD\n    - DuReader\n    - DuReader_robust\n    - CMRC 2018\n    - DRCD\n\n  - Models of MRC\n\n  - - Stanford Attentive Reader\n    - BiDAF\n    - Others\n\n  - Metric of MRC\n\n  - Product in Clould\n\n  - Conclusion\n\n  - Reference\n\n> DBQA\u662fdocument-based Question and Answer \u7684\u7b80\u79f0\uff0cMRC\u662fMachine Reading Comprehension\u673a\u5668\u9605\u8bfb\u7406\u89e3\u7684\u7b80\u79f0\u3002\n> \u6309\u7167\u4e2a\u4eba\u6d45\u8584\u7684\u7406\u89e3\uff0c**DBQA\u66f4\u52a0\u504f\u5411\u4e8e\u7cfb\u7edf**\uff0c\u4e1a\u52a1\u573a\u666f\u4f1a\u6bd4\u76ee\u524d\u7684\u9605\u8bfb\u7406\u89e3\u66f4\u590d\u6742\uff0c**MRC\u5219\u6bd4\u8f83\u504f\u4e8e\u4efb\u52a1\u672c\u8eab**\uff0c\u7528\u4e8e\u6307\u5b9a\u5185\u5bb9\uff08\u6587\u6863\uff09\u4e2d\u627e\u7b54\u6848\uff08\u53ef\u4ee5\u62d2\u8bc6\uff09\u3002\u5982\u679c\u7b80\u5355\u7684\u4ece\u6587\u6863\u4e2d\u62bd\u53d6\u51fa\u90e8\u5206\u5185\u5bb9\u4f5c\u4e3a\u56de\u7b54\u8fd9\u4e2a\u4efb\u52a1\u89d2\u5ea6\u4e0a\uff0c\u53ef\u4ee5\u7b80\u5355\u7684\u628a\u4e24\u8005\u7b49\u4ef7\u5316\uff0c\u6216\u8005\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u9605\u8bfb\u7406\u89e3\u7684\u601d\u8def\u53bb\u5b9e\u73b0DBQA\u3002\u56e0\u6b64\uff0c\u867d\u7136\u672c\u6587\u4e3b\u8981\u6574\u7406\u7684\u662fMRC\u4e5f\u5f52\u7c7b\u5728\u95ee\u7b54\u4e2d\u4e86\uff08\u5927\u90e8\u5206\u53c2\u8003\u65af\u5766\u798f\u8bfe\u7a0bcs224n\uff09\n> PS\uff1a\u4fa7\u91cd\u4e8e\u4e2d\u6587MRC\uff0c\u6d89\u53ca\u8bba\u6587\u8f83\u5c11\uff08\u5f85\u540e\u7eed\u8865\u5145\uff09\n\n### Abstract\n\n\u673a\u5668\u9605\u8bfb\u7406\u89e3\u57fa\u7840\u4efb\u52a1\u662f\u6839\u636e\u95ee\u9898\uff08`Question`\uff09\uff0c\u5728\u975e\u7ed3\u6784\u5316\u6587\u6863\uff08`Passege`\uff09\u5bfb\u627e\u5408\u9002\u7684\u7b54\u6848\uff08`Answer`\uff09.\n\n`MCTestReading`\n\n![img](img/1.webp)\n\n\u901a\u5e38\u6765\u8bb2\uff0c\u57fa\u4e8e\u6587\u6863\u7684\u641c\u7d22\uff08\u53ef\u4ee5\u662fQA\uff09\u53ef\u4ee5\u5206\u4e3a2\u4e2a\u90e8\u5206\u3002\n\n1. \u4eceQuery\u4e2d\u8bc6\u522b\u7279\u5f81\uff0c\u53ec\u56de\u76f8\u5173\u6587\u6863\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4e00\u822c\u6210\u4e3a\uff08information retrieval\uff09\uff0c\u53ef\u4ee5\u901a\u8fc7\u4f20\u7edf\u7684\u4fe1\u606f\u68c0\u7d22/web\u641c\u7d22\u5904\u7406\uff08tf-idf\uff0cBM25\uff0cetc...\uff09\n2. \u4ece\u6587\u6863\u4e2d\u68c0\u7d22\u51fa\u6211\u4eec\u60f3\u8981\u7684\u7b54\u6848\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u5373MRC\u3002\n\n### Task of MRC\n\n\u603b\u7684\u6765\u770bMRC\u4efb\u52a1\u5927\u81f4\u53ef\u4ee5\u5206\u4e3a\u56db\u79cd\u7c7b\u578b\n\n- **\u5b8c\u5f62\u586b\u7a7a**\n  \u8be5\u4efb\u52a1\u76f8\u5bf9\u6bd4\u8f83\u65e9\uff0c\u6bd4\u8f83\u6709\u4ee3\u8868\u6027\u7684\u6709The Children\u2019s Book Test\uff0cCMRC2017\u7b49\u3002\n- **\u591a\u9879\u9009\u62e9/\u591a\u9879\u9009\u62e9\u5f0f**\n  \u6570\u636e\u96c6\u4e3a\uff08\u6587\u6863\uff0c\u95ee\u9898\uff0c\u5019\u9009\u7b54\u6848\u96c6\uff0c\ufffd<|endoftext|>"
    },
    {
        "entity_pair": "Minjoon Seo & AI",
        "step": 26983,
        "passage": " two for backward) of Skim-LSTM+Attention model. We see that the second layer skims more, implying that the second layer is more confident about which tokens are important.\nFigure 6 shows F1 score of LSTM+Attention model using standard LSTM and Skim LSTM, sorted in ascending order by Flop-R. While models tend to perform better with larger computational cost, Skim LSTM (Red) outperforms standard LSTM (Blue) with comparable computational cost. We also observe that the F1 score of Skim-LSTM is more stable across different configurations and computational cost. Moreover, increasing the value of \u03b3 for Skim-LSTM gradually increases skipping rate and Flop-R, while it also leads to reduced accuracy.\nControlling skim rate. An important advantage of Skim-RNN is that the skim rate (and thus computational cost) can be dynamically controlled at inference time by adjusting the threshold for \u2018skim\u2019 decision probability p1t (Equation 1). Figure 6 shows the trade-off between the accuracy and computational cost for two settings, confirming the importance of skimming (d\u2032>0) compared to skipping (d\u2032=0).\nVisualization. Figure 7 shows an example from SQuAD and visualizes which words Skim-LSTM (d=100,d\u2032=20) reads (red) and skims (white). As expected, the model does not skim when the input seems to be relevant to answering the question. In addition, LSTM in second layer skims more than that in the first layer mainly because the second layer is more confident about the importance of each token, as shown in Figure 7. More visualizations are shown in in Appendix C.\nd=100 (batch size = 1) in all three frameworks on a single thread of CPU (averaged over 100 trials), and have observed that NumPy is 1.5 and 2.8 times faster than TensorFlow and PyTorch.888NumPy\u2019s speed becomes similar to that of TensorFlow and PyTorch at d=220 and d=700, respectively. At larger hidden size, NumPy becomes slower. This seems to be mostly due to the fact that the frameworks are primarily (optimized) for GPUs and they have larger overhead than NumPy that they cannot take much advantage of reducing the size of the hidden state of the LSTM below 100.\nFigure 8: Speed up rate of Skim-LSTM (vs LSTM) with varying skimming rates and hidden state sizes.\nFigure 8 shows the relative speed gain of Skim-LSTM compared to standard LSTM with varying hidden state size and skim rate. We use NumPy, and the inferences are run on a single thread of CPU. We also plot the ratio between the reduction of the number of float operations (Flop-R) of LSTM and Skim-LSTM. This can be considered as a theoretical upper bound of the speed gain on CPUs. We note two important observations. First, there is an inevitable gap between the actual gain (solid line) and the theoretical gain (dotted line). This gap will be larger with more overhead of the framework, or more parallelization (e.g. multithreading). Second, the gap decreases as the hidden state size increases because the the overhead becomes negligible with very large matrix operations. Hence, the benefit of Skim-RNN will be greater for larger hidden state size.\nLatency. A modern GPU has much higher throughput than a CPU with parallel processing. However, for small networks, the CPU often has lower latency than the GPU. Comparing between NumPy with CPU and TensorFlow with GPU (Titan X), we observe that the former has 1.5 times lower latency (75 \\upmus vs 110 \\upmus per token) for LSTM of d=100. This means that combining Skim-RNN with CPU-based framework can lead to substantially lower latency than GPUs. For instance, Skim-RNN with CPU on IMDb has 4.5x lower latency than a GPU, requiring only 29 \\upmus per token on average.\nWe present Skim-RNN, a recurrent neural network that can dynamically decide to use the big RNN (read) or the small RNN (skim) at each time step, depending on the importance of the input. While Skim-RNN has significantly lower computational cost than its RNN counterpart, the accuracy of Skim-RNN is still on par with or better than standard RNNs, LSTM-Jump, and VCRNN. Since Skim-RNN has the same input and output interface as an RNN, it can easily replace RNNs in existing applications. We also show that a Skim-RNN can offer better latency results on a CPU compared to a standard RNN on a GPU. Future work involves using Skim-RNN for applications that require much higher hidden state size, such as video understanding, and using multiple small RNN cells for varying degrees of skimming.\nThis research was supported by the NSF (IIS 1616112), Allen Distinguished Investigator Award, Samsung GRO award, and gifts from Google, Amazon, Allen Institute for AI, and Bloomberg. We thank the anonymous reviewers for their helpful comments.\nBalduzzi & Ghifary (2016) David Balduzzi and Muhammad Ghifary. Strongly-typed recurrent neural networks. In ICML, 2016.\nCampos et al. (2017) V\u00edctor Campos, Brendan Jou, Xavier Gir\u00f3-i Nieto, Jordi Torres, and Shih-Fu Chang. Skip rnn: Learning to skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834, 2017.\nChoi et al. (2017) Eunsol Choi, Daniel Hewlett, Alexandre Lacoste, Illia Polosukhin, Jakob Uszkoreit, and Jonathan Berant. Coarse-to-fine question answering for long documents. In ACL, 2017.\nChung et al. (2017) Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In ICLR, 2017.\nDyer et al. (2016) Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network grammars. In NAACL, 2016.\nHahn & Keller (2016) Michael Hahn and Frank Keller. Modeling human reading with neural attention. In EMNLP, 2016.\nJang et al. (2017) Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017.\nJernite et al. (2017) Yacine Jernite, Edouard Grave, Armand Joulin, and Tomas Mikolov. Variable computation in recurrent neural networks. In ICLR, 2017.\nJohansen et al. (2017) Alexander Johansen, Bryan McCann, James Bradbury, and Richard Socher. Learning when to read and when to skim, 2017. URL https://metamind.io/research/learning-when-to-skim-and-when-to-read.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL, 2017.\nKembhavi et al. (2017) Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In CVPR, 2017.\nKokkinos & Potamianos (2017) Filippos Kokkinos and Alexandros Potamianos. Structural attention neural networks for improved sentiment analysis. arXiv preprint arXiv:1701.01811, 2017.\nKong et al. (2016) Lingpeng Kong, Chris Dyer, and Noah A Smith. Segmental recurrent neural networks. In ICLR, 2016.\nMarcel Adam Just (1987) Patricia Anderson Carpenter Marcel Adam Just. The Psychology of Reading and Language Comprehension. 1987.\nMikolov et al. (2015) Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc\u2019Aurelio Ranzato. Learning longer memory in recurrent neural networks. In ICLR Workshop, 2015.\nMin et al. (2017) Sewon Min, Minjoon Seo, and Hannaneh Hajishirzi.\nQuestion answering through transfer learning from large fine-grained supervision data.\nMiyato et al. (2017) Takeru Miyato, Andrew M. Dai, and Ian Goodfellow. Adversarial training methods for semi-supervised text classification. In ICLR, 2017.\nMnih et al. (2014) Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In NIPS, 2014.\nOdena et al. (2017) Augustus Odena, Dieterich Lawson, and Christopher Olah.\nChanging model behavior at test-time using reinforcement learning.\nIn ICLR Workshop, 2017.\nRastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon<|endoftext|>"
    },
    {
        "entity_pair": "Minjoon Seo & AI",
        "step": 27048,
        "passage": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, Kevin Clark, et al., ICLR, 2020.\nTinyBERT: Distilling BERT for Natural Language Understanding, Xiaoqi Jiao, et al., ICLR, 2020.\nMINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers, Wenhui Wang, et al., arXiv, 2020.\nT5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, Colin Raffel, et al., arXiv preprint, 2019.\nERNIE: Enhanced Language Representation with Informative Entities, Zhengyan Zhang, et al., ACL, 2019.\nXLNet: Generalized Autoregressive Pretraining for Language Understanding, Zhilin Yang, et al., arXiv preprint, 2019.\nALBERT: A Lite BERT for Self-supervised Learning of Language Representations, Zhenzhong Lan, et al., arXiv preprint, 2019.\nRoBERTa: A Robustly Optimized BERT Pretraining Approach, Yinhan Liu, et al., arXiv preprint, 2019.\nDistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, Victor sanh, et al., arXiv, 2019.\nSpanBERT: Improving Pre-training by Representing and Predicting Spans, Mandar Joshi, et al., TACL, 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, et al., NAACL 2019, 2018.\nTANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection, Siddhant Garg, et al., AAAI 2020, Nov 2019.\nOverview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering, Asma Ben Abacha, et al., ACL-W 2019, Aug 2019.\nTowards Scalable and Reliable Capsule Networks for Challenging NLP Applications, Wei Zhao, et al., ACL 2019, Jun 2019.\nCognitive Graph for Multi-Hop Reading Comprehension at Scale, Ming Ding, et al., ACL 2019, Jun 2019.\nReal-Time Open-Domain Question Answering with Dense-Sparse Phrase Index, Minjoon Seo, et al., ACL 2019, Jun 2019.\nUnsupervised Question Answering by Cloze Translation, Patrick Lewis, et al., ACL 2019, Jun 2019.\nSemEval-2019 Task 10: Math Question Answering, Mark Hopkins, et al., ACL-W 2019, Jun 2019.\nImproving Question Answering over Incomplete KBs with Knowledge-Aware Reader, Wenhan Xiong, et al., ACL 2019, May 2019.\nMatching Article Pairs with Graphical Decomposition and Convolutions, Bang Liu, et al., ACL 2019, May 2019.\nEpisodic Memory Reader: Learning what to Remember for Question Answering from Streaming Data, Moonsu Han, et al., ACL 2019, Mar 2019.\nNatural Questions: a Benchmark for Question Answering Research, Tom Kwiatkowski, et al., TACL 2019, Jan 2019.\nTextbook Question<|endoftext|>"
    }
]