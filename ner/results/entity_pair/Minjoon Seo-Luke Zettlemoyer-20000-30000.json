[
    {
        "entity_pair": "Minjoon Seo & Luke Zettlemoyer",
        "step": 21295,
        "passage": " negative spans can be better divided with the correct answer \u201cJerusalem\u201d. This shows that SCL in our KECP framework is reliable and can improve the performance for EQA.\nThe Accuracy of Answer Generation. A major difference between previous works and ours is that we model the EQA task as text generation. Intuitively, if the model correctly generates the first answer token, it is easy to generate the remaining answer tokens because of the very small search space. Therefore, we analyze how difficult it is for the model to generate the first token correctly. Specifically, we check whether the generated first token and the first token of the ground truth are within a fixed window size nw. As shown in Table 5, we find the accuracy of our method is lower than RoBERTa-base Liu et al. (2019) when nw=1. Yet, we achieve the best performance when increasing the window size nw to 5. We think that our KECP can generate some rehabilitation text for the answer. For example in Figure 4, the PLM may generate \u201cthe conquest of Jerusalem\u201d rather than the correct answer with single token \u201cJerusalem\u201d. This phenomenon reflects the reason why we achieve lower accuracy when nw=1. But, we think that the generated results are still in the vicinity of the correct answer.\nTable 5: The accuracy of predicting the first [MASK] in the query prompt with full training samples for each task. #nw denotes the window size.\nTo bridge the gap between the pre-training and fine-tuning objectives, KECP views EQA as an answer generation task. In KECP, the knowledge-aware prompt encoder injects external domain-related knowledge into the passage, and then enhances the representations of selected prompt tokens in the query. The span-level contrastive learning objective is proposed to improve the performance of EQA. Experiments on multiple benchmarks show that our framework outperforms the state-of-the-art methods. In the future, we will i) further improve the performance of KECP by applying controllable text generation techniques, and ii) explore the prompt-tuning for other types of MRC tasks, such as cloze-style MRC and multiple-choice MRC.\nBrown et al. (2020) Tom B. Brown, Benjamin Mann, and etc. Nick Ryder. 2020. Language models are few-shot learners. In NeurIPS.\nChen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A simple framework for contrastive learning of visual representations. In ICML, volume 119, pages 1597\u20131607.\nDai et al. (2021) Damai Dai, Hua Zheng, Zhifang Sui, and Baobao Chang. 2021. Incorporating connections beyond knowledge embeddings: A plug-and-play module to enhance commonsense reasoning in machine reading comprehension. CoRR, abs/2103.14443.\nDettmers et al. (2018) Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018.\nConvolutional 2d knowledge graph embeddings.\nDunn et al. (2017) Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur G\u00fcney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q&a dataset augmented with context from a search engine. CoRR, abs/1704.05179.\nFisch et al. (2019) Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In EMNLP, pages 1\u201313.\nGao et al. (2021) Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In ACL, pages 3816\u20133830.\nHan et al. (2021) Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2021. PTR: prompt tuning with rules for text classification. CoRR, abs/2105.11259.\nJoshi et al. (2020) Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predicting spans. TACL, 64\u201377.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL, pages 1601\u20131611.\nKwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, and et al. 2019. Natural questions: a benchmark for question answering research. TACL.\nLai et al. (2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard H. Hovy. 2017. RACE: large-scale reading comprehension dataset from examinations. In EMNLP, pages 785\u2013794.\nLevy et al. (2017) Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In CoNLL, pages 333\u2013342.\nLi and Liang (2021a) Xiang Lisa Li and Percy Liang. 2021a. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP, pages 4582\u20134597. Association for Computational Linguistics.\nLi and Liang (2021b) Xiang Lisa Li and Percy Liang. 2021b. Prefix-tuning: Optimizing continuous prompts for generation. In ACL, pages 4582\u20134597.\nLiu et al. (2021a) Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2021a. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. CoRR.\nLiu et al. (2021b) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT understands, too. CoRR, abs/2103.10385.\nLiu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, and et al. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR.\nQin and Eisner (2021) Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. In NAACL-HLT, pages 5203\u20135212.\nRajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable questions for squad. CoRR, abs/1806.03822.\nRajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. CoRR.\nRam et al. (2021) Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, and Omer Levy. 2021. Few-shot question answering by pretraining span selection. In ACL.\nSchick and Sch\u00fctze (2021) Timo Schick and Hinrich Sch\u00fctze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In EACL, pages 255\u2013269.\nShin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP.\nTrischler et al. (2017) Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017. NewsQA: A machine comprehension dataset. In WRLNLP, pages 191\u2013200.\nVan der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne.\nVinyals et al. (2015) Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In NIPS, pages 2692\u20132700.\nWang and Jiang (2019) Chao Wang and Hui Jiang. 2019. Explicit utilization of general knowledge in machine reading comprehension. In ACL, pages 2263\u20132272. Association for Computational Linguistics.\nWang et al. (2022) Chengyu Wang, Minghui Qiu, Taolin Zhang, Tingting Liu, Lei Li, Jianing Wang, Ming Wang, Jun<|endoftext|>"
    },
    {
        "entity_pair": "Minjoon Seo & Luke Zettlemoyer",
        "step": 22835,
        "passage": " Aware Neural Machine Translation\nKehai Chen, Rui Wang, Masao Utiyama and Eiichiro Sumita\n\nContextual Embeddings: When Are They Worth It?\nSimran Arora, Avner May, Jian Zhang and Christopher R\u00e9\n\nContextual Neural Machine Translation Improves Translation of Cataphoric Pronouns\nKayYen Wong, Sameen Maruf and Gholamreza Haffari\n\nContextualized Sparse Representations for Real-Time Open-Domain Question Answering\nJinhyuk Lee, Minjoon Seo, Hannaneh Hajishirzi and Jaewoo Kang\n\nContextualizing Hate Speech Classifiers with Post-hoc Explanation\nBrendan Kennedy, Xisen Jin, Aida Mostafazadeh Davani, Morteza Dehghani and Xiang Ren\n\nContrastive Self-Supervised Learning for Commonsense Reasoning\nTassilo Klein and Moin Nabi\n\nControlled Crowdsourcing for High-Quality QA-SRL Annotation\nPaul Roit, Ayal Klein, Daniela Stepanov, Jonathan Mamou, Julian Michael, Gabriel Stanovsky, Luke Zettlemoyer and Ido Dagan\n\nConversational Word Embedding for Retrieval-Based Dialog System\nWentao Ma, Yiming Cui, Ting Liu, Dong Wang, Shijin Wang and Guoping Hu\n\nCrawling and Preprocessing Mailing Lists At Scale for Dialog Analysis\nJanek Bevendorff, Khalid Al Khatib, Martin Potthast and Benno Stein\n\nCrossing Variational Autoencoders for Answer Retrieval\nWenhao Yu, Lingfei Wu, Qingkai Zeng, Shu Tao, Yu Deng and Meng Jiang\n\nDeeBERT: Dynamic Early Exiting for Accelerating BERT Inference\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu and Jimmy Lin\n\nDesigning Precise and Robust Dialogue Response Evaluators\nTianyu Zhao, Divesh Lala and Tatsuya Kawahara\n\nDialogue State Tracking with Explicit Slot Connection Modeling\nYawen Ouyang, Moxin Chen, Xinyu Dai, Yinggong Zhao, Shujian Huang and Jiajun Chen\n\nDo Transformers Need Deep Long-Range Memory?\nJack Rae and Ali Razavi\n\nDo you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods\nNing Miao, Yuxuan Song, Hao Zhou and Lei Li\n\nDoes Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation\nBei Li, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu and Changliang Li\n\nDon\u2019t Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction\nZhenkai Wei, Yu Hong, Bowei Zou, Meng Cheng and Jianmin Yao\n\nDscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing\nJiangming Liu, Shay B. Cohen and Mirella Lapata\n\nDynamic Memory Induction Networks for Few-Shot Text Classification\nRuiying Geng, Binhua Li, Yongbin Li, Jian Sun and Xiaodan Zhu\n\nDynamic Sampling Strategies for Multi-Task Reading Comprehension\nAnanth Gottumukkala, Dheeru Dua, Sameer Singh and Matt Gardner\n\nDynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change\nHongfei Xu, Josef van Genabith, Deyi Xiong and Qiuhui Liu\n\nEfficient strategies for hierarchical text classification: external knowledge and auxiliary tasks\nKervy Rivas Rojas, Gina Bustamante, Arturo Oncevay and Marco Antonio Sobrevilla Cabezudo\n\nEmbarrassingly Simple Unsupervised Aspect Extraction\nSt\u00e9phan Tulkens and Andreas van Cranenburgh\n\nEnabling Language Models to Fill in the Blanks\nChris Donahue, Mina Lee and Percy Liang\n\nEncoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction\nMasahiro Kaneko, Masato Mita, Shun Kiyono, Jun Suzuki and Kentaro Inui\n\nENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation\nLifu Tu, Richard Yuanzhe Pang, Sam Wiseman and Kevin Gimpel\n\nEnhancing Machine Translation with Dependency-Aware Self-Attention\nEmanuele Bugliarello and Naoaki Okazaki\n\nEnhancing Pre-trained Chinese Character Representation with Word-aligned Attention\nYanzeng Li, Bowen Yu, Xue Mengge and Tingwen Liu\n\nEnriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing\nDaniel Fern\u00e1ndez-Gonz\u00e1lez and Carlos G\u00f3mez-Rodr\u00edguez\n\nEntity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification\nNianzu Ma, Sahisnu Mazumder, Hao Wang and Bing Liu\n\nEstimating Mutual Information Between Dense Word Embeddings\nVitalii Zhelezniak, Aleksandar Savkov and Nils Hammerla\n\nEvaluating Dialogue Generation Systems via Response Selection\nShiki Sato, Reina Akama, Hiroki Ouchi, Jun Suzuki and Kentaro Inui\n\nEvaluating Robustness to Input Perturbations for Neural Machine Translation\nXing Niu, Prashant Mathur, Georgiana Dinu and Yaser Al-Onaizan\n\nEvery Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks\nYufeng Zhang, Xueli Yu, Zeyu Cui, Shu Wu, Zhongzhen Wen and Liang Wang\n\nExpBERT: Representation Engineering with Natural Language Explanations\nShikhar Murty, Pang Wei Koh and Percy Liang\n\nExploiting Personal Characteristics of Debaters for Predicting Persuasiveness\nKhalid Al Khatib, Michael V\u00f6lske, Shahbaz Syed, Nikolay Kolyada and Benno Stein\n\nExploring Content Selection in Summarization of Novel Chapters\nFaisal Ladhak, Bryan Li, Yaser Al-Onaizan and Kathy McKeown\n\nFact-based Content Weighting for Evaluating Abstractive Summarisation\nXinnuo Xu, Ond\u0159ej Du\u0161ek, Jingyi Li, Verena Rieser and Ioannis Konstas\n\nFatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts\nAgostina Calabrese, Michele Bevilacqua and Roberto Navigli\n\nFew-Shot NLG with Pre-Trained Language Model\nZhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu and William Yang Wang\n\nFLAT: Chinese NER Using Flat-Lattice Transformer\nXiaonan Li, Hang Yan, Xipeng Qiu and Xuanjing Huang\n\nGAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples\nDanilo Croce, Giuseppe Castellucci and Roberto Basili\n\nGeometry-aware domain adaptation for unsupervised alignment of word embeddings\nPratik Jawanpuria, Mayank Meghwanshi and Bamdev Mishra\n\nGive Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?\nKobi Leins, Jey Han Lau and Timothy Baldwin\n\nGlyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs\nHong-You Chen, SZ-HAN YU and Shou-de Lin\n\nGPT-too: A language-model-first approach for AMR-to-text generation\nManuel Mager, Ram\u00f3n Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian and Salim Roukos\n\nHow Can We Accelerate Progress Towards Human-like Linguistic Generalization?\nTal Linzen\n\nHypernymy Detection for Low-Resource Languages via Meta Learning\nChanglong Yu, Jialong Han, Haisong Zhang and Wilfred Ng\n\nIdentifying Principals and Accessories in a Complex Case based on the Comprehension of Fact Description\nYakun Hu, Zhunchen Luo and Wenhan Chao\n\nImplicit Discourse Relation Classification: We Need to Talk about Evaluation\nNajoung Kim, Song Feng, Chulaka Gunasekara and Luis Lastras\n\nImproved Speech Representations with Multi-Target Autoregressive Predictive Coding\nYu-An Chung and James Glass\n\nImproving Entity Linking through Semantic Reinforced Entity Embeddings\nFeng Hou, Ruili Wang, Jun He and Yi Zhou\n\nImproving<|endoftext|>"
    },
    {
        "entity_pair": "Minjoon Seo & Luke Zettlemoyer",
        "step": 25213,
        "passage": " to favor the true solution. The model sometimes gives the wrong prediction\u2014for example, at t=16k, and changes its prediction from the true solution to the wrong solution, \u201837-36\u2019\u2014but again changes its prediction to be a true solution afterward. In addition, its intermediate wrong solution, \u201837-36\u2019 indicates the model was confused with distinguishing the longest field goal of Rob Bironas (40 vs. 37), which is an understandable mistake.\nWe also compare the predictions from the model with our method to those from the model with MML, which is shown in Appendix C.\nQuality of the predicted solution.\nWe analyze if the model outputs the correct solution, since the solution executing the correct answer could be spurious. First, on NarrativeQA and DROPnum, we manually analyze 50 samples from the development set and find that 98% and 92% of correct cases produce the correct solution respectively. Next, on WikiSQL, we compare the predictions from the model to the annotated SQL queries on the development set. This is possible because gold SQL queries are available in the dataset for the full supervision. Out of 8,421 examples, 7,110 predictions execute the correct answers. Among those, 88.5% of the predictions are exactly same as the annotated queries. Others are the cases where (i) both queries are correct, (ii) the model prediction is correct but the annotated query is incorrect, and (iii) the annotated query is correct and the model prediction is spurious. We show a full analysis in Appendix C.\nRobustness to the noise in |Z|.\nSometimes noise arises during the construction of |Z|, such as |Z| constructed based on ROUGE-L for NarrativeQA. To explore the effect of noise in Z, we experiment with more noisy solution set by picking all the spans with scores that is equal to or larger than the 5th highest. The new construction method increases |Z| from 4.3 to 7.1 on NarrativeQA. The result by MML objective drops significantly (56.07\u219251.14) while the result by ours drops marginally (58.77\u219257.97), suggesting that MML suffers more with a noisier Z while ours is more robust.\nIn this paper, we demonstrated that, for many QA tasks which only provide the answer text as supervision, it is possible to precompute a discrete set of possible solutions that contains one correct option. Then, we introduced a discrete latent variable learning algorithm which iterates a procedure of predicting the most likely solution in the precomputed set and further increasing the likelihood of that solution. We showed that this approach significantly outperforms previous approaches on six QA tasks including reading comprehension, open-domain QA, discrete reasoning task and semantic parsing, achieving absolute gains of 2\u201310% and setting the new state-of-the-art on five well-studied datasets.\nThis research was supported by ONR (N00014-18-1-2826, N00014-17-S-B001), DARPA N66001-19-2-403, NSF (IIS-1616112, IIS-1252835, IIS-1562364), ARO (W911NF-16-1-0121), an Allen Distinguished Investigator Award, Samsung GRO and gifts from Allen Institute for AI, Google and Amazon.\nThe authors would like to thank the anonymous reviewers, Eunsol Choi, Christopher Clark, Victor Zhong and UW NLP members for their valuable feedback.\nAbolafia et al. (2018) Daniel A Abolafia, Mohammad Norouzi, Jonathan Shen, Rui Zhao, and Quoc V Le. 2018. Neural program synthesis with priority queue training. arXiv preprint arXiv:1801.03526.\nAgarwal et al. (2019) Rishabh Agarwal, Chen Liang, Dale Schuurmans, and Mohammad Norouzi. 2019. Learning to generalize from sparse and underspecified rewards. In ICML.\nAlberti et al. (2019) Chris Alberti, Kenton Lee, and Michael Collins. 2019. A BERT baseline for the Natural Questions. arXiv preprint arXiv:1901.08634.\nArtzi and Zettlemoyer (2013) Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. In ACL.\nBerant et al. (2013) Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In EMNLP.\nChen et al. (2017) Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open-domain questions. In ACL.\nClarke et al. (2010) James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world\u2019s response. In CoNLL.\nDong and Lapata (2018) Li Dong and Mirella Lapata. 2018. Coarse-to-fine decoding for neural semantic parsing. In ACL.\nDua et al. (2019) Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL.\nHe et al. (2019) Pengcheng He, Yi Mao, Kaushik Chakrabarti, and Weizhu Chen. 2019. X-SQL: reinforce schema representation with context. arXiv preprint arXiv:1908.08113.\nHurley and Rickard (2009) Niall Hurley and Scott Rickard. 2009. Comparing measures of sparsity. IEEE Transactions on Information Theory.\nHwang et al. (2019) Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and Minjoon Seo. 2019. A comprehensive exploration on WikiSQL with table-aware word contextualization. arXiv preprint arXiv:1902.01069.\nIyyer et al. (2017) Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In ACL.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In ACL.\nKadlec et al. (2016) Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. 2016. Text understanding with the attention sum reader network. In ACL.\nKo\u010disk\u1ef3 et al. (2018) Tom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. TACL.\nKrishnamurthy et al. (2017) Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gardner. 2017. Neural semantic parsing with type constraints for semi-structured tables. In EMNLP.\nKwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Change, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. TACL.\nLee et al. (2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In ACL.\nLiang et al. (2017) Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. 2017.\nLiang et al. (2018) Chen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V Le, and Ni Lao. 2018. Memory augmented policy optimization for program synthesis and semantic parsing. In NIPS.\nLin (2004) Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. Text Summarization Branches Out.\nMin et al. (2019) Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. Compositional questions do not necessitate multi-hop reasoning. In ACL.\nNishida et al. (2019) Kyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazutoshi Shinoda, Atsushi Otsuka, Hisako Asano, and Junji Tomita. 2019. Multi-style generative reading comprehension. In ACL.\nPaszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017.\nAutomatic differentiation in PyTorch.\nRajpur<|endoftext|>"
    },
    {
        "entity_pair": "Minjoon Seo & Luke Zettlemoyer",
        "step": 26983,
        "passage": " two for backward) of Skim-LSTM+Attention model. We see that the second layer skims more, implying that the second layer is more confident about which tokens are important.\nFigure 6 shows F1 score of LSTM+Attention model using standard LSTM and Skim LSTM, sorted in ascending order by Flop-R. While models tend to perform better with larger computational cost, Skim LSTM (Red) outperforms standard LSTM (Blue) with comparable computational cost. We also observe that the F1 score of Skim-LSTM is more stable across different configurations and computational cost. Moreover, increasing the value of \u03b3 for Skim-LSTM gradually increases skipping rate and Flop-R, while it also leads to reduced accuracy.\nControlling skim rate. An important advantage of Skim-RNN is that the skim rate (and thus computational cost) can be dynamically controlled at inference time by adjusting the threshold for \u2018skim\u2019 decision probability p1t (Equation 1). Figure 6 shows the trade-off between the accuracy and computational cost for two settings, confirming the importance of skimming (d\u2032>0) compared to skipping (d\u2032=0).\nVisualization. Figure 7 shows an example from SQuAD and visualizes which words Skim-LSTM (d=100,d\u2032=20) reads (red) and skims (white). As expected, the model does not skim when the input seems to be relevant to answering the question. In addition, LSTM in second layer skims more than that in the first layer mainly because the second layer is more confident about the importance of each token, as shown in Figure 7. More visualizations are shown in in Appendix C.\nd=100 (batch size = 1) in all three frameworks on a single thread of CPU (averaged over 100 trials), and have observed that NumPy is 1.5 and 2.8 times faster than TensorFlow and PyTorch.888NumPy\u2019s speed becomes similar to that of TensorFlow and PyTorch at d=220 and d=700, respectively. At larger hidden size, NumPy becomes slower. This seems to be mostly due to the fact that the frameworks are primarily (optimized) for GPUs and they have larger overhead than NumPy that they cannot take much advantage of reducing the size of the hidden state of the LSTM below 100.\nFigure 8: Speed up rate of Skim-LSTM (vs LSTM) with varying skimming rates and hidden state sizes.\nFigure 8 shows the relative speed gain of Skim-LSTM compared to standard LSTM with varying hidden state size and skim rate. We use NumPy, and the inferences are run on a single thread of CPU. We also plot the ratio between the reduction of the number of float operations (Flop-R) of LSTM and Skim-LSTM. This can be considered as a theoretical upper bound of the speed gain on CPUs. We note two important observations. First, there is an inevitable gap between the actual gain (solid line) and the theoretical gain (dotted line). This gap will be larger with more overhead of the framework, or more parallelization (e.g. multithreading). Second, the gap decreases as the hidden state size increases because the the overhead becomes negligible with very large matrix operations. Hence, the benefit of Skim-RNN will be greater for larger hidden state size.\nLatency. A modern GPU has much higher throughput than a CPU with parallel processing. However, for small networks, the CPU often has lower latency than the GPU. Comparing between NumPy with CPU and TensorFlow with GPU (Titan X), we observe that the former has 1.5 times lower latency (75 \\upmus vs 110 \\upmus per token) for LSTM of d=100. This means that combining Skim-RNN with CPU-based framework can lead to substantially lower latency than GPUs. For instance, Skim-RNN with CPU on IMDb has 4.5x lower latency than a GPU, requiring only 29 \\upmus per token on average.\nWe present Skim-RNN, a recurrent neural network that can dynamically decide to use the big RNN (read) or the small RNN (skim) at each time step, depending on the importance of the input. While Skim-RNN has significantly lower computational cost than its RNN counterpart, the accuracy of Skim-RNN is still on par with or better than standard RNNs, LSTM-Jump, and VCRNN. Since Skim-RNN has the same input and output interface as an RNN, it can easily replace RNNs in existing applications. We also show that a Skim-RNN can offer better latency results on a CPU compared to a standard RNN on a GPU. Future work involves using Skim-RNN for applications that require much higher hidden state size, such as video understanding, and using multiple small RNN cells for varying degrees of skimming.\nThis research was supported by the NSF (IIS 1616112), Allen Distinguished Investigator Award, Samsung GRO award, and gifts from Google, Amazon, Allen Institute for AI, and Bloomberg. We thank the anonymous reviewers for their helpful comments.\nBalduzzi & Ghifary (2016) David Balduzzi and Muhammad Ghifary. Strongly-typed recurrent neural networks. In ICML, 2016.\nCampos et al. (2017) V\u00edctor Campos, Brendan Jou, Xavier Gir\u00f3-i Nieto, Jordi Torres, and Shih-Fu Chang. Skip rnn: Learning to skip state updates in recurrent neural networks. arXiv preprint arXiv:1708.06834, 2017.\nChoi et al. (2017) Eunsol Choi, Daniel Hewlett, Alexandre Lacoste, Illia Polosukhin, Jakob Uszkoreit, and Jonathan Berant. Coarse-to-fine question answering for long documents. In ACL, 2017.\nChung et al. (2017) Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In ICLR, 2017.\nDyer et al. (2016) Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network grammars. In NAACL, 2016.\nHahn & Keller (2016) Michael Hahn and Frank Keller. Modeling human reading with neural attention. In EMNLP, 2016.\nJang et al. (2017) Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017.\nJernite et al. (2017) Yacine Jernite, Edouard Grave, Armand Joulin, and Tomas Mikolov. Variable computation in recurrent neural networks. In ICLR, 2017.\nJohansen et al. (2017) Alexander Johansen, Bryan McCann, James Bradbury, and Richard Socher. Learning when to read and when to skim, 2017. URL https://metamind.io/research/learning-when-to-skim-and-when-to-read.\nJoshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL, 2017.\nKembhavi et al. (2017) Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In CVPR, 2017.\nKokkinos & Potamianos (2017) Filippos Kokkinos and Alexandros Potamianos. Structural attention neural networks for improved sentiment analysis. arXiv preprint arXiv:1701.01811, 2017.\nKong et al. (2016) Lingpeng Kong, Chris Dyer, and Noah A Smith. Segmental recurrent neural networks. In ICLR, 2016.\nMarcel Adam Just (1987) Patricia Anderson Carpenter Marcel Adam Just. The Psychology of Reading and Language Comprehension. 1987.\nMikolov et al. (2015) Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc\u2019Aurelio Ranzato. Learning longer memory in recurrent neural networks. In ICLR Workshop, 2015.\nMin et al. (2017) Sewon Min, Minjoon Seo, and Hannaneh Hajishirzi.\nQuestion answering through transfer learning from large fine-grained supervision data.\nMiyato et al. (2017) Takeru Miyato, Andrew M. Dai, and Ian Goodfellow. Adversarial training methods for semi-supervised text classification. In ICLR, 2017.\nMnih et al. (2014) Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In NIPS, 2014.\nOdena et al. (2017) Augustus Odena, Dieterich Lawson, and Christopher Olah.\nChanging model behavior at test-time using reinforcement learning.\nIn ICLR Workshop, 2017.\nRastegari et al. (2016) Mohammad Rastegari, Vicente Ordonez, Joseph Redmon<|endoftext|>"
    }
]